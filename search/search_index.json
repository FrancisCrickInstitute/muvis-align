{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"muvis-align","text":""},{"location":"poster/","title":"muvis-align: building a registration pipeline for large data &amp; next gen file format","text":""},{"location":"poster/#joost-de-folter-francis-crick-institute-london-marvin-albert-eth-zurich","title":"Joost de Folter (Francis Crick Institute, London), Marvin Albert (ETH, Zurich)","text":"<p>We present a new registration pipeline called muvis-align, which is based on the multiview-stitcher toolbox. Multiview-stitcher is an open-source modular toolbox developed for distributed and tiled stitching of 2-3D data. This modular, powerful toolbox is used to develop a flexible registration pipeline including pre- and post-processing steps as well as custom functions allowing x-y stitching and z reconstruction, for different image modalities. Importantly muvis-align overcomes limitations in existing tools used commonly, in particular handling large datasets (TBs) and fully supports the Next Generation File Format Ome-zarr. Preliminary registration tests show results of various LM and EM datasets equal or better compared to existing tools using affine transformations. This tool is further being developed with a napari user interface allowing easy configuration, exploring responsive dynamic configuration and providing visual feedback on preliminary and final results.</p> <p>github.com/FrancisCrickInstitute/muvis-align</p> <p>github.com/multiview-stitcher/multiview-stitcher</p>"},{"location":"poster/#fair","title":"FAIR","text":"<ul> <li>Findable: Searchable good metadata</li> <li>Accessible: Online &amp; open</li> <li>Interoperable: Compatible format</li> <li>Reusable: Record method</li> </ul> <p>General: Unambiguously descriptive, follow standards, persistent</p> <p>FAIR 2.0: Use standard vocabularies, include level of confidence</p> <pre><code>---\nconfig:\n  themeVariables:\n    fontSize: 20px\n---\nflowchart LR\n    subgraph sub_input[\"Input\"]\n        ome(\"OME/NGFF data&lt;br&gt;(Ome-tiff/&lt;br&gt;Ome-zarr)\"):::io\n        params(\"Params&lt;br&gt;(.yaml)\"):::io\n    end\n\n    subgraph sub_init[\"Init\"]\n        direction TB\n        load(\"Import OME\"):::core\n        init(\"To Spatial Image&lt;br&gt;(SIM)\"):::core\n    end\n\n    subgraph sub_preprocessing[\"Pre-processing\"]\n        direction TB\n        flatfield(\"flatfield correction\"):::core\n        normalise(\"Normalisation\"):::core\n        filter(\"Foreground filter\"):::core\n    end\n\n    subgraph sub_reg[\"Registration\"]\n        direction TB\n        reg_method(\"Method selection\"):::core\n        pairing(\"Pairing\"):::core\n        reg(\"Registration&lt;br&gt;&lt;b&gt;(Multiview-stitcher)\"):::ext\n    end\n\n    subgraph sub_fusion[\"Fusion\"]\n        direction TB\n        fusion_stack(\"Fusion stack calculation\"):::core\n        fuse(\"Fuse&lt;br&gt;&lt;b&gt;(Multiview-stitcher)\"):::ext\n    end\n\n    subgraph sub_export[\"Export\"]\n        direction LR\n        write_reg(\"Registration&lt;br&gt;(.json + .csv)\"):::io\n        write_metrics(\"Metrics&lt;br&gt;(.json)\"):::io\n        write_report(\"Report&lt;br&gt;(.pdf)\"):::io\n        write(\"OME/NGFF data&lt;br&gt;(Ome-tiff/&lt;br&gt;Ome-zarr)\"):::io\n    end\n\n    ome --&gt;|image data| sub_init\n    ome --&gt;|metadata| sub_init\n    params --&gt; sub_init\n    load ==&gt;|image data &amp; metadata| init\n\n    sub_init ==&gt; sub_preprocessing\n    flatfield ==&gt; normalise ==&gt; filter\n\n    sub_preprocessing ==&gt; sub_reg\n    reg_method ==&gt; pairing ==&gt; reg\n\n    sub_reg --&gt;|transforms| write_reg\n    sub_reg --&gt;|metrics| write_metrics\n    sub_reg --&gt;|overview/metrics| write_report\n    sub_reg ==&gt; sub_fusion\n    fusion_stack ==&gt; fuse\n    sub_fusion ==&gt;|SIMs &amp; transforms| write\n\n    %% Styles\n    %% classDef default font-size:20\n    classDef core fill:#e0f7fa,stroke:#006064,color:#006064\n    classDef io fill:#fff5e9,stroke:#5e4e20,color:#5e4e20\n    classDef plugin fill:#fff3e0,stroke:#e65100,color:#e65100\n    classDef ext fill:#eceff1,stroke:#37474f,color:#37474f\n\n    linkStyle 0 stroke:#006064\n    linkStyle 1 stroke:#ddbb00\n\n    linkStyle 3 stroke:green,stroke-width:8\n    linkStyle 4 stroke:green,stroke-width:8\n    linkStyle 5 stroke:green,stroke-width:8\n    linkStyle 6 stroke:green,stroke-width:8\n    linkStyle 7 stroke:green,stroke-width:8\n    linkStyle 8 stroke:green,stroke-width:8\n    linkStyle 9 stroke:green,stroke-width:8\n    linkStyle 13 stroke:green,stroke-width:8\n    linkStyle 14 stroke:green,stroke-width:8\n    linkStyle 15 stroke:green,stroke-width:8</code></pre> <pre><code>---\nconfig:\n  themeVariables:\n    fontSize: 20px\n---\nflowchart LR\n    subgraph data_layers[\"Persistent&amp;nbsp;metadata\"]\n        direction TB\n        layers(\"Multiscale Spatial-image&lt;hr&gt;&lt;b&gt;Spatial-image\n&lt;hr&gt;Xarray&lt;hr&gt;Dask array&lt;hr&gt;File store\"):::green\n        image(\"Image data&lt;br&gt;(voxels)\"):::blue --&gt; layers\n        size(\"Pixel size&lt;br&gt;(w,h,d)\"):::yellow --&gt; layers\n        coords(\"Coordinates&lt;br&gt;(x,y,z)\"):::yellow --&gt; layers\n        transforms(\"Transforms&lt;br&gt;[]\"):::yellow --&gt; layers\n    end\n\n    %% Styles\n    %% classDef default font-size:20\n    classDef blue fill:#e0f7fa,stroke:#006064,color:#006064\n    classDef yellow fill:#fff5e9,stroke:#5e4e20,color:#5e4e20\n    classDef green fill:#e8f5e9,stroke:#1b5e20,color:#1b5e20\n\n    linkStyle 0 stroke:#006064\n    linkStyle 1 stroke:#ddbb00\n    linkStyle 2 stroke:#ddbb00\n    linkStyle 3 stroke:#ddbb00</code></pre> <p>CCP-volumeEM: Martin Jones, Michele Darrow, Matthew Hartley, Helen Spiers, Martyn Winn, Amy Strange</p>"},{"location":"references/","title":"References","text":""},{"location":"references/#run.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"references/#run.napari_ui","title":"<code>napari_ui = 'napari' in params['general'].get('ui', '')</code>  <code>module-attribute</code>","text":""},{"location":"references/#run.params","title":"<code>params = yaml.safe_load(file)</code>  <code>module-attribute</code>","text":""},{"location":"references/#run.parser","title":"<code>parser = argparse.ArgumentParser(description=f'multiview-stitcher')</code>  <code>module-attribute</code>","text":""},{"location":"references/#run.pipeline","title":"<code>pipeline = Pipeline(params, viewer)</code>  <code>module-attribute</code>","text":""},{"location":"references/#run.viewer","title":"<code>viewer = napari.Viewer()</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.MVSRegistration","title":"<code>MVSRegistration</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration","title":"<code>MVSRegistration</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>class MVSRegistration:\n    def __init__(self, params_general):\n        super().__init__()\n        self.params_general = params_general\n\n        params_logging = self.params_general.get('logging', {})\n        self.verbose = params_logging.get('verbose', False)\n        self.logging_dask = params_logging.get('dask', False)\n        self.logging_time = params_logging.get('time', False)\n        self.ui = self.params_general.get('ui', '')\n        self.mpl_ui = ('mpl' in self.ui or 'plot' in self.ui)\n        self.napari_ui = ('napari' in self.ui)\n        self.source_transform_key = 'source_metadata'\n        self.reg_transform_key = 'registered'\n        self.transition_transform_key = 'transition'\n\n        logging.info(f'Multiview-stitcher version: {multiview_stitcher.__version__}')\n\n    def run_operation(self, fileset_label, filenames, params, global_rotation=None, global_center=None):\n        self.fileset_label = fileset_label\n        self.filenames = filenames\n        self.file_labels = get_unique_file_labels(filenames)\n        self.params = params\n        self.global_rotation = global_rotation\n        self.global_center = global_center\n\n        input_dir = os.path.dirname(filenames[0])\n        parts = split_numeric_dict(filenames[0])\n        output_pattern = params['output'].format_map(parts)\n        self.output = os.path.join(input_dir, output_pattern)    # preserve trailing slash: do not use os.path.normpath()\n\n        with ProgressBar(minimum=10, dt=1) if self.logging_dask else nullcontext():\n            return self._run_operation()\n\n    def _run_operation(self):\n        params = self.params\n        filenames = self.filenames\n        file_labels = self.file_labels\n        output = self.output\n\n        operation = params['operation']\n        overlap_threshold = params.get('overlap_threshold', 0.5)\n        source_metadata = import_metadata(params.get('source_metadata', {}), input_path=params['input'])\n        save_images = params.get('save_images', True)\n        target_scale = params.get('scale')\n        extra_metadata = import_metadata(params.get('extra_metadata', {}), input_path=params['input'])\n        channels = extra_metadata.get('channels', [])\n        normalise_orientation = 'norm' in source_metadata\n\n        show_original = self.params_general.get('show_original', False)\n        output_params = self.params_general.get('output', {})\n        clear = output_params.get('clear', False)\n        overwrite = output_params.get('overwrite', True)\n\n        is_stack = ('stack' in operation)\n        is_3d = ('3d' in operation)\n        is_simple_stack = is_stack and not is_3d\n        is_transition = ('transition' in operation)\n        is_channel_overlay = (len(channels) &gt; 1)\n\n        mappings_header = ['id','x_pixels', 'y_pixels', 'z_pixels', 'x', 'y', 'z', 'rotation']\n\n        if len(filenames) == 0:\n            logging.warning('Skipping (no images)')\n            return False\n\n        registered_fused_filename = output + 'registered'\n        mappings_filename = os.path.join(output, params.get('mappings', 'mappings.json'))\n\n        output_dir = os.path.dirname(output)\n        if not overwrite and exists_output_image(registered_fused_filename, output_params.get('format')):\n            logging.warning(f'Skipping existing output {os.path.normpath(output_dir)}')\n            return False\n        if clear:\n            shutil.rmtree(output_dir, ignore_errors=True)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        with Timer('init sims', self.logging_time):\n            sims, scales, positions, rotations = self.init_sims(target_scale=target_scale)\n\n        with Timer('pre-process', self.logging_time):\n            sims, register_sims, indices = self.preprocess(sims, params)\n\n        data = []\n        for label, sim, scale in zip(file_labels, sims, scales):\n            position, rotation = get_data_mapping(sim, transform_key=self.source_transform_key)\n            position_pixels = np.array(position) / scale\n            row = [label] + list(position_pixels) + list(position) + [rotation]\n            data.append(row)\n        export_csv(output + 'prereg_mappings.csv', data, header=mappings_header)\n\n        if show_original:\n            # before registration:\n            logging.info('Exporting original...')\n            original_positions_filename = output + 'positions_original.pdf'\n\n            with Timer('plot positions', self.logging_time):\n                vis_utils.plot_positions(sims, transform_key=self.source_transform_key,\n                                         use_positional_colors=False, view_labels=file_labels, view_labels_size=3,\n                                         show_plot=self.mpl_ui, output_filename=original_positions_filename)\n\n            if self.napari_ui:\n                shapes = [get_sim_shape_2d(sim, transform_key=self.source_transform_key) for sim in sims]\n                self.update_napari_signal.emit(f'{self.fileset_label} original', shapes, file_labels)\n\n            if save_images:\n                if output_params.get('thumbnail'):\n                    with Timer('create thumbnail', self.logging_time):\n                        self.save_thumbnail(output + 'thumb_original',\n                                            nom_sims=sims,\n                                            transform_key=self.source_transform_key)\n\n                original_fused = self.fuse(sims, transform_key=self.source_transform_key)\n\n                original_fused_filename = output + 'original'\n                save_image(original_fused_filename, output_params.get('format'), original_fused, channels=channels,\n                           transform_key=self.source_transform_key, params=output_params)\n\n        if len(filenames) == 1 and save_images:\n            logging.warning('Skipping registration (single image)')\n            save_image(registered_fused_filename, output_params.get('format'), sims[0], channels=channels,\n                       translation0=positions[0], params=output_params)\n            return False\n\n        is_simple_stack = is_stack and not is_3d\n        _, has_overlaps = self.validate_overlap(sims, file_labels, is_simple_stack, is_simple_stack or is_channel_overlay)\n        overall_overlap = np.mean(has_overlaps)\n        if overall_overlap &lt; overlap_threshold:\n            raise ValueError(f'Not enough overlap: {overall_overlap * 100:.1f}%')\n\n        if not overwrite and os.path.exists(mappings_filename):\n            logging.info('Loading registration mappings...')\n            # load registration mappings\n            mappings = import_json(mappings_filename)\n            # copy transforms to sims\n            for sim, label in zip(sims, file_labels):\n                mapping = param_utils.affine_to_xaffine(np.array(mappings[label]))\n                if is_stack:\n                    transform = param_utils.identity_transform(ndim=3)\n                    transform.loc[{dim: mapping.coords[dim] for dim in mapping.dims}] = mapping\n                else:\n                    transform = mapping\n                si_utils.set_sim_affine(sim, transform, transform_key=self.reg_transform_key)\n        else:\n            with Timer('register', self.logging_time):\n                results = self.register(sims, register_sims, indices, params)\n\n            reg_result = results['reg_result']\n            sims = results['sims']\n\n            logging.info('Exporting registered...')\n            metrics = self.calc_metrics(results, file_labels)\n            mappings = metrics['mappings']\n            logging.info(metrics['summary'])\n            export_json(mappings_filename, mappings)\n            export_json(output + 'metrics.json', metrics)\n            data = []\n            for sim, (label, mapping), scale, position, rotation in zip(sims, mappings.items(), scales, positions, rotations):\n                if not normalise_orientation:\n                    # rotation already in msim affine transform\n                    rotation = None\n                position, rotation = get_data_mapping(sim, transform_key=self.reg_transform_key,\n                                                      transform=np.array(mapping),\n                                                      translation0=position,\n                                                      rotation=rotation)\n                position_pixels = np.array(position) / scale\n                row = [label] + list(position_pixels) + list(position) + [rotation]\n                data.append(row)\n            export_csv(output + 'mappings.csv', data, header=mappings_header)\n\n            for reg_label, reg_item in reg_result.items():\n                if isinstance(reg_item, dict):\n                    summary_plot = reg_item.get('summary_plot')\n                    if summary_plot is not None:\n                        figure, axes = summary_plot\n                        summary_plot_filename = output + f'{reg_label}.pdf'\n                        figure.savefig(summary_plot_filename)\n\n        registered_positions_filename = output + 'positions_registered.pdf'\n        with Timer('plot positions', self.logging_time):\n            vis_utils.plot_positions(sims, transform_key=self.reg_transform_key,\n                                     use_positional_colors=False, view_labels=file_labels, view_labels_size=3,\n                                     show_plot=self.mpl_ui, output_filename=registered_positions_filename)\n\n        if self.napari_ui:\n            shapes = [get_sim_shape_2d(sim, transform_key=self.reg_transform_key) for sim in sims]\n            self.update_napari_signal.emit(f'{self.fileset_label} registered', shapes, file_labels)\n\n        if save_images:\n            if output_params.get('thumbnail'):\n                with Timer('create thumbnail', self.logging_time):\n                    self.save_thumbnail(output + 'thumb', nom_sims=sims, transform_key=self.reg_transform_key)\n\n            with Timer('fuse image', self.logging_time):\n                fused_image = self.fuse(sims)\n\n            logging.info('Saving fused image...')\n            with Timer('save fused image', self.logging_time):\n                save_image(registered_fused_filename, output_params.get('format'), fused_image, channels=channels,\n                           transform_key=self.reg_transform_key, translation0=positions[0], params=output_params)\n\n        if is_transition:\n            self.save_video(output, sims, fused_image)\n\n        return True\n\n    def init_sims(self, target_scale=None):\n        operation = self.params['operation']\n        source_metadata = import_metadata(self.params.get('source_metadata', 'source'), input_path=self.params['input'])\n        chunk_size = self.params_general.get('chunk_size', [1024, 1024])\n        extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n        z_scale = extra_metadata.get('scale', {}).get('z')\n\n        logging.info('Initialising sims...')\n        sources = [create_dask_source(file, source_metadata) for file in self.filenames]\n        source0 = sources[0]\n        images = []\n        sims = []\n        scales = []\n        translations = []\n        rotations = []\n\n        is_stack = ('stack' in operation)\n        has_z_size = (source0.get_size().get('z', 0) &gt; 0)\n        is_3d = (has_z_size or '3d' in operation)\n        pyramid_level = 0\n\n        output_order = 'zyx' if is_stack or is_3d else 'yx'\n        ndims = len(output_order)\n        if source0.get_nchannels() &gt; 1:\n            output_order += 'c'\n\n        last_z_position = None\n        different_z_positions = False\n        delta_zs = []\n        for filename, source in zip(self.filenames, sources):\n            scale = source.get_pixel_size()\n            translation = source.get_position()\n            rotation = source.get_rotation()\n\n            if target_scale:\n                pyramid_level = np.argmin(abs(np.array(source.scales) - target_scale))\n                pyramid_scale = source.scales[pyramid_level]\n                scale = {dim: size * pyramid_scale if dim in 'xy' else size for dim, size in scale.items()}\n            if 'invert' in source_metadata:\n                translation[0] = -translation[0]\n                translation[1] = -translation[1]\n            if len(translation) &gt;= 3:\n                z_position = translation['z']\n            else:\n                z_position = 0\n            if last_z_position is not None and z_position != last_z_position:\n                different_z_positions = True\n                delta_zs.append(z_position - last_z_position)\n            if self.global_rotation is not None:\n                rotation = self.global_rotation\n\n            dask_data = source.get_data(level=pyramid_level)\n            image = redimension_data(dask_data, source.dimension_order, output_order)\n\n            scales.append(scale)\n            translations.append(translation)\n            rotations.append(rotation)\n            images.append(image)\n            last_z_position = z_position\n\n        if z_scale is None:\n            if len(delta_zs) &gt; 0:\n                z_scale = np.min(delta_zs)\n            else:\n                z_scale = 1\n\n        if 'norm' in source_metadata:\n            size = np.array(source0.get_size()) * source0.get_pixel_size_micrometer()\n            center = None\n            if 'center' in source_metadata:\n                if 'global' in source_metadata:\n                    center = self.global_center\n                else:\n                    center = np.mean(translations, 0)\n            elif 'origin' in source_metadata:\n                center = np.zeros(ndims)\n            translations, rotations = normalise_rotated_positions(translations, rotations, size, center)\n\n        #translations = [np.array(translation) * 1.25 for translation in translations]\n\n        increase_z_positions = is_stack and not different_z_positions\n        z_position = 0\n        scales2 = []\n        translations2 = []\n        for source, image, scale, translation, rotation, file_label in zip(sources, images, scales, translations, rotations, self.file_labels):\n            # transform #dimensions need to match\n            if len(scale) &gt; 0 and 'z' not in scale:\n                scale['z'] = abs(z_scale)\n            if (len(translation) &gt; 0 and 'z' not in translation) or increase_z_positions:\n                translation['z'] = z_position\n            if increase_z_positions:\n                z_position += z_scale\n            channel_labels = [channel.get('label', '') for channel in source.get_channels()]\n            if rotation is None or 'norm' in source_metadata:\n                # if positions are normalised, don't use rotation\n                transform = None\n            else:\n                transform = param_utils.invert_coordinate_order(\n                    create_transform(translation, rotation, matrix_size=ndims + 1)\n                )\n            if file_label in extra_metadata:\n                transform2 = extra_metadata[file_label]\n                if transform is None:\n                    transform = np.array(transform2)\n                else:\n                    transform = np.array(combine_transforms([transform, transform2]))\n            sim = si_utils.get_sim_from_array(\n                image,\n                dims=list(output_order),\n                scale=scale,\n                translation=translation,\n                affine=transform,\n                transform_key=self.source_transform_key,\n                c_coords=channel_labels\n            )\n            if len(sim.chunksizes.get('x')) == 1 and len(sim.chunksizes.get('y')) == 1:\n                sim = sim.chunk(xyz_to_dict(chunk_size))\n            sims.append(sim)\n            scales2.append(dict_to_xyz(scale))\n            translations2.append(dict_to_xyz(translation))\n        return sims, scales2, translations2, rotations\n\n    def validate_overlap(self, sims, labels, is_stack=False, expect_large_overlap=False):\n        min_dists = []\n        has_overlaps = []\n        n = len(sims)\n        positions = [get_sim_position_final(sim) for sim in sims]\n        sizes = [np.linalg.norm(get_sim_physical_size(sim)) for sim in sims]\n        for i in range(n):\n            norm_dists = []\n            # check if only single z slices\n            if is_stack:\n                if i + 1 &lt; n:\n                    compare_indices = [i + 1]\n                else:\n                    compare_indices = []\n            else:\n                compare_indices = range(n)\n            for j in compare_indices:\n                if not j == i:\n                    distance = math.dist(positions[i], positions[j])\n                    norm_dist = distance / np.mean([sizes[i], sizes[j]])\n                    norm_dists.append(norm_dist)\n            if len(norm_dists) &gt; 0:\n                norm_dist = min(norm_dists)\n                min_dists.append(float(norm_dist))\n                if norm_dist &gt;= 1:\n                    logging.warning(f'{labels[i]} has no overlap')\n                    has_overlaps.append(False)\n                elif expect_large_overlap and norm_dist &gt; 0.5:\n                    logging.warning(f'{labels[i]} has small overlap')\n                    has_overlaps.append(False)\n                else:\n                    has_overlaps.append(True)\n        return min_dists, has_overlaps\n\n    def preprocess(self, sims, params):\n        flatfield_quantiles = params.get('flatfield_quantiles')\n        normalisation = params.get('normalisation', '')\n        filter_foreground = params.get('filter_foreground', False)\n\n        if filter_foreground:\n            foreground_map = calc_foreground_map(sims)\n        else:\n            foreground_map = None\n        if flatfield_quantiles is not None:\n            logging.info('Flat-field correction...')\n            new_sims = [None] * len(sims)\n            for sim_indices in group_sims_by_z(sims):\n                sims_z_set = [sims[i] for i in sim_indices]\n                foreground_map_z_set = [foreground_map[i] for i in sim_indices] if foreground_map is not None else None\n                new_sims_z_set = flatfield_correction(sims_z_set, self.source_transform_key, flatfield_quantiles,\n                                                      foreground_map=foreground_map_z_set)\n                for sim_index, sim in zip(sim_indices, new_sims_z_set):\n                    new_sims[sim_index] = sim\n            sims = new_sims\n\n        if normalisation:\n            use_global = ('global' in normalisation)\n            if use_global:\n                logging.info('Normalising (global)...')\n            else:\n                logging.info('Normalising (individual)...')\n            new_sims = normalise(sims, self.source_transform_key, use_global=use_global)\n        else:\n            new_sims = sims\n\n        if filter_foreground:\n            logging.info('Filtering foreground images...')\n            #tile_vars = np.array([np.asarray(np.std(sim)).item() for sim in sims])\n            #threshold1 = np.mean(tile_vars)\n            #threshold2 = np.median(tile_vars)\n            #threshold3, _ = cv.threshold(np.array(tile_vars).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n            #threshold = min(threshold1, threshold2, threshold3)\n            #foregrounds = (tile_vars &gt;= threshold)\n            new_sims = [sim for sim, is_foreground in zip(new_sims, foreground_map) if is_foreground]\n            logging.info(f'Foreground images: {len(new_sims)} / {len(sims)}')\n            indices = np.where(foreground_map)[0]\n        else:\n            indices = range(len(sims))\n        return sims, new_sims, indices\n\n    def register(self, sims, register_sims, indices, params):\n        sim0 = sims[0]\n        ndims = si_utils.get_ndim_from_sim(sim0)\n\n        operation = params['operation']\n        reg_params = params.get('method')\n        if isinstance(reg_params, dict):\n            reg_method = reg_params.get('name', '').lower()\n        else:\n            reg_method = reg_params.lower()\n        use_orthogonal_pairs = params.get('use_orthogonal_pairs', False)\n\n        is_stack = ('stack' in operation)\n        is_3d = ('3d' in operation)\n        debug = self.params_general.get('debug', False)\n\n        reg_channel = params.get('channel', 0)\n        if isinstance(reg_channel, int):\n            reg_channel_index = reg_channel\n            reg_channel = None\n        else:\n            reg_channel_index = None\n\n        groupwise_resolution_kwargs = {\n            'transform': params.get('transform_type')  # options include 'translation', 'rigid', 'affine'\n        }\n        pairwise_reg_func_kwargs = None\n        if is_stack and not is_3d:\n            # register in 2d; pairwise consecutive views\n            register_sims = [si_utils.max_project_sim(sim, dim='z') for sim in register_sims]\n            pairs = [(index, index + 1) for index in range(len(register_sims) - 1)]\n        elif use_orthogonal_pairs:\n            origins = np.array([get_sim_position_final(sim) for sim in register_sims])\n            size = get_sim_physical_size(sim0)\n            pairs, _ = get_orthogonal_pairs(origins, size)\n            logging.info(f'#pairs: {len(pairs)}')\n            for pair in pairs:\n                print(f'{self.file_labels[pair[0]]} - {self.file_labels[pair[1]]}')\n        else:\n            pairs = None\n\n        if is_3d:\n            overlap_tolerance = {'z': 1}\n        else:\n            overlap_tolerance = None\n\n        if '3din2d' in reg_method:\n            from src.registration_methods.RegistrationMethodANTs3Din2D import RegistrationMethodANTs3Din2D\n            registration_method = RegistrationMethodANTs3Din2D(sim0, reg_params, debug)\n            pairwise_reg_func = registration_method.registration\n        elif 'cpd' in reg_method:\n            from src.registration_methods.RegistrationMethodCPD import RegistrationMethodCPD\n            registration_method = RegistrationMethodCPD(sim0, reg_params, debug)\n            pairwise_reg_func = registration_method.registration\n        elif 'feature' in reg_method or 'orb' in reg_method or 'sift' in reg_method:\n            if 'cv' in reg_method:\n                from src.registration_methods.RegistrationMethodCvFeatures import RegistrationMethodCvFeatures\n                registration_method = RegistrationMethodCvFeatures(sim0, reg_params, debug)\n            else:\n                from src.registration_methods.RegistrationMethodSkFeatures import RegistrationMethodSkFeatures\n                registration_method = RegistrationMethodSkFeatures(sim0, reg_params, debug)\n            pairwise_reg_func = registration_method.registration\n        elif 'ant' in reg_method:\n            pairwise_reg_func = registration.registration_ANTsPy\n            # args for ANTsPy registration: used internally by ANYsPy algorithm\n            pairwise_reg_func_kwargs = {\n                'transform_types': ['Rigid'],\n                \"aff_random_sampling_rate\": 0.5,\n                \"aff_iterations\": (2000, 2000, 1000, 1000),\n                \"aff_smoothing_sigmas\": (4, 2, 1, 0),\n                \"aff_shrink_factors\": (16, 8, 2, 1),\n            }\n        else:\n            pairwise_reg_func = registration.phase_correlation_registration\n\n        # Pass registration through metrics method\n        #from src.registration_methods.RegistrationMetrics import RegistrationMetrics\n        #registration_metrics = RegistrationMetrics(sim0, pairwise_reg_function)\n        #pairwise_reg_function = registration_metrics.registration\n        # TODO: extract metrics from registration_metrics\n\n        logging.info(f'Registration method: {reg_method}')\n\n        try:\n            logging.info('Registering...')\n            register_msims = [msi_utils.get_msim_from_sim(sim) for sim in register_sims]\n            reg_result = registration.register(\n                register_msims,\n                reg_channel=reg_channel,\n                reg_channel_index=reg_channel_index,\n                transform_key=self.source_transform_key,\n                new_transform_key=self.reg_transform_key,\n\n                pairs=pairs,\n                pre_registration_pruning_method=None,\n\n                pairwise_reg_func=pairwise_reg_func,\n                pairwise_reg_func_kwargs=pairwise_reg_func_kwargs,\n                groupwise_resolution_kwargs=groupwise_resolution_kwargs,\n\n                post_registration_do_quality_filter=True,\n                post_registration_quality_threshold=0.1,\n\n                plot_summary=self.mpl_ui,\n                return_dict=True,\n\n                overlap_tolerance=overlap_tolerance,\n            )\n            # copy transforms from register sims to unmodified sims\n            for reg_msim, index in zip(register_msims, indices):\n                si_utils.set_sim_affine(\n                    sims[index],\n                    msi_utils.get_transform_from_msim(reg_msim, transform_key=self.reg_transform_key),\n                    transform_key=self.reg_transform_key)\n\n            # set missing transforms\n            for sim in sims:\n                if self.reg_transform_key not in si_utils.get_tranform_keys_from_sim(sim):\n                    si_utils.set_sim_affine(\n                        sim,\n                        param_utils.identity_transform(ndim=ndims, t_coords=[0]),\n                        transform_key=self.reg_transform_key)\n\n            mappings = reg_result['params']\n            # re-index from subset of sims\n            residual_error_dict = reg_result.get('groupwise_resolution', {}).get('metrics', {}).get('residuals', {})\n            residual_error_dict = {(indices[key[0]], indices[key[1]]): value.item()\n                                   for key, value in residual_error_dict.items()}\n            registration_qualities_dict = reg_result.get('pairwise_registration', {}).get('metrics', {}).get('qualities', {})\n            registration_qualities_dict = {(indices[key[0]], indices[key[1]]): value\n                                           for key, value in registration_qualities_dict.items()}\n        except NotEnoughOverlapError:\n            logging.warning('Not enough overlap')\n            reg_result = {}\n            mappings = [param_utils.identity_transform(ndim=ndims, t_coords=[0])] * len(sims)\n            residual_error_dict = {}\n            registration_qualities_dict = {}\n\n        # re-index from subset of sims\n        mappings_dict = {index: mapping for index, mapping in zip(indices, mappings)}\n\n        if is_stack:\n            # set 3D affine transforms from 2D registration params\n            for index, sim in enumerate(sims):\n                # check if already 3D\n                if 4 not in si_utils.get_affine_from_sim(sim, transform_key=self.reg_transform_key).shape:\n                    affine_3d = param_utils.identity_transform(ndim=3)\n                    affine_3d.loc[{dim: mappings[index].coords[dim] for dim in mappings[index].sel(t=0).dims}] = mappings[index].sel(t=0)\n                    si_utils.set_sim_affine(sim, affine_3d, transform_key=self.reg_transform_key)\n\n        return {'reg_result': reg_result,\n                'mappings': mappings_dict,\n                'residual_errors': residual_error_dict,\n                'registration_qualities': registration_qualities_dict,\n                'sims': sims,\n                'pairs': pairs}\n\n    def fuse(self, sims, transform_key=None):\n        if transform_key is None:\n            transform_key = self.reg_transform_key\n        operation = self.params['operation']\n        extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n        channels = extra_metadata.get('channels', [])\n        z_scale = extra_metadata.get('scale', {}).get('z')\n        if z_scale is None:\n            if 'z' in sims[0].dims:\n                z_scale = np.min(np.diff(sorted(set([si_utils.get_origin_from_sim(sim).get('z', 0) for sim in sims]))))\n        if not z_scale:\n            z_scale = 1\n\n        is_3d = ('3d' in operation)\n        is_channel_overlay = (len(channels) &gt; 1)\n\n        sim0 = sims[0]\n        source_type = sim0.dtype\n\n        output_stack_properties = calc_output_properties(sims, transform_key, z_scale=z_scale)\n        if is_channel_overlay:\n            # convert to multichannel images\n            if self.verbose:\n                logging.info(f'Output stack: {output_stack_properties}')\n            data_size = np.prod(list(output_stack_properties['shape'].values())) * len(sims) * source_type.itemsize\n            logging.info(f'Fusing channels {print_hbytes(data_size)}')\n\n            channel_sims = [fusion.fuse(\n                [sim],\n                transform_key=transform_key,\n                output_stack_properties=output_stack_properties\n            ) for sim in sims]\n            channel_sims = [sim.assign_coords({'c': [channels[simi]['label']]}) for simi, sim in enumerate(channel_sims)]\n            fused_image = xr.combine_nested([sim.rename() for sim in channel_sims], concat_dim='c', combine_attrs='override')\n        else:\n            if is_3d:\n                z_positions = sorted(set([si_utils.get_origin_from_sim(sim).get('z', 0) for sim in sims]))\n                z_shape = len(z_positions)\n                if z_shape &lt;= 1:\n                    z_shape = len(sims)\n                output_stack_properties['shape']['z'] = z_shape\n            if self.verbose:\n                logging.info(f'Output stack: {output_stack_properties}')\n            data_size = np.prod(list(output_stack_properties['shape'].values())) * source_type.itemsize\n            logging.info(f'Fusing {print_hbytes(data_size)}')\n\n            fused_image = fusion.fuse(\n                sims,\n                transform_key=transform_key,\n                output_stack_properties=output_stack_properties,\n                fusion_func=fusion.simple_average_fusion,\n            )\n        return fused_image\n\n    def save_thumbnail(self, output_filename, nom_sims=None, transform_key=None):\n        extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n        channels = extra_metadata.get('channels', [])\n        output_params = self.params_general['output']\n        thumbnail_scale = output_params.get('thumbnail_scale', 16)\n        sims = self.init_sims(target_scale=thumbnail_scale)[0]\n\n        if nom_sims is not None:\n            if sims[0].sizes['x'] &gt;= nom_sims[0].sizes['x']:\n                logging.warning('Unable to generate scaled down thumbnail due to lack of source pyramid sizes')\n                return\n\n            if transform_key is not None and transform_key != self.source_transform_key:\n                for nom_sim, sim in zip(nom_sims, sims):\n                    si_utils.set_sim_affine(sim,\n                                            si_utils.get_affine_from_sim(nom_sim, transform_key=transform_key),\n                                            transform_key=transform_key)\n        fused_image = self.fuse(sims, transform_key=transform_key).squeeze()\n        save_image(output_filename, output_params.get('thumbnail'), fused_image, channels=channels,\n                   transform_key=transform_key, params=output_params)\n\n    def calc_overlap_metrics(self, results):\n        nccs = {}\n        ssims = {}\n        sims = results['sims']\n        pairs = results['pairs']\n        if pairs is None:\n            origins = np.array([get_sim_position_final(sim) for sim in sims])\n            size = get_sim_physical_size(sims[0])\n            pairs, _ = get_orthogonal_pairs(origins, size)\n        for pair in pairs:\n            try:\n                # experimental; in case fail to extract overlap images\n                overlap_sims = self.get_overlap_images((sims[pair[0]], sims[pair[1]]), self.reg_transform_key)\n                nccs[pair] = calc_ncc(overlap_sims[0], overlap_sims[1])\n                ssims[pair] = calc_ssim(overlap_sims[0], overlap_sims[1])\n                #frcs[pair] = calc_frc(overlap_sims[0], overlap_sims[1])\n            except Exception as e:\n                logging.exception(e)\n                #logging.warning(f'Failed to calculate resolution metric')\n        return {'ncc': nccs, 'ssim': ssims}\n\n    def get_overlap_images(self, sims, transform_key):\n        # functionality copied from registration.register_pair_of_msims()\n        spatial_dims = si_utils.get_spatial_dims_from_sim(sims[0])\n        overlap_tolerance = {dim: 0.0 for dim in spatial_dims}\n        overlap_sims = []\n        for sim in sims:\n            if 't' in sim.coords.xindexes:\n                # work-around for points error in get_overlap_bboxes()\n                sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n            else:\n                sim1 = sim\n            overlap_sims.append(sim1)\n        lowers, uppers = get_overlap_bboxes(\n            overlap_sims[0],\n            overlap_sims[1],\n            input_transform_key=transform_key,\n            output_transform_key=None,\n            overlap_tolerance=overlap_tolerance,\n        )\n\n        reg_sims_spacing = [\n            si_utils.get_spacing_from_sim(sim) for sim in sims\n        ]\n\n        tol = 1e-6\n        overlaps_sims = [\n            sim.sel(\n                {\n                    # add spacing to include bounding pixels\n                    dim: slice(\n                        lowers[isim][idim] - tol - reg_sims_spacing[isim][dim],\n                        uppers[isim][idim] + tol + reg_sims_spacing[isim][dim],\n                    )\n                    for idim, dim in enumerate(spatial_dims)\n                },\n            )\n            for isim, sim in enumerate(sims)\n        ]\n        overlaps_sims = [sim.squeeze() for sim in overlaps_sims]\n        return overlaps_sims\n\n    def calc_metrics(self, results, labels):\n        mappings0 = results['mappings']\n        mappings = {labels[index]: mapping.data[0].tolist() for index, mapping in mappings0.items()}\n\n        distances = [np.linalg.norm(param_utils.translation_from_affine(mapping.data[0]))\n                     for mapping in mappings0.values()]\n        if len(distances) &gt; 2:\n            # Coefficient of variation\n            cvar = np.std(distances) / np.mean(distances)\n            var = cvar\n        else:\n            size = get_sim_physical_size(results['sims'][0])\n            norm_distance = np.sum(distances) / np.linalg.norm(size)\n            var = norm_distance\n\n        residual_errors = {labels[key[0]] + ' - ' + labels[key[1]]: value\n                           for key, value in results['residual_errors'].items()}\n        if len(residual_errors) &gt; 0:\n            residual_error = np.nanmean(list(residual_errors.values()))\n        else:\n            residual_error = 1\n\n        registration_qualities = {labels[key[0]] + ' - ' + labels[key[1]]: value.item()\n                                  for key, value in results['registration_qualities'].items()}\n        if len(registration_qualities) &gt; 0:\n            registration_quality = np.nanmean(list(registration_qualities.values()))\n        else:\n            registration_quality = 0\n\n        #overlap_metrics = self.calc_overlap_metrics(results)\n\n        #nccs = {labels[key[0]] + ' - ' + labels[key[1]]: value\n        #         for key, value in overlap_metrics['ncc'].items()}\n        #ncc = np.nanmean(list(nccs.values()))\n\n        #ssims = {labels[key[0]] + ' - ' + labels[key[1]]: value\n        #         for key, value in overlap_metrics['ssim'].items()}\n        #ssim = np.nanmean(list(ssims.values()))\n\n        summary = (f'Residual error: {residual_error:.3f}'\n                   f' Registration quality: {registration_quality:.3f}'\n        #           f' NCC: {ncc:.3f}'\n        #           f' SSIM: {ssim:.3f}'\n                   f' Variation: {var:.3f}')\n\n        return {'mappings': mappings,\n                'variation': var,\n                'residual_error': residual_error,\n                'residual_errors': residual_errors,\n                'registration_quality': registration_quality,\n                'registration_qualities': registration_qualities,\n         #       'ncc': ncc,\n         #       'nccs': nccs,\n         #       'ssim': ssim,\n         #       'ssims': ssims,\n                'summary': summary}\n\n    def save_video(self, output, sims, fused_image):\n        logging.info('Creating transition video...')\n        pixel_size = [si_utils.get_spacing_from_sim(sims[0]).get(dim, 1) for dim in 'xy']\n        params = self.params\n        nframes = params.get('frames', 1)\n        spacing = params.get('spacing', [1.1, 1])\n        scale = params.get('scale', 1)\n        transition_filename = output + 'transition'\n        video = Video(transition_filename + '.mp4', fps=params.get('fps', 1))\n        positions0 = np.array([si_utils.get_origin_from_sim(sim, asarray=True) for sim in sims])\n        center = np.mean(positions0, 0)\n        window = get_image_window(fused_image)\n\n        max_size = None\n        acum = 0\n        for framei in range(nframes):\n            c = (1 - np.cos(framei / (nframes - 1) * 2 * math.pi)) / 2\n            acum += c / (nframes / 2)\n            spacing1 = spacing[0] + (spacing[1] - spacing[0]) * acum\n            for sim, position0 in zip(sims, positions0):\n                transform = param_utils.identity_transform(ndim=2, t_coords=[0])\n                transform[0][:2, 2] += (position0 - center) * spacing1\n                si_utils.set_sim_affine(sim, transform, transform_key=self.transition_transform_key)\n            frame = fusion.fuse(sims, transform_key=self.transition_transform_key).squeeze()\n            frame = float2int_image(normalise_values(frame, window[0], window[1]))\n            frame = cv.resize(np.asarray(frame), None, fx=scale, fy=scale)\n            if max_size is None:\n                max_size = frame.shape[1], frame.shape[0]\n                video.size = max_size\n            frame = image_reshape(frame, max_size)\n            save_tiff(transition_filename + f'{framei:04d}.tiff', frame, None, pixel_size)\n            video.write(frame)\n\n        video.close()\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.logging_dask","title":"<code>logging_dask = params_logging.get('dask', False)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.logging_time","title":"<code>logging_time = params_logging.get('time', False)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.mpl_ui","title":"<code>mpl_ui = 'mpl' in self.ui or 'plot' in self.ui</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.napari_ui","title":"<code>napari_ui = 'napari' in self.ui</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.params_general","title":"<code>params_general = params_general</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.reg_transform_key","title":"<code>reg_transform_key = 'registered'</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.source_transform_key","title":"<code>source_transform_key = 'source_metadata'</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.transition_transform_key","title":"<code>transition_transform_key = 'transition'</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.ui","title":"<code>ui = self.params_general.get('ui', '')</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.verbose","title":"<code>verbose = params_logging.get('verbose', False)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistration.MVSRegistration.__init__","title":"<code>__init__(params_general)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def __init__(self, params_general):\n    super().__init__()\n    self.params_general = params_general\n\n    params_logging = self.params_general.get('logging', {})\n    self.verbose = params_logging.get('verbose', False)\n    self.logging_dask = params_logging.get('dask', False)\n    self.logging_time = params_logging.get('time', False)\n    self.ui = self.params_general.get('ui', '')\n    self.mpl_ui = ('mpl' in self.ui or 'plot' in self.ui)\n    self.napari_ui = ('napari' in self.ui)\n    self.source_transform_key = 'source_metadata'\n    self.reg_transform_key = 'registered'\n    self.transition_transform_key = 'transition'\n\n    logging.info(f'Multiview-stitcher version: {multiview_stitcher.__version__}')\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.calc_metrics","title":"<code>calc_metrics(results, labels)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def calc_metrics(self, results, labels):\n    mappings0 = results['mappings']\n    mappings = {labels[index]: mapping.data[0].tolist() for index, mapping in mappings0.items()}\n\n    distances = [np.linalg.norm(param_utils.translation_from_affine(mapping.data[0]))\n                 for mapping in mappings0.values()]\n    if len(distances) &gt; 2:\n        # Coefficient of variation\n        cvar = np.std(distances) / np.mean(distances)\n        var = cvar\n    else:\n        size = get_sim_physical_size(results['sims'][0])\n        norm_distance = np.sum(distances) / np.linalg.norm(size)\n        var = norm_distance\n\n    residual_errors = {labels[key[0]] + ' - ' + labels[key[1]]: value\n                       for key, value in results['residual_errors'].items()}\n    if len(residual_errors) &gt; 0:\n        residual_error = np.nanmean(list(residual_errors.values()))\n    else:\n        residual_error = 1\n\n    registration_qualities = {labels[key[0]] + ' - ' + labels[key[1]]: value.item()\n                              for key, value in results['registration_qualities'].items()}\n    if len(registration_qualities) &gt; 0:\n        registration_quality = np.nanmean(list(registration_qualities.values()))\n    else:\n        registration_quality = 0\n\n    #overlap_metrics = self.calc_overlap_metrics(results)\n\n    #nccs = {labels[key[0]] + ' - ' + labels[key[1]]: value\n    #         for key, value in overlap_metrics['ncc'].items()}\n    #ncc = np.nanmean(list(nccs.values()))\n\n    #ssims = {labels[key[0]] + ' - ' + labels[key[1]]: value\n    #         for key, value in overlap_metrics['ssim'].items()}\n    #ssim = np.nanmean(list(ssims.values()))\n\n    summary = (f'Residual error: {residual_error:.3f}'\n               f' Registration quality: {registration_quality:.3f}'\n    #           f' NCC: {ncc:.3f}'\n    #           f' SSIM: {ssim:.3f}'\n               f' Variation: {var:.3f}')\n\n    return {'mappings': mappings,\n            'variation': var,\n            'residual_error': residual_error,\n            'residual_errors': residual_errors,\n            'registration_quality': registration_quality,\n            'registration_qualities': registration_qualities,\n     #       'ncc': ncc,\n     #       'nccs': nccs,\n     #       'ssim': ssim,\n     #       'ssims': ssims,\n            'summary': summary}\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.calc_overlap_metrics","title":"<code>calc_overlap_metrics(results)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def calc_overlap_metrics(self, results):\n    nccs = {}\n    ssims = {}\n    sims = results['sims']\n    pairs = results['pairs']\n    if pairs is None:\n        origins = np.array([get_sim_position_final(sim) for sim in sims])\n        size = get_sim_physical_size(sims[0])\n        pairs, _ = get_orthogonal_pairs(origins, size)\n    for pair in pairs:\n        try:\n            # experimental; in case fail to extract overlap images\n            overlap_sims = self.get_overlap_images((sims[pair[0]], sims[pair[1]]), self.reg_transform_key)\n            nccs[pair] = calc_ncc(overlap_sims[0], overlap_sims[1])\n            ssims[pair] = calc_ssim(overlap_sims[0], overlap_sims[1])\n            #frcs[pair] = calc_frc(overlap_sims[0], overlap_sims[1])\n        except Exception as e:\n            logging.exception(e)\n            #logging.warning(f'Failed to calculate resolution metric')\n    return {'ncc': nccs, 'ssim': ssims}\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.fuse","title":"<code>fuse(sims, transform_key=None)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def fuse(self, sims, transform_key=None):\n    if transform_key is None:\n        transform_key = self.reg_transform_key\n    operation = self.params['operation']\n    extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n    channels = extra_metadata.get('channels', [])\n    z_scale = extra_metadata.get('scale', {}).get('z')\n    if z_scale is None:\n        if 'z' in sims[0].dims:\n            z_scale = np.min(np.diff(sorted(set([si_utils.get_origin_from_sim(sim).get('z', 0) for sim in sims]))))\n    if not z_scale:\n        z_scale = 1\n\n    is_3d = ('3d' in operation)\n    is_channel_overlay = (len(channels) &gt; 1)\n\n    sim0 = sims[0]\n    source_type = sim0.dtype\n\n    output_stack_properties = calc_output_properties(sims, transform_key, z_scale=z_scale)\n    if is_channel_overlay:\n        # convert to multichannel images\n        if self.verbose:\n            logging.info(f'Output stack: {output_stack_properties}')\n        data_size = np.prod(list(output_stack_properties['shape'].values())) * len(sims) * source_type.itemsize\n        logging.info(f'Fusing channels {print_hbytes(data_size)}')\n\n        channel_sims = [fusion.fuse(\n            [sim],\n            transform_key=transform_key,\n            output_stack_properties=output_stack_properties\n        ) for sim in sims]\n        channel_sims = [sim.assign_coords({'c': [channels[simi]['label']]}) for simi, sim in enumerate(channel_sims)]\n        fused_image = xr.combine_nested([sim.rename() for sim in channel_sims], concat_dim='c', combine_attrs='override')\n    else:\n        if is_3d:\n            z_positions = sorted(set([si_utils.get_origin_from_sim(sim).get('z', 0) for sim in sims]))\n            z_shape = len(z_positions)\n            if z_shape &lt;= 1:\n                z_shape = len(sims)\n            output_stack_properties['shape']['z'] = z_shape\n        if self.verbose:\n            logging.info(f'Output stack: {output_stack_properties}')\n        data_size = np.prod(list(output_stack_properties['shape'].values())) * source_type.itemsize\n        logging.info(f'Fusing {print_hbytes(data_size)}')\n\n        fused_image = fusion.fuse(\n            sims,\n            transform_key=transform_key,\n            output_stack_properties=output_stack_properties,\n            fusion_func=fusion.simple_average_fusion,\n        )\n    return fused_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.get_overlap_images","title":"<code>get_overlap_images(sims, transform_key)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def get_overlap_images(self, sims, transform_key):\n    # functionality copied from registration.register_pair_of_msims()\n    spatial_dims = si_utils.get_spatial_dims_from_sim(sims[0])\n    overlap_tolerance = {dim: 0.0 for dim in spatial_dims}\n    overlap_sims = []\n    for sim in sims:\n        if 't' in sim.coords.xindexes:\n            # work-around for points error in get_overlap_bboxes()\n            sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n        else:\n            sim1 = sim\n        overlap_sims.append(sim1)\n    lowers, uppers = get_overlap_bboxes(\n        overlap_sims[0],\n        overlap_sims[1],\n        input_transform_key=transform_key,\n        output_transform_key=None,\n        overlap_tolerance=overlap_tolerance,\n    )\n\n    reg_sims_spacing = [\n        si_utils.get_spacing_from_sim(sim) for sim in sims\n    ]\n\n    tol = 1e-6\n    overlaps_sims = [\n        sim.sel(\n            {\n                # add spacing to include bounding pixels\n                dim: slice(\n                    lowers[isim][idim] - tol - reg_sims_spacing[isim][dim],\n                    uppers[isim][idim] + tol + reg_sims_spacing[isim][dim],\n                )\n                for idim, dim in enumerate(spatial_dims)\n            },\n        )\n        for isim, sim in enumerate(sims)\n    ]\n    overlaps_sims = [sim.squeeze() for sim in overlaps_sims]\n    return overlaps_sims\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.init_sims","title":"<code>init_sims(target_scale=None)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def init_sims(self, target_scale=None):\n    operation = self.params['operation']\n    source_metadata = import_metadata(self.params.get('source_metadata', 'source'), input_path=self.params['input'])\n    chunk_size = self.params_general.get('chunk_size', [1024, 1024])\n    extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n    z_scale = extra_metadata.get('scale', {}).get('z')\n\n    logging.info('Initialising sims...')\n    sources = [create_dask_source(file, source_metadata) for file in self.filenames]\n    source0 = sources[0]\n    images = []\n    sims = []\n    scales = []\n    translations = []\n    rotations = []\n\n    is_stack = ('stack' in operation)\n    has_z_size = (source0.get_size().get('z', 0) &gt; 0)\n    is_3d = (has_z_size or '3d' in operation)\n    pyramid_level = 0\n\n    output_order = 'zyx' if is_stack or is_3d else 'yx'\n    ndims = len(output_order)\n    if source0.get_nchannels() &gt; 1:\n        output_order += 'c'\n\n    last_z_position = None\n    different_z_positions = False\n    delta_zs = []\n    for filename, source in zip(self.filenames, sources):\n        scale = source.get_pixel_size()\n        translation = source.get_position()\n        rotation = source.get_rotation()\n\n        if target_scale:\n            pyramid_level = np.argmin(abs(np.array(source.scales) - target_scale))\n            pyramid_scale = source.scales[pyramid_level]\n            scale = {dim: size * pyramid_scale if dim in 'xy' else size for dim, size in scale.items()}\n        if 'invert' in source_metadata:\n            translation[0] = -translation[0]\n            translation[1] = -translation[1]\n        if len(translation) &gt;= 3:\n            z_position = translation['z']\n        else:\n            z_position = 0\n        if last_z_position is not None and z_position != last_z_position:\n            different_z_positions = True\n            delta_zs.append(z_position - last_z_position)\n        if self.global_rotation is not None:\n            rotation = self.global_rotation\n\n        dask_data = source.get_data(level=pyramid_level)\n        image = redimension_data(dask_data, source.dimension_order, output_order)\n\n        scales.append(scale)\n        translations.append(translation)\n        rotations.append(rotation)\n        images.append(image)\n        last_z_position = z_position\n\n    if z_scale is None:\n        if len(delta_zs) &gt; 0:\n            z_scale = np.min(delta_zs)\n        else:\n            z_scale = 1\n\n    if 'norm' in source_metadata:\n        size = np.array(source0.get_size()) * source0.get_pixel_size_micrometer()\n        center = None\n        if 'center' in source_metadata:\n            if 'global' in source_metadata:\n                center = self.global_center\n            else:\n                center = np.mean(translations, 0)\n        elif 'origin' in source_metadata:\n            center = np.zeros(ndims)\n        translations, rotations = normalise_rotated_positions(translations, rotations, size, center)\n\n    #translations = [np.array(translation) * 1.25 for translation in translations]\n\n    increase_z_positions = is_stack and not different_z_positions\n    z_position = 0\n    scales2 = []\n    translations2 = []\n    for source, image, scale, translation, rotation, file_label in zip(sources, images, scales, translations, rotations, self.file_labels):\n        # transform #dimensions need to match\n        if len(scale) &gt; 0 and 'z' not in scale:\n            scale['z'] = abs(z_scale)\n        if (len(translation) &gt; 0 and 'z' not in translation) or increase_z_positions:\n            translation['z'] = z_position\n        if increase_z_positions:\n            z_position += z_scale\n        channel_labels = [channel.get('label', '') for channel in source.get_channels()]\n        if rotation is None or 'norm' in source_metadata:\n            # if positions are normalised, don't use rotation\n            transform = None\n        else:\n            transform = param_utils.invert_coordinate_order(\n                create_transform(translation, rotation, matrix_size=ndims + 1)\n            )\n        if file_label in extra_metadata:\n            transform2 = extra_metadata[file_label]\n            if transform is None:\n                transform = np.array(transform2)\n            else:\n                transform = np.array(combine_transforms([transform, transform2]))\n        sim = si_utils.get_sim_from_array(\n            image,\n            dims=list(output_order),\n            scale=scale,\n            translation=translation,\n            affine=transform,\n            transform_key=self.source_transform_key,\n            c_coords=channel_labels\n        )\n        if len(sim.chunksizes.get('x')) == 1 and len(sim.chunksizes.get('y')) == 1:\n            sim = sim.chunk(xyz_to_dict(chunk_size))\n        sims.append(sim)\n        scales2.append(dict_to_xyz(scale))\n        translations2.append(dict_to_xyz(translation))\n    return sims, scales2, translations2, rotations\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.preprocess","title":"<code>preprocess(sims, params)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def preprocess(self, sims, params):\n    flatfield_quantiles = params.get('flatfield_quantiles')\n    normalisation = params.get('normalisation', '')\n    filter_foreground = params.get('filter_foreground', False)\n\n    if filter_foreground:\n        foreground_map = calc_foreground_map(sims)\n    else:\n        foreground_map = None\n    if flatfield_quantiles is not None:\n        logging.info('Flat-field correction...')\n        new_sims = [None] * len(sims)\n        for sim_indices in group_sims_by_z(sims):\n            sims_z_set = [sims[i] for i in sim_indices]\n            foreground_map_z_set = [foreground_map[i] for i in sim_indices] if foreground_map is not None else None\n            new_sims_z_set = flatfield_correction(sims_z_set, self.source_transform_key, flatfield_quantiles,\n                                                  foreground_map=foreground_map_z_set)\n            for sim_index, sim in zip(sim_indices, new_sims_z_set):\n                new_sims[sim_index] = sim\n        sims = new_sims\n\n    if normalisation:\n        use_global = ('global' in normalisation)\n        if use_global:\n            logging.info('Normalising (global)...')\n        else:\n            logging.info('Normalising (individual)...')\n        new_sims = normalise(sims, self.source_transform_key, use_global=use_global)\n    else:\n        new_sims = sims\n\n    if filter_foreground:\n        logging.info('Filtering foreground images...')\n        #tile_vars = np.array([np.asarray(np.std(sim)).item() for sim in sims])\n        #threshold1 = np.mean(tile_vars)\n        #threshold2 = np.median(tile_vars)\n        #threshold3, _ = cv.threshold(np.array(tile_vars).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n        #threshold = min(threshold1, threshold2, threshold3)\n        #foregrounds = (tile_vars &gt;= threshold)\n        new_sims = [sim for sim, is_foreground in zip(new_sims, foreground_map) if is_foreground]\n        logging.info(f'Foreground images: {len(new_sims)} / {len(sims)}')\n        indices = np.where(foreground_map)[0]\n    else:\n        indices = range(len(sims))\n    return sims, new_sims, indices\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.register","title":"<code>register(sims, register_sims, indices, params)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def register(self, sims, register_sims, indices, params):\n    sim0 = sims[0]\n    ndims = si_utils.get_ndim_from_sim(sim0)\n\n    operation = params['operation']\n    reg_params = params.get('method')\n    if isinstance(reg_params, dict):\n        reg_method = reg_params.get('name', '').lower()\n    else:\n        reg_method = reg_params.lower()\n    use_orthogonal_pairs = params.get('use_orthogonal_pairs', False)\n\n    is_stack = ('stack' in operation)\n    is_3d = ('3d' in operation)\n    debug = self.params_general.get('debug', False)\n\n    reg_channel = params.get('channel', 0)\n    if isinstance(reg_channel, int):\n        reg_channel_index = reg_channel\n        reg_channel = None\n    else:\n        reg_channel_index = None\n\n    groupwise_resolution_kwargs = {\n        'transform': params.get('transform_type')  # options include 'translation', 'rigid', 'affine'\n    }\n    pairwise_reg_func_kwargs = None\n    if is_stack and not is_3d:\n        # register in 2d; pairwise consecutive views\n        register_sims = [si_utils.max_project_sim(sim, dim='z') for sim in register_sims]\n        pairs = [(index, index + 1) for index in range(len(register_sims) - 1)]\n    elif use_orthogonal_pairs:\n        origins = np.array([get_sim_position_final(sim) for sim in register_sims])\n        size = get_sim_physical_size(sim0)\n        pairs, _ = get_orthogonal_pairs(origins, size)\n        logging.info(f'#pairs: {len(pairs)}')\n        for pair in pairs:\n            print(f'{self.file_labels[pair[0]]} - {self.file_labels[pair[1]]}')\n    else:\n        pairs = None\n\n    if is_3d:\n        overlap_tolerance = {'z': 1}\n    else:\n        overlap_tolerance = None\n\n    if '3din2d' in reg_method:\n        from src.registration_methods.RegistrationMethodANTs3Din2D import RegistrationMethodANTs3Din2D\n        registration_method = RegistrationMethodANTs3Din2D(sim0, reg_params, debug)\n        pairwise_reg_func = registration_method.registration\n    elif 'cpd' in reg_method:\n        from src.registration_methods.RegistrationMethodCPD import RegistrationMethodCPD\n        registration_method = RegistrationMethodCPD(sim0, reg_params, debug)\n        pairwise_reg_func = registration_method.registration\n    elif 'feature' in reg_method or 'orb' in reg_method or 'sift' in reg_method:\n        if 'cv' in reg_method:\n            from src.registration_methods.RegistrationMethodCvFeatures import RegistrationMethodCvFeatures\n            registration_method = RegistrationMethodCvFeatures(sim0, reg_params, debug)\n        else:\n            from src.registration_methods.RegistrationMethodSkFeatures import RegistrationMethodSkFeatures\n            registration_method = RegistrationMethodSkFeatures(sim0, reg_params, debug)\n        pairwise_reg_func = registration_method.registration\n    elif 'ant' in reg_method:\n        pairwise_reg_func = registration.registration_ANTsPy\n        # args for ANTsPy registration: used internally by ANYsPy algorithm\n        pairwise_reg_func_kwargs = {\n            'transform_types': ['Rigid'],\n            \"aff_random_sampling_rate\": 0.5,\n            \"aff_iterations\": (2000, 2000, 1000, 1000),\n            \"aff_smoothing_sigmas\": (4, 2, 1, 0),\n            \"aff_shrink_factors\": (16, 8, 2, 1),\n        }\n    else:\n        pairwise_reg_func = registration.phase_correlation_registration\n\n    # Pass registration through metrics method\n    #from src.registration_methods.RegistrationMetrics import RegistrationMetrics\n    #registration_metrics = RegistrationMetrics(sim0, pairwise_reg_function)\n    #pairwise_reg_function = registration_metrics.registration\n    # TODO: extract metrics from registration_metrics\n\n    logging.info(f'Registration method: {reg_method}')\n\n    try:\n        logging.info('Registering...')\n        register_msims = [msi_utils.get_msim_from_sim(sim) for sim in register_sims]\n        reg_result = registration.register(\n            register_msims,\n            reg_channel=reg_channel,\n            reg_channel_index=reg_channel_index,\n            transform_key=self.source_transform_key,\n            new_transform_key=self.reg_transform_key,\n\n            pairs=pairs,\n            pre_registration_pruning_method=None,\n\n            pairwise_reg_func=pairwise_reg_func,\n            pairwise_reg_func_kwargs=pairwise_reg_func_kwargs,\n            groupwise_resolution_kwargs=groupwise_resolution_kwargs,\n\n            post_registration_do_quality_filter=True,\n            post_registration_quality_threshold=0.1,\n\n            plot_summary=self.mpl_ui,\n            return_dict=True,\n\n            overlap_tolerance=overlap_tolerance,\n        )\n        # copy transforms from register sims to unmodified sims\n        for reg_msim, index in zip(register_msims, indices):\n            si_utils.set_sim_affine(\n                sims[index],\n                msi_utils.get_transform_from_msim(reg_msim, transform_key=self.reg_transform_key),\n                transform_key=self.reg_transform_key)\n\n        # set missing transforms\n        for sim in sims:\n            if self.reg_transform_key not in si_utils.get_tranform_keys_from_sim(sim):\n                si_utils.set_sim_affine(\n                    sim,\n                    param_utils.identity_transform(ndim=ndims, t_coords=[0]),\n                    transform_key=self.reg_transform_key)\n\n        mappings = reg_result['params']\n        # re-index from subset of sims\n        residual_error_dict = reg_result.get('groupwise_resolution', {}).get('metrics', {}).get('residuals', {})\n        residual_error_dict = {(indices[key[0]], indices[key[1]]): value.item()\n                               for key, value in residual_error_dict.items()}\n        registration_qualities_dict = reg_result.get('pairwise_registration', {}).get('metrics', {}).get('qualities', {})\n        registration_qualities_dict = {(indices[key[0]], indices[key[1]]): value\n                                       for key, value in registration_qualities_dict.items()}\n    except NotEnoughOverlapError:\n        logging.warning('Not enough overlap')\n        reg_result = {}\n        mappings = [param_utils.identity_transform(ndim=ndims, t_coords=[0])] * len(sims)\n        residual_error_dict = {}\n        registration_qualities_dict = {}\n\n    # re-index from subset of sims\n    mappings_dict = {index: mapping for index, mapping in zip(indices, mappings)}\n\n    if is_stack:\n        # set 3D affine transforms from 2D registration params\n        for index, sim in enumerate(sims):\n            # check if already 3D\n            if 4 not in si_utils.get_affine_from_sim(sim, transform_key=self.reg_transform_key).shape:\n                affine_3d = param_utils.identity_transform(ndim=3)\n                affine_3d.loc[{dim: mappings[index].coords[dim] for dim in mappings[index].sel(t=0).dims}] = mappings[index].sel(t=0)\n                si_utils.set_sim_affine(sim, affine_3d, transform_key=self.reg_transform_key)\n\n    return {'reg_result': reg_result,\n            'mappings': mappings_dict,\n            'residual_errors': residual_error_dict,\n            'registration_qualities': registration_qualities_dict,\n            'sims': sims,\n            'pairs': pairs}\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.run_operation","title":"<code>run_operation(fileset_label, filenames, params, global_rotation=None, global_center=None)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def run_operation(self, fileset_label, filenames, params, global_rotation=None, global_center=None):\n    self.fileset_label = fileset_label\n    self.filenames = filenames\n    self.file_labels = get_unique_file_labels(filenames)\n    self.params = params\n    self.global_rotation = global_rotation\n    self.global_center = global_center\n\n    input_dir = os.path.dirname(filenames[0])\n    parts = split_numeric_dict(filenames[0])\n    output_pattern = params['output'].format_map(parts)\n    self.output = os.path.join(input_dir, output_pattern)    # preserve trailing slash: do not use os.path.normpath()\n\n    with ProgressBar(minimum=10, dt=1) if self.logging_dask else nullcontext():\n        return self._run_operation()\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.save_thumbnail","title":"<code>save_thumbnail(output_filename, nom_sims=None, transform_key=None)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def save_thumbnail(self, output_filename, nom_sims=None, transform_key=None):\n    extra_metadata = import_metadata(self.params.get('extra_metadata', {}), input_path=self.params['input'])\n    channels = extra_metadata.get('channels', [])\n    output_params = self.params_general['output']\n    thumbnail_scale = output_params.get('thumbnail_scale', 16)\n    sims = self.init_sims(target_scale=thumbnail_scale)[0]\n\n    if nom_sims is not None:\n        if sims[0].sizes['x'] &gt;= nom_sims[0].sizes['x']:\n            logging.warning('Unable to generate scaled down thumbnail due to lack of source pyramid sizes')\n            return\n\n        if transform_key is not None and transform_key != self.source_transform_key:\n            for nom_sim, sim in zip(nom_sims, sims):\n                si_utils.set_sim_affine(sim,\n                                        si_utils.get_affine_from_sim(nom_sim, transform_key=transform_key),\n                                        transform_key=transform_key)\n    fused_image = self.fuse(sims, transform_key=transform_key).squeeze()\n    save_image(output_filename, output_params.get('thumbnail'), fused_image, channels=channels,\n               transform_key=transform_key, params=output_params)\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.save_video","title":"<code>save_video(output, sims, fused_image)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def save_video(self, output, sims, fused_image):\n    logging.info('Creating transition video...')\n    pixel_size = [si_utils.get_spacing_from_sim(sims[0]).get(dim, 1) for dim in 'xy']\n    params = self.params\n    nframes = params.get('frames', 1)\n    spacing = params.get('spacing', [1.1, 1])\n    scale = params.get('scale', 1)\n    transition_filename = output + 'transition'\n    video = Video(transition_filename + '.mp4', fps=params.get('fps', 1))\n    positions0 = np.array([si_utils.get_origin_from_sim(sim, asarray=True) for sim in sims])\n    center = np.mean(positions0, 0)\n    window = get_image_window(fused_image)\n\n    max_size = None\n    acum = 0\n    for framei in range(nframes):\n        c = (1 - np.cos(framei / (nframes - 1) * 2 * math.pi)) / 2\n        acum += c / (nframes / 2)\n        spacing1 = spacing[0] + (spacing[1] - spacing[0]) * acum\n        for sim, position0 in zip(sims, positions0):\n            transform = param_utils.identity_transform(ndim=2, t_coords=[0])\n            transform[0][:2, 2] += (position0 - center) * spacing1\n            si_utils.set_sim_affine(sim, transform, transform_key=self.transition_transform_key)\n        frame = fusion.fuse(sims, transform_key=self.transition_transform_key).squeeze()\n        frame = float2int_image(normalise_values(frame, window[0], window[1]))\n        frame = cv.resize(np.asarray(frame), None, fx=scale, fy=scale)\n        if max_size is None:\n            max_size = frame.shape[1], frame.shape[0]\n            video.size = max_size\n        frame = image_reshape(frame, max_size)\n        save_tiff(transition_filename + f'{framei:04d}.tiff', frame, None, pixel_size)\n        video.write(frame)\n\n    video.close()\n</code></pre>"},{"location":"references/#src.MVSRegistration.MVSRegistration.validate_overlap","title":"<code>validate_overlap(sims, labels, is_stack=False, expect_large_overlap=False)</code>","text":"Source code in <code>src\\MVSRegistration.py</code> <pre><code>def validate_overlap(self, sims, labels, is_stack=False, expect_large_overlap=False):\n    min_dists = []\n    has_overlaps = []\n    n = len(sims)\n    positions = [get_sim_position_final(sim) for sim in sims]\n    sizes = [np.linalg.norm(get_sim_physical_size(sim)) for sim in sims]\n    for i in range(n):\n        norm_dists = []\n        # check if only single z slices\n        if is_stack:\n            if i + 1 &lt; n:\n                compare_indices = [i + 1]\n            else:\n                compare_indices = []\n        else:\n            compare_indices = range(n)\n        for j in compare_indices:\n            if not j == i:\n                distance = math.dist(positions[i], positions[j])\n                norm_dist = distance / np.mean([sizes[i], sizes[j]])\n                norm_dists.append(norm_dist)\n        if len(norm_dists) &gt; 0:\n            norm_dist = min(norm_dists)\n            min_dists.append(float(norm_dist))\n            if norm_dist &gt;= 1:\n                logging.warning(f'{labels[i]} has no overlap')\n                has_overlaps.append(False)\n            elif expect_large_overlap and norm_dist &gt; 0.5:\n                logging.warning(f'{labels[i]} has small overlap')\n                has_overlaps.append(False)\n            else:\n                has_overlaps.append(True)\n    return min_dists, has_overlaps\n</code></pre>"},{"location":"references/#src.MVSRegistration.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.MVSRegistration.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.MVSRegistration.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.MVSRegistration.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.MVSRegistration.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.MVSRegistration.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.MVSRegistration.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.MVSRegistration.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.MVSRegistration.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.MVSRegistration.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.MVSRegistration.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.MVSRegistration.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.MVSRegistration.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.MVSRegistration.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.MVSRegistration.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.MVSRegistration.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.MVSRegistration.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.MVSRegistration.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.MVSRegistration.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.MVSRegistration.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.MVSRegistration.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.MVSRegistration.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.MVSRegistration.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.MVSRegistration.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.MVSRegistration.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.MVSRegistration.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.MVSRegistration.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.MVSRegistration.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.MVSRegistration.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.MVSRegistration.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.MVSRegistration.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.MVSRegistration.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.MVSRegistration.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.MVSRegistration.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.MVSRegistration.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.MVSRegistration.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.MVSRegistration.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.MVSRegistration.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.MVSRegistration.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.MVSRegistration.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.MVSRegistration.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.MVSRegistration.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.MVSRegistration.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.MVSRegistration.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.MVSRegistration.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.MVSRegistration.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.MVSRegistration.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.MVSRegistration.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.MVSRegistration.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.MVSRegistration.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.MVSRegistration.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.MVSRegistration.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.MVSRegistration.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.MVSRegistration.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.MVSRegistration.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.MVSRegistration.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.MVSRegistration.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.MVSRegistration.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.MVSRegistration.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.MVSRegistration.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.MVSRegistration.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.MVSRegistration.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.MVSRegistration.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.MVSRegistration.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.MVSRegistrationNapari","title":"<code>MVSRegistrationNapari</code>","text":""},{"location":"references/#src.MVSRegistrationNapari.MVSRegistrationNapari","title":"<code>MVSRegistrationNapari</code>","text":"<p>               Bases: <code>MVSRegistration</code>, <code>QObject</code></p> Source code in <code>src\\MVSRegistrationNapari.py</code> <pre><code>class MVSRegistrationNapari(MVSRegistration, QObject):\n    update_napari_signal = Signal(str, list, list)\n\n    def __init__(self, params_general, viewer):\n        super().__init__(params_general)\n        self.viewer = viewer\n        self.update_napari_signal.connect(self.update_napari)\n\n    @Slot(str, list, list)\n    def update_napari(self, layer_name, shapes, labels):\n        if len(shapes) &gt; 0:\n            text = {'string': '{labels}'}\n            features = {'labels': labels}\n            self.viewer.add_shapes(shapes, name=layer_name, text=text, features=features, opacity=0.5)\n            self.viewer.show()\n</code></pre>"},{"location":"references/#src.MVSRegistrationNapari.MVSRegistrationNapari.update_napari_signal","title":"<code>update_napari_signal = Signal(str, list, list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistrationNapari.MVSRegistrationNapari.viewer","title":"<code>viewer = viewer</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.MVSRegistrationNapari.MVSRegistrationNapari.__init__","title":"<code>__init__(params_general, viewer)</code>","text":"Source code in <code>src\\MVSRegistrationNapari.py</code> <pre><code>def __init__(self, params_general, viewer):\n    super().__init__(params_general)\n    self.viewer = viewer\n    self.update_napari_signal.connect(self.update_napari)\n</code></pre>"},{"location":"references/#src.MVSRegistrationNapari.MVSRegistrationNapari.update_napari","title":"<code>update_napari(layer_name, shapes, labels)</code>","text":"Source code in <code>src\\MVSRegistrationNapari.py</code> <pre><code>@Slot(str, list, list)\ndef update_napari(self, layer_name, shapes, labels):\n    if len(shapes) &gt; 0:\n        text = {'string': '{labels}'}\n        features = {'labels': labels}\n        self.viewer.add_shapes(shapes, name=layer_name, text=text, features=features, opacity=0.5)\n        self.viewer.show()\n</code></pre>"},{"location":"references/#src.Pipeline","title":"<code>Pipeline</code>","text":""},{"location":"references/#src.Pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>Thread</code></p> Source code in <code>src\\Pipeline.py</code> <pre><code>class Pipeline(Thread):\n    def __init__(self, params, viewer=None):\n        super().__init__()\n        self.params = params\n        self.viewer = viewer\n\n        self.params_general = params['general']\n        self.init_logging()\n\n        napari_ui = 'napari' in self.params_general.get('ui', '')\n        if napari_ui:\n            from src.MVSRegistrationNapari import MVSRegistrationNapari\n            self.mvs_registration = MVSRegistrationNapari(self.params_general, self.viewer)\n        else:\n            from src.MVSRegistration import MVSRegistration\n            self.mvs_registration = MVSRegistration(self.params_general)\n\n    def init_logging(self):\n        params_logging = self.params_general.get('logging', {})\n        self.log_filename = params_logging.get('filename', 'muvis-align.log')\n        self.verbose = params_logging.get('verbose', False)\n        logging_mvs = params_logging.get('mvs', False)\n        log_format = params_logging.get('format')\n        basepath = os.path.dirname(self.log_filename)\n        if not os.path.exists(basepath):\n            os.makedirs(basepath)\n\n        handlers = [logging.FileHandler(self.log_filename, encoding='utf-8')]\n        if self.verbose:\n            handlers += [logging.StreamHandler()]\n        logging.basicConfig(level=logging.INFO, format=log_format, handlers=handlers, encoding='utf-8')\n\n        # verbose external modules\n        if logging_mvs:\n            # expose multiview_stitcher.registration logger and make more verbose\n            mvsr_logger = logging.getLogger('multiview_stitcher.registration')\n            mvsr_logger.setLevel(logging.INFO)\n            if len(mvsr_logger.handlers) == 0:\n                mvsr_logger.addHandler(logging.StreamHandler())\n        else:\n            # reduce verbose level\n            for module in ['multiview_stitcher', 'multiview_stitcher.registration', 'multiview_stitcher.fusion']:\n                logging.getLogger(module).setLevel(logging.WARNING)\n\n        for module in ['ome_zarr']:\n            logging.getLogger(module).setLevel(logging.WARNING)\n\n        logging.info(f'muvis-align version {version}')\n\n    def run(self):\n        break_on_error = self.params_general.get('break_on_error', False)\n        for operation_params in tqdm(self.params['operations']):\n            error = False\n            input_path = operation_params['input']\n            logging.info(f'Input: {input_path}')\n            try:\n                self.run_operation(operation_params)\n            except Exception as e:\n                logging.exception(f'Error processing: {input_path}')\n                print(f'Error processing: {input_path}: {e}')\n                error = True\n\n            if error and break_on_error:\n                break\n\n        logging.info('Done!')\n\n    def run_operation(self, params):\n        operation = params['operation']\n        use_global_metadata = 'global' in params.get('source_metadata', '')\n        metadata_summary = self.params_general.get('metadata_summary', False)\n\n        filenames = dir_regex(params['input'])\n        filenames = sorted(filenames, key=lambda file: list(find_all_numbers(file)))    # sort first key first\n        if len(filenames) == 0:\n            logging.warning(f'Skipping operation {operation} (no files)')\n            return False\n        elif self.verbose:\n            logging.info(f'# total files: {len(filenames)}')\n\n        operation_parts = operation.split()\n        if 'match' in operation_parts:\n            # check if match label provided\n            index = operation_parts.index('match') + 1\n            if index &lt; len(operation_parts):\n                match_label = operation_parts[index]\n            else:\n                match_label = None\n            matches = {}\n            for filename in filenames:\n                parts = split_numeric_dict(filename)\n                match_value = parts.get(match_label)\n                if match_value is not None:\n                    if match_value.isdecimal():\n                        match_value = int(match_value)\n                    if match_value not in matches:\n                        matches[match_value] = []\n                    matches[match_value].append(filename)\n                if len(matches) == 0:\n                    matches[0] = filenames\n            filesets = []\n            fileset_labels = []\n            for label in sorted(matches):\n                filesets.append(matches[label])\n                fileset_labels.append(f'{match_label}:{label}')\n            logging.info(f'# matched file sets: {len(filesets)}')\n        else:\n            filesets = [filenames]\n            fileset_labels = [get_filetitle(filename) for filename in filenames]\n\n        metadatas = []\n        rotations = []\n        global_center = None\n        if metadata_summary or use_global_metadata:\n            for fileset, fileset_label in zip(filesets, fileset_labels):\n                metadata = get_images_metadata(fileset, params.get('source_metadata'))\n                if metadata_summary:\n                    logging.info(f'File set: {fileset_label} metadata:\\n' + metadata['summary'])\n                metadatas.append(metadata)\n            if use_global_metadata:\n                global_center = np.mean([metadata['center'] for metadata in metadatas], 0)\n                rotations = [metadata['rotation'] for metadata in metadatas]\n                # fix missing rotation values\n                rotations = pd.Series(rotations).interpolate(limit_direction='both').to_numpy()\n\n        ok = False\n        for index, (fileset, fileset_label) in enumerate(zip(filesets, fileset_labels)):\n            if len(filesets) &gt; 1:\n                logging.info(f'File set: {fileset_label}')\n            center = global_center if use_global_metadata else None\n            rotation = rotations[index] if use_global_metadata else None\n            ok |= self.mvs_registration.run_operation(fileset_label, fileset, params,\n                                                      global_center=center, global_rotation=rotation)\n\n        return ok\n</code></pre>"},{"location":"references/#src.Pipeline.Pipeline.mvs_registration","title":"<code>mvs_registration = MVSRegistrationNapari(self.params_general, self.viewer)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Pipeline.Pipeline.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Pipeline.Pipeline.params_general","title":"<code>params_general = params['general']</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Pipeline.Pipeline.viewer","title":"<code>viewer = viewer</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Pipeline.Pipeline.__init__","title":"<code>__init__(params, viewer=None)</code>","text":"Source code in <code>src\\Pipeline.py</code> <pre><code>def __init__(self, params, viewer=None):\n    super().__init__()\n    self.params = params\n    self.viewer = viewer\n\n    self.params_general = params['general']\n    self.init_logging()\n\n    napari_ui = 'napari' in self.params_general.get('ui', '')\n    if napari_ui:\n        from src.MVSRegistrationNapari import MVSRegistrationNapari\n        self.mvs_registration = MVSRegistrationNapari(self.params_general, self.viewer)\n    else:\n        from src.MVSRegistration import MVSRegistration\n        self.mvs_registration = MVSRegistration(self.params_general)\n</code></pre>"},{"location":"references/#src.Pipeline.Pipeline.init_logging","title":"<code>init_logging()</code>","text":"Source code in <code>src\\Pipeline.py</code> <pre><code>def init_logging(self):\n    params_logging = self.params_general.get('logging', {})\n    self.log_filename = params_logging.get('filename', 'muvis-align.log')\n    self.verbose = params_logging.get('verbose', False)\n    logging_mvs = params_logging.get('mvs', False)\n    log_format = params_logging.get('format')\n    basepath = os.path.dirname(self.log_filename)\n    if not os.path.exists(basepath):\n        os.makedirs(basepath)\n\n    handlers = [logging.FileHandler(self.log_filename, encoding='utf-8')]\n    if self.verbose:\n        handlers += [logging.StreamHandler()]\n    logging.basicConfig(level=logging.INFO, format=log_format, handlers=handlers, encoding='utf-8')\n\n    # verbose external modules\n    if logging_mvs:\n        # expose multiview_stitcher.registration logger and make more verbose\n        mvsr_logger = logging.getLogger('multiview_stitcher.registration')\n        mvsr_logger.setLevel(logging.INFO)\n        if len(mvsr_logger.handlers) == 0:\n            mvsr_logger.addHandler(logging.StreamHandler())\n    else:\n        # reduce verbose level\n        for module in ['multiview_stitcher', 'multiview_stitcher.registration', 'multiview_stitcher.fusion']:\n            logging.getLogger(module).setLevel(logging.WARNING)\n\n    for module in ['ome_zarr']:\n        logging.getLogger(module).setLevel(logging.WARNING)\n\n    logging.info(f'muvis-align version {version}')\n</code></pre>"},{"location":"references/#src.Pipeline.Pipeline.run","title":"<code>run()</code>","text":"Source code in <code>src\\Pipeline.py</code> <pre><code>def run(self):\n    break_on_error = self.params_general.get('break_on_error', False)\n    for operation_params in tqdm(self.params['operations']):\n        error = False\n        input_path = operation_params['input']\n        logging.info(f'Input: {input_path}')\n        try:\n            self.run_operation(operation_params)\n        except Exception as e:\n            logging.exception(f'Error processing: {input_path}')\n            print(f'Error processing: {input_path}: {e}')\n            error = True\n\n        if error and break_on_error:\n            break\n\n    logging.info('Done!')\n</code></pre>"},{"location":"references/#src.Pipeline.Pipeline.run_operation","title":"<code>run_operation(params)</code>","text":"Source code in <code>src\\Pipeline.py</code> <pre><code>def run_operation(self, params):\n    operation = params['operation']\n    use_global_metadata = 'global' in params.get('source_metadata', '')\n    metadata_summary = self.params_general.get('metadata_summary', False)\n\n    filenames = dir_regex(params['input'])\n    filenames = sorted(filenames, key=lambda file: list(find_all_numbers(file)))    # sort first key first\n    if len(filenames) == 0:\n        logging.warning(f'Skipping operation {operation} (no files)')\n        return False\n    elif self.verbose:\n        logging.info(f'# total files: {len(filenames)}')\n\n    operation_parts = operation.split()\n    if 'match' in operation_parts:\n        # check if match label provided\n        index = operation_parts.index('match') + 1\n        if index &lt; len(operation_parts):\n            match_label = operation_parts[index]\n        else:\n            match_label = None\n        matches = {}\n        for filename in filenames:\n            parts = split_numeric_dict(filename)\n            match_value = parts.get(match_label)\n            if match_value is not None:\n                if match_value.isdecimal():\n                    match_value = int(match_value)\n                if match_value not in matches:\n                    matches[match_value] = []\n                matches[match_value].append(filename)\n            if len(matches) == 0:\n                matches[0] = filenames\n        filesets = []\n        fileset_labels = []\n        for label in sorted(matches):\n            filesets.append(matches[label])\n            fileset_labels.append(f'{match_label}:{label}')\n        logging.info(f'# matched file sets: {len(filesets)}')\n    else:\n        filesets = [filenames]\n        fileset_labels = [get_filetitle(filename) for filename in filenames]\n\n    metadatas = []\n    rotations = []\n    global_center = None\n    if metadata_summary or use_global_metadata:\n        for fileset, fileset_label in zip(filesets, fileset_labels):\n            metadata = get_images_metadata(fileset, params.get('source_metadata'))\n            if metadata_summary:\n                logging.info(f'File set: {fileset_label} metadata:\\n' + metadata['summary'])\n            metadatas.append(metadata)\n        if use_global_metadata:\n            global_center = np.mean([metadata['center'] for metadata in metadatas], 0)\n            rotations = [metadata['rotation'] for metadata in metadatas]\n            # fix missing rotation values\n            rotations = pd.Series(rotations).interpolate(limit_direction='both').to_numpy()\n\n    ok = False\n    for index, (fileset, fileset_label) in enumerate(zip(filesets, fileset_labels)):\n        if len(filesets) &gt; 1:\n            logging.info(f'File set: {fileset_label}')\n        center = global_center if use_global_metadata else None\n        rotation = rotations[index] if use_global_metadata else None\n        ok |= self.mvs_registration.run_operation(fileset_label, fileset, params,\n                                                  global_center=center, global_rotation=rotation)\n\n    return ok\n</code></pre>"},{"location":"references/#src.Timer","title":"<code>Timer</code>","text":""},{"location":"references/#src.Timer.Timer","title":"<code>Timer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>src\\Timer.py</code> <pre><code>class Timer(object):\n    def __init__(self, title, auto_unit=True, verbose=True):\n        self.title = title\n        self.auto_unit = auto_unit\n        self.verbose = verbose\n\n    def __enter__(self):\n        self.ptime_start = time.process_time()\n        self.time_start = time.time()\n\n    def __exit__(self, type, value, traceback):\n        if self.verbose:\n            ptime_end = time.process_time()\n            time_end = time.time()\n            pelapsed = ptime_end - self.ptime_start\n            elapsed = time_end - self.time_start\n            unit = 'seconds'\n            if self.auto_unit and elapsed &gt;= 60:\n                pelapsed /= 60\n                elapsed /= 60\n                unit = 'minutes'\n                if elapsed &gt;= 60:\n                    pelapsed /= 60\n                    elapsed /= 60\n                    unit = 'hours'\n            logging.info(f'Time {self.title}: {elapsed:.1f} ({pelapsed:.1f}) {unit}')\n</code></pre>"},{"location":"references/#src.Timer.Timer.auto_unit","title":"<code>auto_unit = auto_unit</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Timer.Timer.title","title":"<code>title = title</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Timer.Timer.verbose","title":"<code>verbose = verbose</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.Timer.Timer.__enter__","title":"<code>__enter__()</code>","text":"Source code in <code>src\\Timer.py</code> <pre><code>def __enter__(self):\n    self.ptime_start = time.process_time()\n    self.time_start = time.time()\n</code></pre>"},{"location":"references/#src.Timer.Timer.__exit__","title":"<code>__exit__(type, value, traceback)</code>","text":"Source code in <code>src\\Timer.py</code> <pre><code>def __exit__(self, type, value, traceback):\n    if self.verbose:\n        ptime_end = time.process_time()\n        time_end = time.time()\n        pelapsed = ptime_end - self.ptime_start\n        elapsed = time_end - self.time_start\n        unit = 'seconds'\n        if self.auto_unit and elapsed &gt;= 60:\n            pelapsed /= 60\n            elapsed /= 60\n            unit = 'minutes'\n            if elapsed &gt;= 60:\n                pelapsed /= 60\n                elapsed /= 60\n                unit = 'hours'\n        logging.info(f'Time {self.title}: {elapsed:.1f} ({pelapsed:.1f}) {unit}')\n</code></pre>"},{"location":"references/#src.Timer.Timer.__init__","title":"<code>__init__(title, auto_unit=True, verbose=True)</code>","text":"Source code in <code>src\\Timer.py</code> <pre><code>def __init__(self, title, auto_unit=True, verbose=True):\n    self.title = title\n    self.auto_unit = auto_unit\n    self.verbose = verbose\n</code></pre>"},{"location":"references/#src.constants","title":"<code>constants</code>","text":""},{"location":"references/#src.constants.tiff_extension","title":"<code>tiff_extension = '.ome.tiff'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.constants.version","title":"<code>version = '0.2.10'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.constants.zarr_extension","title":"<code>zarr_extension = '.ome.zarr'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image","title":"<code>image</code>","text":""},{"location":"references/#src.image.DaskSource","title":"<code>DaskSource</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource","title":"<code>DaskSource</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>class DaskSource:\n    default_physical_unit = '\u00b5m'\n\n    def __init__(self, filename, source_metadata=None):\n        self.filename = filename\n        self.dimension_order = ''\n        self.is_rgb = False\n        self.shapes = []\n        self.shape = []\n        self.dtype = None\n        self.pixel_sizes = []\n        self.pixel_size = {}\n        self.scales = []\n        self.positions = []\n        self.position = {}\n        self.rotation = 0\n        self.channels = []\n        self.init_metadata()\n        self.fix_metadata(source_metadata)\n\n    def init_metadata(self):\n        raise NotImplementedError(\"Dask source should implement init_metadata() to initialize metadata\")\n\n    def fix_metadata(self, source_metadata=None):\n        if isinstance(source_metadata, dict):\n            filename_numeric = find_all_numbers(self.filename)\n            filename_dict = {key: int(value) for key, value in split_numeric_dict(self.filename).items()}\n            context = {'filename_numeric': filename_numeric, 'fn': filename_numeric} | filename_dict\n            if 'position' in source_metadata:\n                translation0 = source_metadata['position']\n                if 'x' in translation0:\n                    self.position['x'] = eval_context(translation0, 'x', 0, context)\n                if 'y' in translation0:\n                    self.position['y'] = eval_context(translation0, 'y', 0, context)\n                if 'z' in translation0:\n                    self.position['z'] = eval_context(translation0, 'z', 0, context)\n            if 'scale' in source_metadata:\n                scale0 = source_metadata['scale']\n                if 'x' in scale0:\n                    self.pixel_size['x'] = eval_context(scale0, 'x', 1, context)\n                if 'y' in scale0:\n                    self.pixel_size['y'] = eval_context(scale0, 'y', 1, context)\n                if 'z' in scale0:\n                    self.pixel_size['z'] = eval_context(scale0, 'z', 1, context)\n            if 'rotation' in source_metadata:\n                self.rotation = source_metadata['rotation']\n\n        for shape in self.shapes:\n            scale1 = []\n            for dim in 'xy':\n                index = self.dimension_order.index(dim)\n                scale1.append(self.shape[index] / shape[index])\n            self.scales.append(np.mean(scale1))\n\n    def get_shape(self, level=0):\n        # shape in pixels\n        return self.shapes[level]\n\n    def get_size(self, level=0):\n        # size in pixels\n        return {dim: size for dim, size in zip(self.dimension_order, self.get_shape(level))}\n\n    def get_pixel_size(self, level=0):\n        # pixel size in micrometers\n        if self.pixel_sizes:\n            pixel_size = get_value_units_micrometer(self.pixel_sizes[level])\n        else:\n            scale = self.scales[level]\n            pixel_size0 = get_value_units_micrometer(self.pixel_size)\n            pixel_size = {dim: size * scale for dim, size in pixel_size0.items()}\n        return pixel_size\n\n    def get_physical_size(self):\n        pixel_size = self.get_pixel_size()\n        size = self.get_size()\n        return {dim: size[dim] * pixel_size[dim] for dim in pixel_size.keys() if dim in size}\n\n    def get_position(self, level=0):\n        # position in micrometers\n        if self.positions:\n            return get_value_units_micrometer(self.positions[level])\n        else:\n            return get_value_units_micrometer(self.position)\n\n    def get_rotation(self):\n        # rotation in degrees\n        return self.rotation\n\n    def get_nchannels(self):\n        return self.get_size().get('c', 1)\n\n    def get_channels(self):\n        if len(self.channels) == 0:\n            if self.is_rgb:\n                return [{'label': ''}]\n            else:\n                return [{'label': ''}] * self.get_nchannels()\n        return self.channels\n\n    def get_data(self, level=0):\n        raise NotImplementedError()\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.channels","title":"<code>channels = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.default_physical_unit","title":"<code>default_physical_unit = '\u00b5m'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.dimension_order","title":"<code>dimension_order = ''</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.dtype","title":"<code>dtype = None</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.filename","title":"<code>filename = filename</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.is_rgb","title":"<code>is_rgb = False</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.pixel_size","title":"<code>pixel_size = {}</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.pixel_sizes","title":"<code>pixel_sizes = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.position","title":"<code>position = {}</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.positions","title":"<code>positions = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.rotation","title":"<code>rotation = 0</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.scales","title":"<code>scales = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.shape","title":"<code>shape = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.shapes","title":"<code>shapes = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.DaskSource.DaskSource.__init__","title":"<code>__init__(filename, source_metadata=None)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def __init__(self, filename, source_metadata=None):\n    self.filename = filename\n    self.dimension_order = ''\n    self.is_rgb = False\n    self.shapes = []\n    self.shape = []\n    self.dtype = None\n    self.pixel_sizes = []\n    self.pixel_size = {}\n    self.scales = []\n    self.positions = []\n    self.position = {}\n    self.rotation = 0\n    self.channels = []\n    self.init_metadata()\n    self.fix_metadata(source_metadata)\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.fix_metadata","title":"<code>fix_metadata(source_metadata=None)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def fix_metadata(self, source_metadata=None):\n    if isinstance(source_metadata, dict):\n        filename_numeric = find_all_numbers(self.filename)\n        filename_dict = {key: int(value) for key, value in split_numeric_dict(self.filename).items()}\n        context = {'filename_numeric': filename_numeric, 'fn': filename_numeric} | filename_dict\n        if 'position' in source_metadata:\n            translation0 = source_metadata['position']\n            if 'x' in translation0:\n                self.position['x'] = eval_context(translation0, 'x', 0, context)\n            if 'y' in translation0:\n                self.position['y'] = eval_context(translation0, 'y', 0, context)\n            if 'z' in translation0:\n                self.position['z'] = eval_context(translation0, 'z', 0, context)\n        if 'scale' in source_metadata:\n            scale0 = source_metadata['scale']\n            if 'x' in scale0:\n                self.pixel_size['x'] = eval_context(scale0, 'x', 1, context)\n            if 'y' in scale0:\n                self.pixel_size['y'] = eval_context(scale0, 'y', 1, context)\n            if 'z' in scale0:\n                self.pixel_size['z'] = eval_context(scale0, 'z', 1, context)\n        if 'rotation' in source_metadata:\n            self.rotation = source_metadata['rotation']\n\n    for shape in self.shapes:\n        scale1 = []\n        for dim in 'xy':\n            index = self.dimension_order.index(dim)\n            scale1.append(self.shape[index] / shape[index])\n        self.scales.append(np.mean(scale1))\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_channels","title":"<code>get_channels()</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_channels(self):\n    if len(self.channels) == 0:\n        if self.is_rgb:\n            return [{'label': ''}]\n        else:\n            return [{'label': ''}] * self.get_nchannels()\n    return self.channels\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_data","title":"<code>get_data(level=0)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_data(self, level=0):\n    raise NotImplementedError()\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_nchannels","title":"<code>get_nchannels()</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_nchannels(self):\n    return self.get_size().get('c', 1)\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_physical_size","title":"<code>get_physical_size()</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_physical_size(self):\n    pixel_size = self.get_pixel_size()\n    size = self.get_size()\n    return {dim: size[dim] * pixel_size[dim] for dim in pixel_size.keys() if dim in size}\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_pixel_size","title":"<code>get_pixel_size(level=0)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_pixel_size(self, level=0):\n    # pixel size in micrometers\n    if self.pixel_sizes:\n        pixel_size = get_value_units_micrometer(self.pixel_sizes[level])\n    else:\n        scale = self.scales[level]\n        pixel_size0 = get_value_units_micrometer(self.pixel_size)\n        pixel_size = {dim: size * scale for dim, size in pixel_size0.items()}\n    return pixel_size\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_position","title":"<code>get_position(level=0)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_position(self, level=0):\n    # position in micrometers\n    if self.positions:\n        return get_value_units_micrometer(self.positions[level])\n    else:\n        return get_value_units_micrometer(self.position)\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_rotation","title":"<code>get_rotation()</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_rotation(self):\n    # rotation in degrees\n    return self.rotation\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_shape","title":"<code>get_shape(level=0)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_shape(self, level=0):\n    # shape in pixels\n    return self.shapes[level]\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.get_size","title":"<code>get_size(level=0)</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def get_size(self, level=0):\n    # size in pixels\n    return {dim: size for dim, size in zip(self.dimension_order, self.get_shape(level))}\n</code></pre>"},{"location":"references/#src.image.DaskSource.DaskSource.init_metadata","title":"<code>init_metadata()</code>","text":"Source code in <code>src\\image\\DaskSource.py</code> <pre><code>def init_metadata(self):\n    raise NotImplementedError(\"Dask source should implement init_metadata() to initialize metadata\")\n</code></pre>"},{"location":"references/#src.image.TiffDaskSource","title":"<code>TiffDaskSource</code>","text":""},{"location":"references/#src.image.TiffDaskSource.TiffDaskSource","title":"<code>TiffDaskSource</code>","text":"<p>               Bases: <code>DaskSource</code></p> Source code in <code>src\\image\\TiffDaskSource.py</code> <pre><code>class TiffDaskSource(DaskSource):\n    def init_metadata(self):\n        tiff = tifffile.TiffFile(self.filename)\n        pages = []\n        if tiff.series and not tiff.is_mmstack:\n            for level in tiff.series[0].levels:\n                pages.append(level.pages[0])\n        if len(pages) == 0:\n            pages = tiff.pages\n        page0 = pages[0]\n        self.shapes = [page.shape for page in pages]\n        self.shape = self.shapes[0]\n        self.dtype = page0.dtype.type\n        self.dimension_order = page0.axes.lower()\n        photometric = page0.keyframe.photometric\n        nchannels = self.get_nchannels()\n        self.is_rgb = (photometric in (PHOTOMETRIC.RGB, PHOTOMETRIC.PALETTE) and nchannels in (3, 4))\n\n        pixel_size = {}\n        position = {}\n        rotation = None\n        channels = []\n        if tiff.is_ome and tiff.ome_metadata is not None:\n            xml_metadata = tiff.ome_metadata\n            self.metadata = tifffile.xml2dict(xml_metadata)\n            if 'OME' in self.metadata:\n                self.metadata = self.metadata['OME']\n\n                images = ensure_list(self.metadata.get('Image', {}))[0]\n                pixels = images.get('Pixels', {})\n                size = float(pixels.get('PhysicalSizeX', 0))\n                if size:\n                    pixel_size['x'] = (size, pixels.get('PhysicalSizeXUnit', self.default_physical_unit))\n                size = float(pixels.get('PhysicalSizeY', 0))\n                if size:\n                    pixel_size['y'] = (size, pixels.get('PhysicalSizeYUnit', self.default_physical_unit))\n                size = float(pixels.get('PhysicalSizeZ', 0))\n                if size:\n                    pixel_size['z'] =  (size, pixels.get('PhysicalSizeZUnit', self.default_physical_unit))\n\n                for plane in ensure_list(pixels.get('Plane', [])):\n                    if 'PositionX' in plane:\n                        position['x'] = (float(plane.get('PositionX')), plane.get('PositionXUnit', self.default_physical_unit))\n                    if 'PositionY' in plane:\n                        position['y'] = (float(plane.get('PositionY')), plane.get('PositionYUnit', self.default_physical_unit))\n                    if 'PositionZ' in plane:\n                        position['z'] = (float(plane.get('PositionZ')), plane.get('PositionZUnit', self.default_physical_unit))\n                    # c, z, t = plane.get('TheC'), plane.get('TheZ'), plane.get('TheT')\n\n                annotations = self.metadata.get('StructuredAnnotations')\n                if annotations is not None:\n                    if not isinstance(annotations, (list, tuple)):\n                        annotations = [annotations]\n                    for annotation_item in annotations:\n                        for annotations2 in annotation_item.values():\n                            if not isinstance(annotations2, (list, tuple)):\n                                annotations2 = [annotations2]\n                            for annotation in annotations2:\n                                value = annotation.get('Value')\n                                unit = None\n                                if isinstance(value, dict) and 'Modulo' in value:\n                                    modulo = value.get('Modulo', {}).get('ModuloAlongZ', {})\n                                    unit = modulo.get('Unit')\n                                    value = modulo.get('Label')\n                                elif isinstance(value, str) and value.lower().startswith('angle'):\n                                    if ':' in value:\n                                        value = value.split(':')[1].split()\n                                    elif '=' in value:\n                                        value = value.split('=')[1].split()\n                                    else:\n                                        value = value.split()[1:]\n                                    if len(value) &gt;= 2:\n                                        unit = value[1]\n                                    value = value[0]\n                                else:\n                                    value = None\n                                if value is not None:\n                                    rotation = float(value)\n                                    if 'rad' in unit.lower():\n                                        rotation = np.rad2deg(rotation)\n\n                for channel0 in ensure_list(pixels.get('Channel', [])):\n                    channel = {'label': channel0.get('Name', '')}\n                    color = channel0.get('Color')\n                    if color:\n                        channel['color'] = int_to_rgba(int(color))\n                    channels.append(channel)\n        self.pixel_size = pixel_size\n        self.position = position\n        self.rotation = rotation\n        self.channels = channels\n\n    def get_data(self, level=0):\n        lazy_array = dask.delayed(tifffile.imread)(self.filename, level=level)\n        dask_data = dask.array.from_delayed(lazy_array, shape=self.shapes[level], dtype=self.dtype)\n        return dask_data\n</code></pre>"},{"location":"references/#src.image.TiffDaskSource.TiffDaskSource.get_data","title":"<code>get_data(level=0)</code>","text":"Source code in <code>src\\image\\TiffDaskSource.py</code> <pre><code>def get_data(self, level=0):\n    lazy_array = dask.delayed(tifffile.imread)(self.filename, level=level)\n    dask_data = dask.array.from_delayed(lazy_array, shape=self.shapes[level], dtype=self.dtype)\n    return dask_data\n</code></pre>"},{"location":"references/#src.image.TiffDaskSource.TiffDaskSource.init_metadata","title":"<code>init_metadata()</code>","text":"Source code in <code>src\\image\\TiffDaskSource.py</code> <pre><code>def init_metadata(self):\n    tiff = tifffile.TiffFile(self.filename)\n    pages = []\n    if tiff.series and not tiff.is_mmstack:\n        for level in tiff.series[0].levels:\n            pages.append(level.pages[0])\n    if len(pages) == 0:\n        pages = tiff.pages\n    page0 = pages[0]\n    self.shapes = [page.shape for page in pages]\n    self.shape = self.shapes[0]\n    self.dtype = page0.dtype.type\n    self.dimension_order = page0.axes.lower()\n    photometric = page0.keyframe.photometric\n    nchannels = self.get_nchannels()\n    self.is_rgb = (photometric in (PHOTOMETRIC.RGB, PHOTOMETRIC.PALETTE) and nchannels in (3, 4))\n\n    pixel_size = {}\n    position = {}\n    rotation = None\n    channels = []\n    if tiff.is_ome and tiff.ome_metadata is not None:\n        xml_metadata = tiff.ome_metadata\n        self.metadata = tifffile.xml2dict(xml_metadata)\n        if 'OME' in self.metadata:\n            self.metadata = self.metadata['OME']\n\n            images = ensure_list(self.metadata.get('Image', {}))[0]\n            pixels = images.get('Pixels', {})\n            size = float(pixels.get('PhysicalSizeX', 0))\n            if size:\n                pixel_size['x'] = (size, pixels.get('PhysicalSizeXUnit', self.default_physical_unit))\n            size = float(pixels.get('PhysicalSizeY', 0))\n            if size:\n                pixel_size['y'] = (size, pixels.get('PhysicalSizeYUnit', self.default_physical_unit))\n            size = float(pixels.get('PhysicalSizeZ', 0))\n            if size:\n                pixel_size['z'] =  (size, pixels.get('PhysicalSizeZUnit', self.default_physical_unit))\n\n            for plane in ensure_list(pixels.get('Plane', [])):\n                if 'PositionX' in plane:\n                    position['x'] = (float(plane.get('PositionX')), plane.get('PositionXUnit', self.default_physical_unit))\n                if 'PositionY' in plane:\n                    position['y'] = (float(plane.get('PositionY')), plane.get('PositionYUnit', self.default_physical_unit))\n                if 'PositionZ' in plane:\n                    position['z'] = (float(plane.get('PositionZ')), plane.get('PositionZUnit', self.default_physical_unit))\n                # c, z, t = plane.get('TheC'), plane.get('TheZ'), plane.get('TheT')\n\n            annotations = self.metadata.get('StructuredAnnotations')\n            if annotations is not None:\n                if not isinstance(annotations, (list, tuple)):\n                    annotations = [annotations]\n                for annotation_item in annotations:\n                    for annotations2 in annotation_item.values():\n                        if not isinstance(annotations2, (list, tuple)):\n                            annotations2 = [annotations2]\n                        for annotation in annotations2:\n                            value = annotation.get('Value')\n                            unit = None\n                            if isinstance(value, dict) and 'Modulo' in value:\n                                modulo = value.get('Modulo', {}).get('ModuloAlongZ', {})\n                                unit = modulo.get('Unit')\n                                value = modulo.get('Label')\n                            elif isinstance(value, str) and value.lower().startswith('angle'):\n                                if ':' in value:\n                                    value = value.split(':')[1].split()\n                                elif '=' in value:\n                                    value = value.split('=')[1].split()\n                                else:\n                                    value = value.split()[1:]\n                                if len(value) &gt;= 2:\n                                    unit = value[1]\n                                value = value[0]\n                            else:\n                                value = None\n                            if value is not None:\n                                rotation = float(value)\n                                if 'rad' in unit.lower():\n                                    rotation = np.rad2deg(rotation)\n\n            for channel0 in ensure_list(pixels.get('Channel', [])):\n                channel = {'label': channel0.get('Name', '')}\n                color = channel0.get('Color')\n                if color:\n                    channel['color'] = int_to_rgba(int(color))\n                channels.append(channel)\n    self.pixel_size = pixel_size\n    self.position = position\n    self.rotation = rotation\n    self.channels = channels\n</code></pre>"},{"location":"references/#src.image.Video","title":"<code>Video</code>","text":""},{"location":"references/#src.image.Video.Video","title":"<code>Video</code>","text":"Source code in <code>src\\image\\Video.py</code> <pre><code>class Video:\n    def __init__(self, filename, fps=1, scale=1, size=None):\n        self.filename = filename\n        self.fps = fps\n        self.scale = scale\n        self.size = size\n        self.min = 0\n        self.range = 1\n        self.vidwriter = None\n\n    def write(self, frame):\n        frame = frame.squeeze()\n        if self.scale != 1:\n            frame = cv.resize(np.asarray(frame), None, fx=self.scale, fy=self.scale)\n        if self.vidwriter is None:\n            if self.size is None:\n                height, width = frame.shape[:2]\n                self.size = (width, height)\n            self.vidwriter = cv.VideoWriter(self.filename, -1, self.fps, self.size)\n        frame = image_reshape(frame, self.size)\n        self.vidwriter.write(np.asarray(frame))\n\n    def close(self):\n        if self.vidwriter is not None:\n            self.vidwriter.release()\n</code></pre>"},{"location":"references/#src.image.Video.Video.filename","title":"<code>filename = filename</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.fps","title":"<code>fps = fps</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.min","title":"<code>min = 0</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.range","title":"<code>range = 1</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.scale","title":"<code>scale = scale</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.size","title":"<code>size = size</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.vidwriter","title":"<code>vidwriter = None</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.image.Video.Video.__init__","title":"<code>__init__(filename, fps=1, scale=1, size=None)</code>","text":"Source code in <code>src\\image\\Video.py</code> <pre><code>def __init__(self, filename, fps=1, scale=1, size=None):\n    self.filename = filename\n    self.fps = fps\n    self.scale = scale\n    self.size = size\n    self.min = 0\n    self.range = 1\n    self.vidwriter = None\n</code></pre>"},{"location":"references/#src.image.Video.Video.close","title":"<code>close()</code>","text":"Source code in <code>src\\image\\Video.py</code> <pre><code>def close(self):\n    if self.vidwriter is not None:\n        self.vidwriter.release()\n</code></pre>"},{"location":"references/#src.image.Video.Video.write","title":"<code>write(frame)</code>","text":"Source code in <code>src\\image\\Video.py</code> <pre><code>def write(self, frame):\n    frame = frame.squeeze()\n    if self.scale != 1:\n        frame = cv.resize(np.asarray(frame), None, fx=self.scale, fy=self.scale)\n    if self.vidwriter is None:\n        if self.size is None:\n            height, width = frame.shape[:2]\n            self.size = (width, height)\n        self.vidwriter = cv.VideoWriter(self.filename, -1, self.fps, self.size)\n    frame = image_reshape(frame, self.size)\n    self.vidwriter.write(np.asarray(frame))\n</code></pre>"},{"location":"references/#src.image.Video.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.image.Video.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.Video.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.image.Video.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.image.Video.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.image.Video.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.image.Video.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.image.Video.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.image.Video.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.image.Video.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.Video.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.image.Video.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.Video.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.image.Video.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.image.Video.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.image.Video.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.image.Video.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.image.Video.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.image.Video.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.image.Video.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.image.Video.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.image.Video.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.image.Video.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.Video.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.image.Video.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.Video.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.image.Video.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.image.Video.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.image.Video.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.image.Video.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.image.Video.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.image.Video.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.image.Video.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.image.Video.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.image.Video.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.image.Video.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.image.Video.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.Video.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.image.Video.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.image.Video.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.image.Video.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.image.Video.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.image.Video.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.image.Video.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.image.Video.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.image.Video.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.image.Video.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.image.Video.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.image.Video.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.image.Video.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.image.Video.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.image.Video.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.image.Video.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.image.Video.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.image.Video.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.image.Video.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.image.Video.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.image.Video.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.image.Video.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.image.Video.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.image.Video.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.image.Video.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.Video.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.image.Video.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.image.Video.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.Video.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.Video.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.Video.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.image.Video.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.Video.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.Video.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.Video.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.image.Video.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.image.Video.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.image.Video.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.image.Video.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.image.Video.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.image.Video.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.Video.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.image.Video.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.image.Video.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.image.Video.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.image.Video.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.image.Video.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.image.Video.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.image.Video.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.image.Video.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.image.Video.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.Video.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.Video.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.image.Video.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.image.Video.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.image.Video.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.image.Video.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.image.ZarrDaskSource","title":"<code>ZarrDaskSource</code>","text":""},{"location":"references/#src.image.ZarrDaskSource.ZarrDaskSource","title":"<code>ZarrDaskSource</code>","text":"<p>               Bases: <code>DaskSource</code></p> Source code in <code>src\\image\\ZarrDaskSource.py</code> <pre><code>class ZarrDaskSource(DaskSource):\n    def init_metadata(self):\n        location = parse_url(self.filename)\n        if location is None:\n            raise FileNotFoundError(f'Error parsing ome-zarr file {self.filename}')\n        if 'bioformats2raw.layout' in location.root_attrs:\n            location = parse_url(os.path.join(self.filename, '0'))\n            if location is None:\n                raise FileNotFoundError(f'Error parsing ome-zarr file {self.filename}')\n        reader = Reader(location)\n        nodes = list(reader())\n        image_node = nodes[0]\n        self.data = image_node.data\n        self.metadata = image_node.metadata\n\n        self.shapes = [level.shape for level in self.data]\n        self.shape = self.shapes[0]\n        self.dtype = self.data[0].dtype\n        axes = self.metadata['axes']\n        self.dimension_order = ''.join([axis['name'] for axis in axes])\n        units = {axis['name']: axis['unit'] for axis in axes if 'unit' in axis}\n\n        pixel_sizes = []\n        positions = []\n        channels = []\n        for transforms in self.metadata.get('coordinateTransformations', []):\n            pixel_size = {}\n            position = {}\n            scale1 = []\n            position1 = None\n            for transform in transforms:\n                if transform['type'] == 'scale':\n                    scale1 = transform['scale']\n                if transform['type'] == 'translation':\n                    position1 = transform.get('translation')\n            for index, dim in enumerate(self.dimension_order):\n                if dim in 'xyz':\n                    pixel_size[dim] = convert_to_um(scale1[index], units.get(dim, ''))\n                    if position1 is not None:\n                        position[dim] = (position1[index], units.get(dim, ''))\n                    else:\n                        position[dim] = 0\n            pixel_sizes.append(pixel_size)\n            positions.append(position)\n\n        colormaps = self.metadata['colormap']\n        for channeli, channel0 in enumerate(self.metadata['channel_names']):\n            channel = {'label': channel0}\n            if channeli &lt; len(colormaps):\n                channel['color'] = colormaps[channeli][-1]\n            channels.append(channel)\n        self.pixel_sizes = pixel_sizes\n        self.positions = positions\n        self.rotation = 0\n        self.channels = channels\n\n    def get_data(self, level=0):\n        return self.data[level]\n</code></pre>"},{"location":"references/#src.image.ZarrDaskSource.ZarrDaskSource.get_data","title":"<code>get_data(level=0)</code>","text":"Source code in <code>src\\image\\ZarrDaskSource.py</code> <pre><code>def get_data(self, level=0):\n    return self.data[level]\n</code></pre>"},{"location":"references/#src.image.ZarrDaskSource.ZarrDaskSource.init_metadata","title":"<code>init_metadata()</code>","text":"Source code in <code>src\\image\\ZarrDaskSource.py</code> <pre><code>def init_metadata(self):\n    location = parse_url(self.filename)\n    if location is None:\n        raise FileNotFoundError(f'Error parsing ome-zarr file {self.filename}')\n    if 'bioformats2raw.layout' in location.root_attrs:\n        location = parse_url(os.path.join(self.filename, '0'))\n        if location is None:\n            raise FileNotFoundError(f'Error parsing ome-zarr file {self.filename}')\n    reader = Reader(location)\n    nodes = list(reader())\n    image_node = nodes[0]\n    self.data = image_node.data\n    self.metadata = image_node.metadata\n\n    self.shapes = [level.shape for level in self.data]\n    self.shape = self.shapes[0]\n    self.dtype = self.data[0].dtype\n    axes = self.metadata['axes']\n    self.dimension_order = ''.join([axis['name'] for axis in axes])\n    units = {axis['name']: axis['unit'] for axis in axes if 'unit' in axis}\n\n    pixel_sizes = []\n    positions = []\n    channels = []\n    for transforms in self.metadata.get('coordinateTransformations', []):\n        pixel_size = {}\n        position = {}\n        scale1 = []\n        position1 = None\n        for transform in transforms:\n            if transform['type'] == 'scale':\n                scale1 = transform['scale']\n            if transform['type'] == 'translation':\n                position1 = transform.get('translation')\n        for index, dim in enumerate(self.dimension_order):\n            if dim in 'xyz':\n                pixel_size[dim] = convert_to_um(scale1[index], units.get(dim, ''))\n                if position1 is not None:\n                    position[dim] = (position1[index], units.get(dim, ''))\n                else:\n                    position[dim] = 0\n        pixel_sizes.append(pixel_size)\n        positions.append(position)\n\n    colormaps = self.metadata['colormap']\n    for channeli, channel0 in enumerate(self.metadata['channel_names']):\n        channel = {'label': channel0}\n        if channeli &lt; len(colormaps):\n            channel['color'] = colormaps[channeli][-1]\n        channels.append(channel)\n    self.pixel_sizes = pixel_sizes\n    self.positions = positions\n    self.rotation = 0\n    self.channels = channels\n</code></pre>"},{"location":"references/#src.image.color_conversion","title":"<code>color_conversion</code>","text":""},{"location":"references/#src.image.color_conversion.hexrgb_to_rgba","title":"<code>hexrgb_to_rgba(hexrgb)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def hexrgb_to_rgba(hexrgb: str) -&gt; list:\n    rgba = int_to_rgba(eval('0x' + hexrgb + 'FF'))\n    return rgba\n</code></pre>"},{"location":"references/#src.image.color_conversion.int_to_rgba","title":"<code>int_to_rgba(intrgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def int_to_rgba(intrgba: int) -&gt; list:\n    signed = (intrgba &lt; 0)\n    rgba = [x / 255 for x in intrgba.to_bytes(4, signed=signed, byteorder=\"big\")]\n    if rgba[-1] == 0:\n        rgba[-1] = 1\n    return rgba\n</code></pre>"},{"location":"references/#src.image.color_conversion.rgba_to_hexrgb","title":"<code>rgba_to_hexrgb(rgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def rgba_to_hexrgb(rgba: tuple|list) -&gt; str:\n    hexrgb = ''.join([hex(int(x * 255))[2:].upper().zfill(2) for x in rgba[:3]])\n    return hexrgb\n</code></pre>"},{"location":"references/#src.image.color_conversion.rgba_to_int","title":"<code>rgba_to_int(rgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def rgba_to_int(rgba: tuple|list) -&gt; int:\n    intrgba = int.from_bytes([int(x * 255) for x in rgba], signed=True, byteorder=\"big\")\n    return intrgba\n</code></pre>"},{"location":"references/#src.image.flatfield","title":"<code>flatfield</code>","text":""},{"location":"references/#src.image.flatfield.apply_flatfield_correction","title":"<code>apply_flatfield_correction(sims, transform_key, quantiles, quantile_images)</code>","text":"Source code in <code>src\\image\\flatfield.py</code> <pre><code>def apply_flatfield_correction(sims, transform_key, quantiles, quantile_images):\n    new_sims = []\n    sim0 = sims[0]\n    dims0 = sim0.dims\n    has_c_dim = 'c' in dims0\n    dtype = sim0.dtype\n    dark = 0\n    bright = 1\n    for quantile, quantile_image in zip(quantiles, quantile_images):\n        if has_c_dim and dims0.index('c') != -1:\n            quantile_image = da.moveaxis(quantile_image, dims0.index('c'), -1)\n        if quantile &lt;= 0.5:\n            dark = quantile_image\n        else:\n            bright = quantile_image\n\n    bright_dark_range = bright - dark\n    if has_c_dim:\n        axes = list(range(len(dims0) - 1))   # all accept final 'c' axis\n    else:\n        axes = None\n    mean_bright_dark = np.array(np.mean(bright - dark, axis=axes))\n\n    for sim in sims:\n        if has_c_dim:\n            image0 = sim.transpose(..., 'c')\n        else:\n            image0 = sim\n        image = float2int_image(image_flatfield_correction(int2float_image(image0), dark, bright_dark_range, mean_bright_dark), dtype)\n        if has_c_dim:\n            image = image.transpose(*dims0)     # revert to original order\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.image.flatfield.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.image.flatfield.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.flatfield.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_flatfield_images","title":"<code>calc_flatfield_images(sims, quantiles, foreground_map=None)</code>","text":"Source code in <code>src\\image\\flatfield.py</code> <pre><code>def calc_flatfield_images(sims, quantiles, foreground_map=None):\n    if foreground_map is not None:\n        back_sims = [sim for sim, is_foreground in zip(sims, foreground_map) if not is_foreground]\n    else:\n        back_sims = sims\n    dtype = sims[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    flatfield_images = [image.astype(np.float32) / np.float32(maxval)\n                        for image in da.quantile(da.asarray(back_sims), quantiles, axis=0)]\n    return flatfield_images\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.image.flatfield.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.image.flatfield.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.image.flatfield.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.flatfield.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.image.flatfield.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.flatfield.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.image.flatfield.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.image.flatfield.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.image.flatfield.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.image.flatfield.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.image.flatfield.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.image.flatfield.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.image.flatfield.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.image.flatfield.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.image.flatfield.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.image.flatfield.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.flatfield.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.image.flatfield.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.flatfield.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.image.flatfield.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.image.flatfield.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.image.flatfield.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.image.flatfield.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.image.flatfield.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.image.flatfield.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.image.flatfield.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.image.flatfield.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.image.flatfield.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.image.flatfield.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.image.flatfield.flatfield_correction","title":"<code>flatfield_correction(sims, transform_key, quantiles, foreground_map=None, cache_location=None)</code>","text":"Source code in <code>src\\image\\flatfield.py</code> <pre><code>def flatfield_correction(sims, transform_key, quantiles, foreground_map=None, cache_location=None):\n    quantile_images = []\n    if cache_location is not None:\n        for quantile in quantiles:\n            filename = get_quantile_filename(cache_location, quantile)\n            if os.path.exists(filename):\n                quantile_images.append(load_tiff(filename))\n\n    if len(quantile_images) &lt; len(quantiles):\n        quantile_images = calc_flatfield_images(sims, quantiles, foreground_map)\n        if cache_location is not None:\n            for quantile, quantile_image in zip(quantiles, quantile_images):\n                filename = get_quantile_filename(cache_location, quantile)\n                save_tiff(filename, quantile_image)\n\n    return apply_flatfield_correction(sims, transform_key, quantiles, quantile_images)\n</code></pre>"},{"location":"references/#src.image.flatfield.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.flatfield.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.image.flatfield.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.image.flatfield.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.image.flatfield.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.image.flatfield.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.image.flatfield.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.image.flatfield.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.image.flatfield.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.image.flatfield.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.image.flatfield.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.image.flatfield.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.image.flatfield.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.image.flatfield.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.image.flatfield.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.image.flatfield.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.image.flatfield.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.image.flatfield.get_quantile_filename","title":"<code>get_quantile_filename(cache_location, quantile)</code>","text":"Source code in <code>src\\image\\flatfield.py</code> <pre><code>def get_quantile_filename(cache_location, quantile):\n    filename = os.path.join(cache_location, 'quantile_' + f'{quantile}'.replace('.', '_') + '.tiff')\n    return filename\n</code></pre>"},{"location":"references/#src.image.flatfield.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.image.flatfield.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.image.flatfield.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.image.flatfield.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.image.flatfield.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.image.flatfield.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.image.flatfield.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.image.flatfield.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.image.flatfield.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.flatfield.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.image.flatfield.image_flatfield_correction","title":"<code>image_flatfield_correction(image0, dark, bright_dark_range, mean_bright_dark, clip=True)</code>","text":"Source code in <code>src\\image\\flatfield.py</code> <pre><code>def image_flatfield_correction(image0, dark, bright_dark_range, mean_bright_dark, clip=True):\n    # Input/output: float images\n    # https://en.wikipedia.org/wiki/Flat-field_correction\n    image = (image0 - dark) * mean_bright_dark / bright_dark_range\n    if clip:\n        image = image.clip(0, 1)    # np.clip(image) is not dask-compatible, use image.clip() instead\n    else:\n        image -= np.min(image)\n        if np.max(image) &gt; 1:\n            image /= np.max(image)\n    return image\n</code></pre>"},{"location":"references/#src.image.flatfield.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.image.flatfield.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.flatfield.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.flatfield.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.flatfield.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.image.flatfield.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.flatfield.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.flatfield.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.flatfield.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.image.flatfield.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.image.flatfield.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.image.flatfield.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.image.flatfield.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.image.flatfield.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.image.flatfield.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.flatfield.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.image.flatfield.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.image.flatfield.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.image.flatfield.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.image.flatfield.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.image.flatfield.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.image.flatfield.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.image.flatfield.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.image.flatfield.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.image.flatfield.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.flatfield.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.flatfield.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.image.flatfield.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.image.flatfield.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.image.flatfield.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.image.flatfield.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.image.ome_helper","title":"<code>ome_helper</code>","text":""},{"location":"references/#src.image.ome_helper.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.image.ome_helper.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.image.ome_helper.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.image.ome_helper.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.image.ome_helper.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.image.ome_helper.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.image.ome_helper.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.image.ome_helper.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.ome_helper.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.image.ome_helper.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.image.ome_helper.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.image.ome_helper.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.image.ome_helper.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.image.ome_helper.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.image.ome_helper.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.image.ome_helper.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.image.ome_helper.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.image.ome_helper.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.image.ome_helper.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.image.ome_helper.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.image.ome_helper.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.image.ome_helper.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.image.ome_helper.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.image.ome_helper.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.image.ome_helper.exists_output_image","title":"<code>exists_output_image(path, output_format)</code>","text":"Source code in <code>src\\image\\ome_helper.py</code> <pre><code>def exists_output_image(path, output_format):\n    exists = True\n    if 'zar' in output_format:\n        exists = exists and os.path.exists(path + zarr_extension)\n    if 'tif' in output_format:\n        exists = exists and os.path.exists(path + tiff_extension)\n    return exists\n</code></pre>"},{"location":"references/#src.image.ome_helper.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.image.ome_helper.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.image.ome_helper.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.image.ome_helper.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.image.ome_helper.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.image.ome_helper.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.image.ome_helper.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.image.ome_helper.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.image.ome_helper.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.ome_helper.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.image.ome_helper.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.image.ome_helper.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.ome_helper.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.ome_helper.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.image.ome_helper.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.ome_helper.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.ome_helper.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.ome_helper.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.image.ome_helper.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.image.ome_helper.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.image.ome_helper.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.image.ome_helper.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.image.ome_helper.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.image.ome_helper.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.ome_helper.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.image.ome_helper.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.image.ome_helper.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.image.ome_helper.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.image.ome_helper.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.image.ome_helper.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.image.ome_helper.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.image.ome_helper.save_image","title":"<code>save_image(filename, output_format, sim, transform_key=None, channels=None, translation0=None, params={})</code>","text":"Source code in <code>src\\image\\ome_helper.py</code> <pre><code>def save_image(filename, output_format, sim, transform_key=None, channels=None, translation0=None, params={}):\n    dimension_order = ''.join(sim.dims)\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    pixel_size = []\n    for dim in sdims:\n        pixel_size1 = si_utils.get_spacing_from_sim(sim)[dim]\n        if pixel_size1 == 0:\n            pixel_size1 = 1\n        pixel_size.append(pixel_size1)\n    # metadata: only use coords of fused image\n    position, rotation = get_data_mapping(sim, transform_key=transform_key, translation0=translation0)\n    nplanes = sim.sizes.get('z', 1) * sim.sizes.get('c', 1) * sim.sizes.get('t', 1)\n    positions = [position] * nplanes\n\n    if channels is None:\n        channels = sim.attrs.get('channels', [])\n\n    tile_size = params.get('tile_size')\n    if tile_size:\n        if 'z' in sim.dims and len(tile_size) &lt; 3:\n            tile_size = list(tile_size) + [1]\n        tile_size = tuple(reversed(tile_size))\n        chunking = retuple(tile_size, sim.shape)\n        sim = sim.chunk(chunks=chunking)\n\n    compression = params.get('compression')\n    pyramid_downsample = params.get('pyramid_downsample', 2)\n    npyramid_add = get_max_downsamples(sim.shape, params.get('npyramid_add', 0), pyramid_downsample)\n    scaler = Scaler(downscale=pyramid_downsample, max_layer=npyramid_add)\n\n    if 'zar' in output_format:\n        #save_ome_zarr(str(filename) + zarr_extension, sim.data, dimension_order, pixel_size,\n        #              channels, position, rotation, compression=compression, scaler=scaler,\n        #              zarr_version=3, ome_version='0.5')\n        save_ome_ngff(str(filename) + zarr_extension, sim, channels, position, rotation,\n                      pyramid_downsample=pyramid_downsample)\n    if 'tif' in output_format:\n        save_ome_tiff(str(filename) + tiff_extension, sim.data, dimension_order, pixel_size,\n                      channels, positions, rotation, tile_size=tile_size, compression=compression, scaler=scaler)\n</code></pre>"},{"location":"references/#src.image.ome_helper.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.image.ome_helper.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.image.ome_helper.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.ome_helper.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.ome_helper.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.image.ome_helper.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.image.ome_helper.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.image.ome_helper.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.image.ome_helper.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.image.ome_ngff_helper","title":"<code>ome_ngff_helper</code>","text":""},{"location":"references/#src.image.ome_ngff_helper.save_ome_ngff","title":"<code>save_ome_ngff(filename, sim, channels=None, translation=None, rotation=None, compression=None, pyramid_downsample=2)</code>","text":"Source code in <code>src\\image\\ome_ngff_helper.py</code> <pre><code>def save_ome_ngff(filename, sim, channels=None, translation=None, rotation=None,\n                  compression=None, pyramid_downsample=2):\n    pyramid_downsample_dict = {}\n    for dim in sim.dims:\n        if dim in 'xy':\n            pyramid_downsample_dict[dim] = pyramid_downsample\n        else:\n            pyramid_downsample_dict[dim] = 1\n    ngff_utils.write_sim_to_ome_zarr(sim, filename,\n                                     downscale_factors_per_spatial_dim=pyramid_downsample_dict, overwrite=True)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper","title":"<code>ome_tiff_helper</code>","text":""},{"location":"references/#src.image.ome_tiff_helper.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.create_tiff_metadata","title":"<code>create_tiff_metadata(pixel_size, dimension_order=None, channels=[], positions=[], rotation=None, is_ome=False)</code>","text":"Source code in <code>src\\image\\ome_tiff_helper.py</code> <pre><code>def create_tiff_metadata(pixel_size, dimension_order=None, channels=[], positions=[], rotation=None, is_ome=False):\n    ome_metadata = None\n    resolution = None\n    resolution_unit = None\n    pixel_size_um = None\n\n    if pixel_size is not None:\n        pixel_size_um = get_value_units_micrometer(pixel_size)[:2]\n        resolution_unit = 'CENTIMETER'\n        resolution = [1e4 / size for size in pixel_size_um]\n\n    if is_ome:\n        ome_metadata = {'Creator': 'muvis-align'}\n        if dimension_order is not None:\n            #ome_metadata['DimensionOrder'] = dimension_order[::-1].upper()\n            ome_metadata['axes'] = dimension_order.upper()\n        ome_channels = []\n        if pixel_size_um is not None:\n            ome_metadata['PhysicalSizeX'] = float(pixel_size_um[0])\n            ome_metadata['PhysicalSizeXUnit'] = '\u00b5m'\n            ome_metadata['PhysicalSizeY'] = float(pixel_size_um[1])\n            ome_metadata['PhysicalSizeYUnit'] = '\u00b5m'\n            if len(pixel_size_um) &gt; 2:\n                ome_metadata['PhysicalSizeZ'] = float(pixel_size_um[2])\n                ome_metadata['PhysicalSizeZUnit'] = '\u00b5m'\n        if positions is not None and len(positions) &gt; 0:\n            plane_metadata = {}\n            plane_metadata['PositionX'] = [float(position[0]) for position in positions]\n            plane_metadata['PositionXUnit'] = ['\u00b5m' for _ in positions]\n            plane_metadata['PositionY'] = [float(position[1]) for position in positions]\n            plane_metadata['PositionYUnit'] = ['\u00b5m' for _ in positions]\n            if len(positions[0]) &gt; 2:\n                plane_metadata['PositionZ'] = [float(position[2]) for position in positions]\n                plane_metadata['PositionZUnit'] = ['\u00b5m' for _ in positions]\n            ome_metadata['Plane'] = plane_metadata\n        if rotation is not None:\n            ome_metadata['StructuredAnnotations'] = {'CommentAnnotation': {'Value': f'Angle: {rotation} degrees'}}\n        for channeli, channel in enumerate(channels):\n            ome_channel = {'Name': channel.get('label', str(channeli))}\n            if 'color' in channel:\n                ome_channel['Color'] = rgba_to_int(channel['color'])\n            ome_channels.append(ome_channel)\n        if ome_channels:\n            ome_metadata['Channel'] = ome_channels\n    return ome_metadata, resolution, resolution_unit\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.load_tiff","title":"<code>load_tiff(filename)</code>","text":"Source code in <code>src\\image\\ome_tiff_helper.py</code> <pre><code>def load_tiff(filename):\n    return tifffile.imread(filename)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.save_ome_tiff","title":"<code>save_ome_tiff(filename, data, dimension_order, pixel_size, channels=[], positions=[], rotation=None, tile_size=None, compression=None, scaler=None)</code>","text":"Source code in <code>src\\image\\ome_tiff_helper.py</code> <pre><code>def save_ome_tiff(filename, data, dimension_order, pixel_size, channels=[], positions=[], rotation=None,\n                  tile_size=None, compression=None, scaler=None):\n\n    ome_metadata, resolution0, resolution_unit0 = create_tiff_metadata(pixel_size, dimension_order,\n                                                                       channels, positions, rotation, is_ome=True)\n    # maximum size (w/o compression)\n    max_size = data.size * data.itemsize\n    size = max_size\n    if scaler is not None:\n        npyramid_add = scaler.max_layer\n        for i in range(npyramid_add):\n            size //= (scaler.downscale ** 2)\n            max_size += size\n    else:\n        npyramid_add = 0\n    bigtiff = (max_size &gt; 2 ** 32)\n\n    tile_size = tile_size[-2:]  # assume order zyx (inversed xyz)\n    shape_yx = [data.shape[dimension_order.index(dim)] for dim in 'yx']\n    if np.any(np.array(tile_size) &gt; np.array(shape_yx)):\n        tile_size = None\n\n    with TiffWriter(filename, bigtiff=bigtiff) as writer:\n        for i in range(npyramid_add + 1):\n            if i == 0:\n                subifds = npyramid_add\n                subfiletype = None\n                metadata = ome_metadata\n                resolution = resolution0[:2]\n                resolutionunit = resolution_unit0\n            else:\n                subifds = None\n                subfiletype = 1\n                metadata = None\n                resolution = None\n                resolutionunit = None\n                data = scaler.resize_image(data)\n                data.rechunk()\n            writer.write(data, subifds=subifds, subfiletype=subfiletype,\n                         tile=tile_size, compression=compression,\n                         resolution=resolution, resolutionunit=resolutionunit, metadata=metadata)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.save_tiff","title":"<code>save_tiff(filename, data, dimension_order=None, pixel_size=None, tile_size=(1024, 1024), compression='LZW')</code>","text":"Source code in <code>src\\image\\ome_tiff_helper.py</code> <pre><code>def save_tiff(filename, data, dimension_order=None, pixel_size=None, tile_size=(1024, 1024), compression='LZW'):\n    _, resolution, resolution_unit = create_tiff_metadata(pixel_size, dimension_order)\n    tifffile.imwrite(filename, data, tile=tile_size, compression=compression,\n                     resolution=resolution, resolutionunit=resolution_unit)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.image.ome_tiff_helper.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.image.ome_zarr_helper","title":"<code>ome_zarr_helper</code>","text":""},{"location":"references/#src.image.ome_zarr_helper.save_ome_zarr","title":"<code>save_ome_zarr(filename, data, dimension_order, pixel_size, channels, translation, rotation, compression=None, scaler=None, zarr_version=2, ome_version='0.4')</code>","text":"Source code in <code>src\\image\\ome_zarr_helper.py</code> <pre><code>def save_ome_zarr(filename, data, dimension_order, pixel_size, channels, translation, rotation,\n                  compression=None, scaler=None, zarr_version=2, ome_version='0.4'):\n\n    storage_options = {'dimension_separator': '/'}\n    compressor, compression_filters = create_compression_filter(compression)\n    if compressor is not None:\n        storage_options['compressor'] = compressor\n    if compression_filters is not None:\n        storage_options['filters'] = compression_filters\n\n    if 'z' not in dimension_order:\n        # add Z dimension to be able to store Z position\n        new_dimension_order = dimension_order.replace('yx', 'zyx')\n        data = redimension_data(data, dimension_order, new_dimension_order)\n        dimension_order = new_dimension_order\n\n    axes = create_axes_metadata(dimension_order)\n\n    if scaler is not None:\n        npyramid_add = scaler.max_layer\n        pyramid_downsample = scaler.downscale\n    else:\n        npyramid_add = 0\n        pyramid_downsample = 1\n\n    coordinate_transformations = []\n    scale = 1\n    for i in range(npyramid_add + 1):\n        transform = create_transformation_metadata(dimension_order, pixel_size, scale, translation, rotation)\n        coordinate_transformations.append(transform)\n        if pyramid_downsample:\n            scale /= pyramid_downsample\n\n    if ome_version == '0.4':\n        ome_zarr_format = ome_zarr.format.FormatV04()\n    elif ome_version == '0.5':\n        ome_zarr_format = ome_zarr.format.FormatV05()   # future support anticipated\n    else:\n        ome_zarr_format = ome_zarr.format.CurrentFormat()\n\n    zarr_root = zarr.open_group(store=filename, mode=\"w\", zarr_version=zarr_version)\n    write_image(image=data, group=zarr_root, axes=axes, coordinate_transformations=coordinate_transformations,\n                scaler=scaler, storage_options=storage_options, fmt=ome_zarr_format)\n\n    keys = list(zarr_root.array_keys())\n    data_smallest = zarr_root.get(keys[-1])\n\n    # get smallest size image\n    zarr_root.attrs['omero'] = create_channel_ome_metadata(data_smallest, dimension_order, channels, ome_version)\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util","title":"<code>ome_zarr_util</code>","text":""},{"location":"references/#src.image.ome_zarr_util.create_axes_metadata","title":"<code>create_axes_metadata(dimension_order)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def create_axes_metadata(dimension_order):\n    axes = []\n    for dimension in dimension_order:\n        unit1 = None\n        if dimension == 't':\n            type1 = 'time'\n            unit1 = 'millisecond'\n        elif dimension == 'c':\n            type1 = 'channel'\n        else:\n            type1 = 'space'\n            unit1 = 'micrometer'\n        axis = {'name': dimension, 'type': type1}\n        if unit1 is not None and unit1 != '':\n            axis['unit'] = unit1\n        axes.append(axis)\n    return axes\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.create_channel_metadata","title":"<code>create_channel_metadata(source, ome_version)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def create_channel_metadata(source, ome_version):\n    channels = source.get_channels()\n    nchannels = source.get_nchannels()\n\n    if len(channels) &lt; nchannels == 3:\n        labels = ['Red', 'Green', 'Blue']\n        colors = [(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)]\n        channels = [{'label': label, 'color': color} for label, color in zip(labels, colors)]\n\n    omezarr_channels = []\n    for channeli, channel0 in enumerate(channels):\n        channel = channel0.copy()\n        color = channel.get('color', (1, 1, 1, 1))\n        channel['color'] = rgba_to_hexrgb(color)\n        if 'window' not in channel:\n            channel['window'] = source.get_channel_window(channeli)\n        omezarr_channels.append(channel)\n\n    metadata = {\n        'version': ome_version,\n        'channels': omezarr_channels,\n    }\n    return metadata\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.create_channel_ome_metadata","title":"<code>create_channel_ome_metadata(data, dimension_order, channels, ome_version)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def create_channel_ome_metadata(data, dimension_order, channels, ome_version):\n    if 'c' in dimension_order:\n        nchannels = data.shape[dimension_order.index('c')]\n    else:\n        nchannels = 1\n    if channels is None or len(channels) &lt; nchannels:\n        if nchannels == 3:\n            labels = ['Red', 'Green', 'Blue']\n            colors = [(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)]\n            channels = [{'label': label, 'color': color} for label, color in zip(labels, colors)]\n        else:\n            channels = [{'label': f'Channel {channeli}'} for channeli in range(nchannels)]\n\n    omezarr_channels = []\n    for channeli, channel0 in enumerate(channels):\n        channel = channel0.copy()\n        color = channel.get('color', (1, 1, 1, 1))\n        channel['color'] = rgba_to_hexrgb(color)\n        if 'window' not in channel:\n            channel['window'] = get_channel_window(data, dimension_order, channeli)\n        omezarr_channels.append(channel)\n\n    metadata = {\n        'version': ome_version,\n        'channels': omezarr_channels,\n    }\n    return metadata\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.create_transformation_metadata","title":"<code>create_transformation_metadata(dimension_order, pixel_size_um, scale, translation_um=[], rotation=None)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def create_transformation_metadata(dimension_order, pixel_size_um, scale, translation_um=[], rotation=None):\n    metadata = []\n    pixel_size_scale = []\n    translation_scale = []\n    for dimension in dimension_order:\n        if dimension == 'z' and len(pixel_size_um) &gt; 2:\n            pixel_size_scale1 = pixel_size_um[2]\n        elif dimension == 'y' and len(pixel_size_um) &gt; 1:\n            pixel_size_scale1 = pixel_size_um[1] / scale\n        elif dimension == 'x' and len(pixel_size_um) &gt; 0:\n            pixel_size_scale1 = pixel_size_um[0] / scale\n        else:\n            pixel_size_scale1 = 1\n        if pixel_size_scale1 == 0:\n            pixel_size_scale1 = 1\n        pixel_size_scale.append(pixel_size_scale1)\n\n        if dimension == 'z' and len(translation_um) &gt; 2:\n            translation1 = translation_um[2]\n        elif dimension == 'y' and len(translation_um) &gt; 1:\n            translation1 = translation_um[1] * scale\n        elif dimension == 'x' and len(translation_um) &gt; 0:\n            translation1 = translation_um[0] * scale\n        else:\n            translation1 = 0\n        translation_scale.append(translation1)\n\n    metadata.append({'type': 'scale', 'scale': pixel_size_scale})\n    if not all(v == 0 for v in translation_scale):\n        metadata.append({'type': 'translation', 'translation': translation_scale})\n    # Supported in ome-zarr V0.6\n    #if rotation is not None:\n    #    metadata.append({'type': 'rotation', 'rotation': rotation})\n    return metadata\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.get_channel_window","title":"<code>get_channel_window(data, dimension_order, channeli)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def get_channel_window(data, dimension_order, channeli):\n    min_quantile = 0.001\n    max_quantile = 0.999\n\n    if data.dtype.kind == 'f':\n        #info = np.finfo(dtype)\n        start, end = 0, 1\n    else:\n        info = np.iinfo(data.dtype)\n        start, end = info.min, info.max\n\n    if 'c' in dimension_order:\n        data = np.take(data, channeli, axis=dimension_order.index('c'))\n    min, max = get_image_quantile(data, min_quantile), get_image_quantile(data, max_quantile)\n    window = {'start': start, 'end': end, 'min': min, 'max': max}\n    return window\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.hexrgb_to_rgba","title":"<code>hexrgb_to_rgba(hexrgb)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def hexrgb_to_rgba(hexrgb: str) -&gt; list:\n    rgba = int_to_rgba(eval('0x' + hexrgb + 'FF'))\n    return rgba\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.int_to_rgba","title":"<code>int_to_rgba(intrgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def int_to_rgba(intrgba: int) -&gt; list:\n    signed = (intrgba &lt; 0)\n    rgba = [x / 255 for x in intrgba.to_bytes(4, signed=signed, byteorder=\"big\")]\n    if rgba[-1] == 0:\n        rgba[-1] = 1\n    return rgba\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.rgba_to_hexrgb","title":"<code>rgba_to_hexrgb(rgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def rgba_to_hexrgb(rgba: tuple|list) -&gt; str:\n    hexrgb = ''.join([hex(int(x * 255))[2:].upper().zfill(2) for x in rgba[:3]])\n    return hexrgb\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.rgba_to_int","title":"<code>rgba_to_int(rgba)</code>","text":"Source code in <code>src\\image\\color_conversion.py</code> <pre><code>def rgba_to_int(rgba: tuple|list) -&gt; int:\n    intrgba = int.from_bytes([int(x * 255) for x in rgba], signed=True, byteorder=\"big\")\n    return intrgba\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.scale_dimensions_dict","title":"<code>scale_dimensions_dict(shape0, scale)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def scale_dimensions_dict(shape0, scale):\n    shape = {}\n    if scale == 1:\n        return shape0\n    for dimension, shape1 in shape0.items():\n        if dimension[0] in ['x', 'y']:\n            shape1 = int(shape1 * scale)\n        shape[dimension] = shape1\n    return shape\n</code></pre>"},{"location":"references/#src.image.ome_zarr_util.scale_dimensions_xy","title":"<code>scale_dimensions_xy(shape0, dimension_order, scale)</code>","text":"Source code in <code>src\\image\\ome_zarr_util.py</code> <pre><code>def scale_dimensions_xy(shape0, dimension_order, scale):\n    shape = []\n    if scale == 1:\n        return shape0\n    for shape1, dimension in zip(shape0, dimension_order):\n        if dimension[0] in ['x', 'y']:\n            shape1 = int(shape1 * scale)\n        shape.append(shape1)\n    return shape\n</code></pre>"},{"location":"references/#src.image.reg_util","title":"<code>reg_util</code>","text":""},{"location":"references/#src.image.reg_util.aligned_path","title":"<code>aligned_path = 'D:/slides/12193/aligned_hpc/mappings.json'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.aligned_transforms","title":"<code>aligned_transforms = import_json(aligned_path)</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.output_path","title":"<code>output_path = 'aligned_stitched_mappings1.json'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.stitched_filenames","title":"<code>stitched_filenames = dir_regex(stitched_path)</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.stitched_path","title":"<code>stitched_path = 'D:/slides/12193/stitched_hpc/S???/mappings.json'</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.stitched_transforms","title":"<code>stitched_transforms = {('S' + split_numeric_dict(filename)['S']): (import_json(filename)) for filename in stitched_filenames}</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.transforms2","title":"<code>transforms2 = make_z_transforms(get_composite_transforms(stitched_transforms, aligned_transforms), to3d=True)</code>  <code>module-attribute</code>","text":""},{"location":"references/#src.image.reg_util.get_composite_transforms","title":"<code>get_composite_transforms(transforms, global_transforms)</code>","text":"Source code in <code>src\\image\\reg_util.py</code> <pre><code>def get_composite_transforms(transforms, global_transforms):\n    transforms2 = {}\n    for key, tile_transforms in transforms.items():\n        global_transform = global_transforms[key]\n        tile_transforms2 = {}\n        for tile_key, transform in tile_transforms.items():\n            tile_transforms2[tile_key] = combine_transforms([transform, global_transform]).tolist()\n        transforms2[key] = tile_transforms2\n    return transforms2\n</code></pre>"},{"location":"references/#src.image.reg_util.make_z_transforms","title":"<code>make_z_transforms(transforms, to3d=False)</code>","text":"Source code in <code>src\\image\\reg_util.py</code> <pre><code>def make_z_transforms(transforms, to3d=False):\n    transforms2 = {}\n    for key, transforms1 in transforms.items():\n        for key2, transform in transforms1.items():\n            if len(transform) == 3 and to3d:\n                transform2 = np.eye(4)\n                transform2[1:, 1:] = transform\n                transform = transform2.tolist()\n            transforms2[f'{key}_{key2}'] = transform\n    return transforms2\n</code></pre>"},{"location":"references/#src.image.source_helper","title":"<code>source_helper</code>","text":""},{"location":"references/#src.image.source_helper.create_dask_data","title":"<code>create_dask_data(filename, level=0)</code>","text":"Source code in <code>src\\image\\source_helper.py</code> <pre><code>def create_dask_data(filename, level=0):\n    ext = os.path.splitext(filename)[1]\n    if 'zar' in ext:\n        group = zarr.open_group(filename, mode='r')\n        # using group.attrs to get multiscales is recommended by cgohlke\n        paths = group.attrs['multiscales'][0]['datasets']\n        path0 = paths[level]['path']\n        dask_data = da.from_zarr(os.path.join(filename, path0))\n    elif 'tif' in ext:\n        with TiffFile(filename) as tif:\n            series0 = tif.series[0]\n            shape = series0.shape\n            dtype = series0.dtype\n        lazy_array = dask.delayed(tifffile.imread)(filename, level=level)\n        dask_data = da.from_delayed(lazy_array, shape=shape, dtype=dtype)\n    else:\n        lazy_array = dask.delayed(imageio.v3.imread)(filename)\n        # TODO get metadata from metadata = PIL.Image.info\n        dask_data = da.from_delayed(lazy_array, shape=shape, dtype=dtype)\n    return dask_data\n</code></pre>"},{"location":"references/#src.image.source_helper.create_dask_source","title":"<code>create_dask_source(filename, source_metadata=None)</code>","text":"Source code in <code>src\\image\\source_helper.py</code> <pre><code>def create_dask_source(filename, source_metadata=None):\n    ext = os.path.splitext(filename)[1].lstrip('.').lower()\n    if ext.startswith('tif'):\n        dask_source = TiffDaskSource(filename, source_metadata)\n    elif '.zar' in filename.lower():\n        dask_source = ZarrDaskSource(filename, source_metadata)\n    else:\n        raise ValueError(f'Unsupported file type: {ext}')\n    return dask_source\n</code></pre>"},{"location":"references/#src.image.source_helper.get_images_metadata","title":"<code>get_images_metadata(filenames, source_metadata=None)</code>","text":"Source code in <code>src\\image\\source_helper.py</code> <pre><code>def get_images_metadata(filenames, source_metadata=None):\n    summary = 'Filename\\tPixel size\\tSize\\tPosition\\tRotation\\n'\n    sizes = []\n    centers = []\n    rotations = []\n    positions = []\n    max_positions = []\n    pixel_sizes = []\n    for filename in filenames:\n        source = create_dask_source(filename, source_metadata)\n        pixel_size = dict_to_xyz(source.get_pixel_size())\n        size = dict_to_xyz(source.get_physical_size())\n        sizes.append(size)\n        position = dict_to_xyz(source.get_position())\n        rotation = source.get_rotation()\n        rotations.append(rotation)\n\n        summary += (f'{get_filetitle(filename)}'\n                    f'\\t{tuple(pixel_size)}'\n                    f'\\t{tuple(size)}'\n                    f'\\t{tuple(position)}')\n        if rotation is not None:\n            summary += f'\\t{rotation}'\n        summary += '\\n'\n\n        if len(size) &lt; len(position):\n            size = list(size) + [0]\n        center = np.array(position) + np.array(size) / 2\n        pixel_sizes.append(pixel_size)\n        centers.append(center)\n        positions.append(position)\n        max_positions.append(np.array(position) + np.array(size))\n    pixel_size = np.mean(pixel_sizes, 0)\n    center = np.mean(centers, 0)\n    area = np.max(max_positions, 0) - np.min(positions, 0)\n    summary += f'Area: {tuple(area)} Center: {tuple(center)}\\n'\n\n    rotations2 = []\n    for rotation, size in zip(rotations, sizes):\n        if rotation is None:\n            _, angles = get_orthogonal_pairs(centers, size)\n            if len(angles) &gt; 0:\n                rotation = -np.mean(angles)\n                rotations2.append(rotation)\n    if len(rotations2) &gt; 0:\n        rotation = np.mean(rotations2)\n    else:\n        rotation = None\n    return {'pixel_size': pixel_size,\n            'center': center,\n            'area': area,\n            'rotation': rotation,\n            'summary': summary}\n</code></pre>"},{"location":"references/#src.image.util","title":"<code>util</code>","text":""},{"location":"references/#src.image.util.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.image.util.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.util.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.image.util.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.image.util.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.image.util.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.image.util.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.image.util.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.image.util.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.image.util.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.util.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.image.util.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.util.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.image.util.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.image.util.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.image.util.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.image.util.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.image.util.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.image.util.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.image.util.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.image.util.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.image.util.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.image.util.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.util.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.image.util.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.image.util.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.image.util.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.image.util.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.image.util.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.image.util.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.image.util.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.image.util.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.image.util.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.image.util.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.image.util.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.image.util.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.image.util.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.util.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.image.util.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.image.util.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.image.util.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.image.util.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.image.util.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.image.util.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.image.util.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.image.util.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.image.util.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.image.util.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.image.util.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.image.util.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.image.util.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.image.util.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.image.util.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.image.util.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.image.util.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.image.util.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.image.util.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.image.util.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.image.util.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.image.util.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.image.util.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.image.util.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.util.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.image.util.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.image.util.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.util.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.util.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.image.util.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.image.util.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.image.util.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.util.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.image.util.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.image.util.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.image.util.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.image.util.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.image.util.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.image.util.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.image.util.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.image.util.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.image.util.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.image.util.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.image.util.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.image.util.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.image.util.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.image.util.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.image.util.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.image.util.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.image.util.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.util.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.image.util.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.image.util.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.image.util.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.image.util.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.image.util.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.metrics","title":"<code>metrics</code>","text":""},{"location":"references/#src.metrics.calc_frc","title":"<code>calc_frc(image1, image2)</code>","text":"Source code in <code>src\\metrics.py</code> <pre><code>def calc_frc(image1, image2):\n    pixel_size1 = si_utils.get_spacing_from_sim(image1)\n    pixel_size2 = si_utils.get_spacing_from_sim(image2)\n    pixel_size = np.mean([pixel_size1['x'], pixel_size1['y'], pixel_size2['x'], pixel_size2['y']])\n    max_size = np.flip(np.max([image1.shape, image2.shape], 0))\n    image1 = frc.util.square_image(image_reshape(image1, max_size), add_padding=True)\n    image2 = frc.util.square_image(image_reshape(image2, max_size), add_padding=True)\n\n    frc_curve = frc.two_frc(image1, image2)\n    xs_pix = np.arange(len(frc_curve)) / max(max_size)\n    # scale has units [pixels &lt;length unit&gt;^-1] corresponding to original image\n    xs_nm_freq = xs_pix / pixel_size\n    frc_res, res_y, thres = frc.frc_res(xs_nm_freq, frc_curve, max_size)\n    #plt.plot(xs_nm_freq, thres(xs_nm_freq))\n    #plt.plot(xs_nm_freq, frc_curve)\n    #plt.show()\n    return frc_res\n</code></pre>"},{"location":"references/#src.metrics.calc_match_metrics","title":"<code>calc_match_metrics(points1, points2, transform, threshold, lowe_ratio=None)</code>","text":"Source code in <code>src\\metrics.py</code> <pre><code>def calc_match_metrics(points1, points2, transform, threshold, lowe_ratio=None):\n    metrics = {}\n    transformed_points1 = apply_transform(points1, transform)\n    npoints1, npoints2 = len(points1), len(points2)\n    npoints = min(npoints1, npoints2)\n    if npoints1 == 0 or npoints2 == 0:\n        return metrics\n\n    swapped = (npoints1 &gt; npoints2)\n    if swapped:\n        points1, points2 = points2, points1\n\n    distance_matrix = euclidean_distances(transformed_points1, points2)\n    matching_distances = np.diag(distance_matrix)\n    if npoints1 == npoints2 and np.mean(matching_distances &lt; threshold) &gt; 0.5:\n        # already matching points lists\n        nmatches = np.sum(matching_distances &lt; threshold)\n    else:\n        matches = []\n        distances0 = []\n        for rowi, row in enumerate(distance_matrix):\n            sorted_indices = np.argsort(row)\n            index0 = sorted_indices[0]\n            distance0 = row[index0]\n            matches.append((rowi, sorted_indices))\n            distances0.append(distance0)\n        sorted_matches = np.argsort(distances0)\n\n        done = []\n        nmatches = 0\n        matching_distances = []\n        for sorted_match in sorted_matches:\n            i, match = matches[sorted_match]\n            for ji, j in enumerate(match):\n                if j not in done:\n                    # found best, available match\n                    distance0 = distance_matrix[i, j]\n                    distance1 = distance_matrix[i, match[ji + 1]] if ji + 1 &lt; len(match) else np.inf\n                    matching_distances.append(distance0)    # use all distances to also weigh in the non-matches\n                    if distance0 &lt; threshold and (lowe_ratio is None or distance0 &lt; lowe_ratio * distance1):\n                        done.append(j)\n                        nmatches += 1\n                    break\n\n    metrics['nmatches'] = nmatches\n    metrics['match_rate'] = nmatches / npoints if npoints &gt; 0 else 0\n    distance = np.mean(matching_distances) if nmatches &gt; 0 else np.inf\n    metrics['distance'] = float(distance)\n    metrics['norm_distance'] = float(distance / threshold)\n    return metrics\n</code></pre>"},{"location":"references/#src.metrics.calc_ncc","title":"<code>calc_ncc(image1, image2)</code>","text":"Source code in <code>src\\metrics.py</code> <pre><code>def calc_ncc(image1, image2):\n    max_size = np.flip(np.max([image1.shape, image2.shape], 0))\n    image1 = image_reshape(image1, max_size)\n    image2 = image_reshape(image2, max_size)\n\n    normimage1 = np.array(image1 - np.mean(image1))\n    normimage2 = np.array(image2 - np.mean(image2))\n    ncc = np.sum(normimage1 * normimage2) / (np.linalg.norm(normimage1) * np.linalg.norm(normimage2))\n    return float(ncc)\n</code></pre>"},{"location":"references/#src.metrics.calc_ncc2","title":"<code>calc_ncc2(image1, image2)</code>","text":"Source code in <code>src\\metrics.py</code> <pre><code>def calc_ncc2(image1, image2):\n    max_size = np.flip(np.max([image1.shape, image2.shape], 0))\n    image1 = image_reshape(image1, max_size)\n    image2 = image_reshape(image2, max_size)\n\n    normimage1 = (image1 - np.mean(image1)) / np.std(image1)\n    normimage2 = (image2 - np.mean(image2)) / np.std(image2)\n    array1 = np.array(normimage1).reshape(-1)\n    array2 = np.array(normimage2).reshape(-1)\n    ncc = (np.correlate(array1, array2) / max(len(array1), len(array2)))[0]\n    return float(ncc)\n</code></pre>"},{"location":"references/#src.metrics.calc_ssim","title":"<code>calc_ssim(image1, image2)</code>","text":"Source code in <code>src\\metrics.py</code> <pre><code>def calc_ssim(image1, image2):\n    dtype = image1.dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    max_size = np.flip(np.max([image1.shape, image2.shape], 0))\n    image1 = image_reshape(image1, max_size)\n    image2 = image_reshape(image2, max_size)\n    try:\n        ssim = structural_similarity(np.array(image1), np.array(image2), data_range=maxval)\n    except ValueError:\n        ssim = np.nan\n    return float(ssim)\n</code></pre>"},{"location":"references/#src.registration_methods","title":"<code>registration_methods</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod","title":"<code>RegistrationMethod</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod","title":"<code>RegistrationMethod</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src\\registration_methods\\RegistrationMethod.py</code> <pre><code>class RegistrationMethod(ABC):\n    def __init__(self, source, params, debug=False):\n        self.source_type = source.dtype\n        if hasattr(source, 'dims'):\n            self.full_size = si_utils.get_shape_from_sim(source, asarray=True)\n            self.ndims = 2 + int('z' in source.dims)\n        else:\n            self.full_size = [size for size in source.shape if size &gt; 4]    # try to filter channel dimension\n            self.ndims = len(self.full_size)\n        self.params = params\n        self.debug = debug\n        self.count = 0  # for debugging\n\n    def convert_data_to_float(self, data):\n        maxval = 2 ** (8 * self.source_type.itemsize) - 1\n        return data / np.float32(maxval)\n\n    @abstractmethod\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        # this returns the transform in pixel space, needs to be thread-safe!\n        # reg_func_transform = linalg.inv(params_transform) / spacing\n        # params_transform = linalg.inv(reg_func_transform * spacing)\n        return {\n            \"affine_matrix\": [],  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n        }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.count","title":"<code>count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.debug","title":"<code>debug = debug</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.full_size","title":"<code>full_size = si_utils.get_shape_from_sim(source, asarray=True)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.ndims","title":"<code>ndims = 2 + int('z' in source.dims)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.source_type","title":"<code>source_type = source.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.__init__","title":"<code>__init__(source, params, debug=False)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethod.py</code> <pre><code>def __init__(self, source, params, debug=False):\n    self.source_type = source.dtype\n    if hasattr(source, 'dims'):\n        self.full_size = si_utils.get_shape_from_sim(source, asarray=True)\n        self.ndims = 2 + int('z' in source.dims)\n    else:\n        self.full_size = [size for size in source.shape if size &gt; 4]    # try to filter channel dimension\n        self.ndims = len(self.full_size)\n    self.params = params\n    self.debug = debug\n    self.count = 0  # for debugging\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.convert_data_to_float","title":"<code>convert_data_to_float(data)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethod.py</code> <pre><code>def convert_data_to_float(self, data):\n    maxval = 2 ** (8 * self.source_type.itemsize) - 1\n    return data / np.float32(maxval)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethod.RegistrationMethod.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethod.py</code> <pre><code>@abstractmethod\ndef registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    # this returns the transform in pixel space, needs to be thread-safe!\n    # reg_func_transform = linalg.inv(params_transform) / spacing\n    # params_transform = linalg.inv(reg_func_transform * spacing)\n    return {\n        \"affine_matrix\": [],  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodANTs3Din2D","title":"<code>RegistrationMethodANTs3Din2D</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodANTs3Din2D.RegistrationMethodANTs3Din2D","title":"<code>RegistrationMethodANTs3Din2D</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMethodANTs3Din2D.py</code> <pre><code>class RegistrationMethodANTs3Din2D(RegistrationMethod):\n    def __init__(self, source, params, debug):\n        super().__init__(source, params, debug)\n\n    def registration(\n            self,\n            fixed_data,\n            moving_data,\n            *,\n            fixed_origin,\n            moving_origin,\n            fixed_spacing,\n            moving_spacing,\n            initial_affine,\n            transform_types=None,\n            **ants_registration_kwargs,\n        ):\n        \"\"\"\n        Register two 3d sims by projecting them to 2d and using 2d registration.\n        The z component of the resulting affine matrix is set to identity.\n        \"\"\"\n\n        fixed_data = fixed_data.max('z')\n        moving_data = moving_data.max('z')\n\n        dims2d = ['y', 'x']\n        fixed_origin = {dim: fixed_origin[dim] for dim in dims2d}\n        moving_origin = {dim: moving_origin[dim] for dim in dims2d}\n        fixed_spacing = {dim: fixed_spacing[dim] for dim in dims2d}\n        moving_spacing = {dim: moving_spacing[dim] for dim in dims2d}\n\n        initial_affine = initial_affine[1: , 1:]\n\n        # call 2d registration on the projected sims\n        # reg_res_2d = registration.phase_correlation_registration(\n        #     sim1, sim2, **kwargs)\n        reg_res_2d = registration.registration_ANTsPy(\n            fixed_data,\n            moving_data,\n            fixed_origin=fixed_origin,\n            moving_origin=moving_origin,\n            fixed_spacing=fixed_spacing,\n            moving_spacing=moving_spacing,\n            initial_affine=initial_affine,\n            transform_types=transform_types,\n            **ants_registration_kwargs,\n            )\n\n        # embed resulting 2d affine matrix into 3d affine matrix\n        reg_res_3d = deepcopy(reg_res_2d)\n        reg_res_3d['affine_matrix'] = param_utils.identity_transform(3)\n        reg_res_3d['affine_matrix'][1:, 1:] = reg_res_2d['affine_matrix']\n\n        return reg_res_3d\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodANTs3Din2D.RegistrationMethodANTs3Din2D.__init__","title":"<code>__init__(source, params, debug)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodANTs3Din2D.py</code> <pre><code>def __init__(self, source, params, debug):\n    super().__init__(source, params, debug)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodANTs3Din2D.RegistrationMethodANTs3Din2D.registration","title":"<code>registration(fixed_data, moving_data, *, fixed_origin, moving_origin, fixed_spacing, moving_spacing, initial_affine, transform_types=None, **ants_registration_kwargs)</code>","text":"<p>Register two 3d sims by projecting them to 2d and using 2d registration. The z component of the resulting affine matrix is set to identity.</p> Source code in <code>src\\registration_methods\\RegistrationMethodANTs3Din2D.py</code> <pre><code>def registration(\n        self,\n        fixed_data,\n        moving_data,\n        *,\n        fixed_origin,\n        moving_origin,\n        fixed_spacing,\n        moving_spacing,\n        initial_affine,\n        transform_types=None,\n        **ants_registration_kwargs,\n    ):\n    \"\"\"\n    Register two 3d sims by projecting them to 2d and using 2d registration.\n    The z component of the resulting affine matrix is set to identity.\n    \"\"\"\n\n    fixed_data = fixed_data.max('z')\n    moving_data = moving_data.max('z')\n\n    dims2d = ['y', 'x']\n    fixed_origin = {dim: fixed_origin[dim] for dim in dims2d}\n    moving_origin = {dim: moving_origin[dim] for dim in dims2d}\n    fixed_spacing = {dim: fixed_spacing[dim] for dim in dims2d}\n    moving_spacing = {dim: moving_spacing[dim] for dim in dims2d}\n\n    initial_affine = initial_affine[1: , 1:]\n\n    # call 2d registration on the projected sims\n    # reg_res_2d = registration.phase_correlation_registration(\n    #     sim1, sim2, **kwargs)\n    reg_res_2d = registration.registration_ANTsPy(\n        fixed_data,\n        moving_data,\n        fixed_origin=fixed_origin,\n        moving_origin=moving_origin,\n        fixed_spacing=fixed_spacing,\n        moving_spacing=moving_spacing,\n        initial_affine=initial_affine,\n        transform_types=transform_types,\n        **ants_registration_kwargs,\n        )\n\n    # embed resulting 2d affine matrix into 3d affine matrix\n    reg_res_3d = deepcopy(reg_res_2d)\n    reg_res_3d['affine_matrix'] = param_utils.identity_transform(3)\n    reg_res_3d['affine_matrix'][1:, 1:] = reg_res_2d['affine_matrix']\n\n    return reg_res_3d\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD","title":"<code>RegistrationMethodCPD</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodCPD.RegistrationMethodCPD","title":"<code>RegistrationMethodCPD</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMethodCPD.py</code> <pre><code>class RegistrationMethodCPD(RegistrationMethod):\n    def detect_points(self, data0):\n        data = data0.astype(self.source_type)\n        area_points = detect_area_points(data)\n        points = [point for point, area in area_points]\n        return points\n\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        max_iter = kwargs.get('max_iter', 1000)\n\n        fixed_points = self.detect_points(fixed_data)\n        moving_points = self.detect_points(moving_data)\n        threshold = get_mean_nn_distance(fixed_points, moving_points)\n\n        transform = None\n        quality = 0\n        if len(moving_points) &gt; 1 and len(fixed_points) &gt; 1:\n            result_cpd = cpd.registration_cpd(points_to_3d(moving_points), points_to_3d(fixed_points),\n                                              maxiter=max_iter)\n            transformation = result_cpd.transformation\n            S = transformation.scale * np.eye(3)\n            R = transformation.rot\n            T = np.eye(3) + np.hstack([np.zeros((3, 2)), transformation.t.reshape(-1, 1)])\n            transform = T @ R @ S\n\n            metrics = calc_match_metrics(fixed_points, moving_points, transform, threshold)\n            quality = metrics['match_rate']\n\n        if not validate_transform(transform, get_sim_physical_size(fixed_data, invert=True)):\n            logging.error('Unable to find CPD registration')\n\n        return {\n            \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n        }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.RegistrationMethodCPD.detect_points","title":"<code>detect_points(data0)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodCPD.py</code> <pre><code>def detect_points(self, data0):\n    data = data0.astype(self.source_type)\n    area_points = detect_area_points(data)\n    points = [point for point, area in area_points]\n    return points\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.RegistrationMethodCPD.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodCPD.py</code> <pre><code>def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    max_iter = kwargs.get('max_iter', 1000)\n\n    fixed_points = self.detect_points(fixed_data)\n    moving_points = self.detect_points(moving_data)\n    threshold = get_mean_nn_distance(fixed_points, moving_points)\n\n    transform = None\n    quality = 0\n    if len(moving_points) &gt; 1 and len(fixed_points) &gt; 1:\n        result_cpd = cpd.registration_cpd(points_to_3d(moving_points), points_to_3d(fixed_points),\n                                          maxiter=max_iter)\n        transformation = result_cpd.transformation\n        S = transformation.scale * np.eye(3)\n        R = transformation.rot\n        T = np.eye(3) + np.hstack([np.zeros((3, 2)), transformation.t.reshape(-1, 1)])\n        transform = T @ R @ S\n\n        metrics = calc_match_metrics(fixed_points, moving_points, transform, threshold)\n        quality = metrics['match_rate']\n\n    if not validate_transform(transform, get_sim_physical_size(fixed_data, invert=True)):\n        logging.error('Unable to find CPD registration')\n\n    return {\n        \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCPD.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCvFeatures","title":"<code>RegistrationMethodCvFeatures</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodCvFeatures.RegistrationMethodCvFeatures","title":"<code>RegistrationMethodCvFeatures</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMethodCvFeatures.py</code> <pre><code>class RegistrationMethodCvFeatures(RegistrationMethod):\n    def detect_features(self, data0):\n        data = data0.astype(self.source_type)\n\n        data = uint8_image(data)\n        scale = min(1000 / np.linalg.norm(data.shape), 1)\n        data = cv.resize(data, (0, 0), fx=scale, fy=scale)\n        feature_model = cv.SIFT_create(contrastThreshold=0.1)\n        #feature_model = cv.ORB_create(patchSize=8, edgeThreshold=8)\n        keypoints, desc = feature_model.detectAndCompute(data, None)\n        points = [np.array(keypoint.pt) / scale for keypoint in keypoints]\n        return points, desc\n\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        fixed_points, fixed_desc = self.detect_features(fixed_data.data)\n        moving_points, moving_desc = self.detect_features(moving_data.data)\n        threshold = get_mean_nn_distance(fixed_points, moving_points)\n\n        matcher = cv.BFMatcher()\n        #matches0 = matcher.match(fixed_desc, moving_desc)\n        matches0 = matcher.knnMatch(fixed_desc, moving_desc, k=2)\n\n        matches = []\n        for m, n in matches0:\n            if m.distance &lt; 0.92 * n.distance:\n                matches.append(m)\n\n        transform = None\n        quality = 0\n        if len(matches) &gt;= 4:\n            fixed_points2 = np.float32([fixed_points[match.queryIdx] for match in matches])\n            moving_points2 = np.float32([moving_points[match.trainIdx] for match in matches])\n            transform, inliers = cv.findHomography(fixed_points2, moving_points2,\n                                                   method=cv.USAC_MAGSAC, ransacReprojThreshold=threshold)\n            if transform is not None:\n                fixed_points3 = [point for point, is_inlier in zip(fixed_points2, inliers) if is_inlier]\n                moving_points3 = [point for point, is_inlier in zip(moving_points2, inliers) if is_inlier]\n                metrics = calc_match_metrics(fixed_points3, moving_points3, transform, threshold)\n                #quality = np.mean(inliers)\n                quality = metrics['match_rate']\n\n        if not validate_transform(transform):\n            logging.error('Unable to find feature-based registration')\n            transform = np.eye(3)\n\n        return {\n            \"affine_matrix\": param_utils.invert_coordinate_order(transform),  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n        }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCvFeatures.RegistrationMethodCvFeatures.detect_features","title":"<code>detect_features(data0)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodCvFeatures.py</code> <pre><code>def detect_features(self, data0):\n    data = data0.astype(self.source_type)\n\n    data = uint8_image(data)\n    scale = min(1000 / np.linalg.norm(data.shape), 1)\n    data = cv.resize(data, (0, 0), fx=scale, fy=scale)\n    feature_model = cv.SIFT_create(contrastThreshold=0.1)\n    #feature_model = cv.ORB_create(patchSize=8, edgeThreshold=8)\n    keypoints, desc = feature_model.detectAndCompute(data, None)\n    points = [np.array(keypoint.pt) / scale for keypoint in keypoints]\n    return points, desc\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodCvFeatures.RegistrationMethodCvFeatures.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodCvFeatures.py</code> <pre><code>def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    fixed_points, fixed_desc = self.detect_features(fixed_data.data)\n    moving_points, moving_desc = self.detect_features(moving_data.data)\n    threshold = get_mean_nn_distance(fixed_points, moving_points)\n\n    matcher = cv.BFMatcher()\n    #matches0 = matcher.match(fixed_desc, moving_desc)\n    matches0 = matcher.knnMatch(fixed_desc, moving_desc, k=2)\n\n    matches = []\n    for m, n in matches0:\n        if m.distance &lt; 0.92 * n.distance:\n            matches.append(m)\n\n    transform = None\n    quality = 0\n    if len(matches) &gt;= 4:\n        fixed_points2 = np.float32([fixed_points[match.queryIdx] for match in matches])\n        moving_points2 = np.float32([moving_points[match.trainIdx] for match in matches])\n        transform, inliers = cv.findHomography(fixed_points2, moving_points2,\n                                               method=cv.USAC_MAGSAC, ransacReprojThreshold=threshold)\n        if transform is not None:\n            fixed_points3 = [point for point, is_inlier in zip(fixed_points2, inliers) if is_inlier]\n            moving_points3 = [point for point, is_inlier in zip(moving_points2, inliers) if is_inlier]\n            metrics = calc_match_metrics(fixed_points3, moving_points3, transform, threshold)\n            #quality = np.mean(inliers)\n            quality = metrics['match_rate']\n\n    if not validate_transform(transform):\n        logging.error('Unable to find feature-based registration')\n        transform = np.eye(3)\n\n    return {\n        \"affine_matrix\": param_utils.invert_coordinate_order(transform),  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodDummy","title":"<code>RegistrationMethodDummy</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodDummy.RegistrationMethodDummy","title":"<code>RegistrationMethodDummy</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMethodDummy.py</code> <pre><code>class RegistrationMethodDummy(RegistrationMethod):\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        transform = cv.getRotationMatrix2D((fixed_data.shape[0] // 2, fixed_data.shape[1] // 2), 38, 1)\n        transform = np.vstack([transform, [0, 0, 1]])\n        transform[:, 2] += [300, 25, 0]\n\n        return {\n            \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n        }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodDummy.RegistrationMethodDummy.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodDummy.py</code> <pre><code>def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    transform = cv.getRotationMatrix2D((fixed_data.shape[0] // 2, fixed_data.shape[1] // 2), 38, 1)\n    transform = np.vstack([transform, [0, 0, 1]])\n    transform[:, 2] += [300, 25, 0]\n\n    return {\n        \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures","title":"<code>RegistrationMethodSkFeatures</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures","title":"<code>RegistrationMethodSkFeatures</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>class RegistrationMethodSkFeatures(RegistrationMethod):\n    def __init__(self, source, params, debug=False):\n        super().__init__(source, params, debug=debug)\n        self.method = params.get('name', 'sift').lower()\n        self.full_size_gaussian_sigma = params.get('gaussian_sigma', params.get('sigma', 1))\n        self.downscale_factor = params.get('downscale_factor', params.get('downscale', np.sqrt(2)))\n        self.nkeypoints = params.get('nkeypoints', 5000)\n        self.cross_check = params.get('cross_check', True)\n        self.lowe_ratio = params.get('lowe_ratio', 0.92)\n        self.inlier_threshold_factor = params.get('inlier_threshold_factor', 0.05)\n        self.min_matches = params.get('min_matches', 10)\n        self.max_trails = params.get('max_trials', 100)\n        self.ransac_iterations = params.get('ransac_iterations', 10)\n\n        transform_type = params.get('transform_type', '').lower()\n        if transform_type == 'affine':\n            self.transform_type = AffineTransform\n        else:\n            self.transform_type = EuclideanTransform\n\n        if transform_type in ['translation', 'translate']:\n            self.max_rotation = 10  # rotation should be ~0; check &lt;10 degrees\n        else:\n            self.max_rotation = None\n\n    def detect_features(self, data0, gaussian_sigma=None):\n        points = []\n        desc = []\n\n        if 'z' in data0.dims:\n            # make data 2D\n            data0 = data0.max('z')\n        data = self.convert_data_to_float(data0)\n        data = norm_image_variance(data)\n        if gaussian_sigma:\n            data = gaussian(data, sigma=gaussian_sigma)\n\n        try:\n            # not thread-safe - create instance that is not re-used in other thread\n            if 'orb' in self.method:\n                feature_model = ORB(n_keypoints=self.nkeypoints, downscale=self.downscale_factor)\n            else:\n                feature_model = SIFT()\n            feature_model.detect_and_extract(data)\n            points = feature_model.keypoints\n            desc = feature_model.descriptors\n            if len(points) &gt; self.nkeypoints:\n                if self.debug:\n                    print('#keypoints0', len(points))\n                indices = np.random.choice(len(points), self.nkeypoints, replace=False)\n                points = points[indices]\n                desc = desc[indices]\n            if len(points) == 0:\n                logging.error('No features detected!')\n        except RuntimeError as e:\n            logging.error(e)\n\n        if len(points) &lt; self.nkeypoints / 100:\n            # TODO: if #points is too low: alternative feature detection?\n            logging.warning(f'Low number of features: {len(points)}')\n\n        #inliers = filter_edge_points(points, np.flip(data0.shape[:2]))\n        #points = points[inliers]\n        #desc = desc[inliers]\n\n        #show_image(draw_keypoints(data, np.flip(self.feature_model.keypoints, axis=-1)))\n\n        return points, desc, data\n\n    def match(self, fixed_points, fixed_desc, moving_points, moving_desc,\n              min_matches, cross_check, lowe_ratio, inlier_threshold, mean_size_dist):\n        transform = None\n        quality = 0\n        inliers = []\n\n        matches = match_descriptors(fixed_desc, moving_desc, cross_check=cross_check, max_ratio=lowe_ratio)\n        if len(matches) &gt;= min_matches:\n            fixed_points2 = np.array([fixed_points[match[0]] for match in matches])\n            moving_points2 = np.array([moving_points[match[1]] for match in matches])\n\n            transforms = []\n            inliers_list = []\n            translations = []\n            tot_weight = 0\n            tot_translation = None\n            for i in range(self.ransac_iterations):\n                transform, inliers = ransac((fixed_points2, moving_points2), self.transform_type,\n                                            min_samples=min_matches,\n                                            residual_threshold=inlier_threshold,\n                                            max_trials=self.max_trails)\n                if inliers is None:\n                    inliers = []\n                if len(inliers) &gt; 0 and validate_transform(transform, max_rotation=self.max_rotation):\n                    weight = np.mean(inliers)\n                    weighted_translation = transform.translation * weight\n                    tot_weight += weight\n                    if tot_translation is None:\n                        tot_translation = weighted_translation\n                    else:\n                        tot_translation += weighted_translation\n                    translations.append(transform.translation)\n                    transforms.append(transform)\n                    inliers_list.append(inliers)\n                    quality += (np.sum(inliers) / self.nkeypoints) ** (1/3) # ^1/3 to decrease sensitivity\n\n            quality /= self.ransac_iterations\n\n            if tot_weight &gt; 0:\n                mean_translation = tot_translation / tot_weight\n                best_index = np.argmin(np.linalg.norm(translations - mean_translation, axis=1))\n                transform = transforms[best_index]\n                inliers = inliers_list[best_index]\n                quality *= 1 - np.clip(np.linalg.norm(np.std(translations, axis=0)) / mean_size_dist, 0, 1) ** 3  # ^3 to increase sensitivity\n                if self.debug:\n                    print('norm translation', mean_translation / mean_size_dist, 'norm SD', np.linalg.norm(np.std(translations, axis=0)) / mean_size_dist)\n            if self.debug:\n                print('%inliers', np.mean(inliers), '#good ransac iterations', len(inliers_list))\n\n        return transform, quality, matches, inliers\n\n    def registration_physical_space(\n            self,\n            fixed_data,\n            moving_data,\n            *,\n            fixed_origin,\n            moving_origin,\n            fixed_spacing,\n            moving_spacing,\n            initial_affine,\n            transform_types=None,\n            **ants_registration_kwargs,\n    ):\n        return {\n            \"affine_matrix\": np.eye(self.ndims + 1),\n            # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n        }\n\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        eye_transform = np.eye(self.ndims + 1)\n        transform = eye_transform\n        quality = 0\n        matches = []\n        inliers = []\n\n        #print(self.count, fixed_data.name, moving_data.name)\n        #self.count+=1\n        #return {\"affine_matrix\": transform, \"quality\": 1}\n\n        if np.isnan(fixed_data).all() or np.isnan(moving_data).all():\n            logging.warning('No overlapping data')\n            return {\n                \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n                \"quality\": 0  # float between 0 and 1 (if not available, set to 1.0)\n            }\n\n        full_size_dist = np.linalg.norm(self.full_size)\n        mean_size_dist = np.mean([np.linalg.norm(data.shape) for data in [fixed_data, moving_data]])\n        scale = mean_size_dist / full_size_dist\n        gaussian_sigma = self.full_size_gaussian_sigma * (scale ** (1/3))\n        mean_size = np.mean([np.linalg.norm(data.shape) / np.sqrt(self.ndims) for data in [fixed_data, moving_data]])\n        inlier_threshold = mean_size * self.inlier_threshold_factor\n\n        fixed_points, fixed_desc, fixed_data2 = self.detect_features(fixed_data, gaussian_sigma)\n        moving_points, moving_desc, moving_data2 = self.detect_features(moving_data, gaussian_sigma)\n\n        if len(fixed_desc) &gt; 0 and len(moving_desc) &gt; 0:\n            transform, quality, matches, inliers = self.match(fixed_points, fixed_desc, moving_points, moving_desc,\n                                                              min_matches=self.min_matches, cross_check=self.cross_check,\n                                                              lowe_ratio=self.lowe_ratio, inlier_threshold=inlier_threshold,\n                                                              mean_size_dist=mean_size_dist)\n\n            #landmark_initializer = sitk.LandmarkBasedTransformInitializerFilter()\n            #landmark_initializer.SetFixedLandmarks(fixed_points2)\n            #landmark_initializer.SetMovingLandmarks(moving_points2)\n            #transform = sitk.Euler2DTransform()\n            #output_transform = landmark_initializer.Execute(transform)\n            #print(output_transform)\n\n            transform = np.array(transform)\n\n        if self.debug:\n            print(f'#keypoints: {len(fixed_desc)},{len(moving_desc)}'\n                  f' #matches: {len(matches)} #inliers: {np.sum(inliers):.0f} quality: {quality:.3f}')\n\n            #output_filename = 'matches_' + datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]\n            #save_tiff(output_filename + '_f.tiff', fixed_data.astype(self.source_type))\n            #save_tiff(output_filename + '_m.tiff', moving_data.astype(self.source_type))\n\n            #if np.sum(inliers) &gt; 0:\n            #    draw_keypoints_matches_sk(fixed_data2, fixed_points,\n            #                              moving_data2, moving_points,\n            #                              matches[inliers],\n            #                              show_plot=False, output_filename=output_filename + '_i.tiff')\n\n            #draw_keypoints_matches(fixed_data2, fixed_points,\n            #                       moving_data2, moving_points,\n            #                       matches, inliers,\n            #                       show_plot=False, output_filename=output_filename + '.tiff')\n\n        if quality == 0 or np.sum(inliers) == 0:\n            logging.error('Unable to find feature-based registration')\n            transform = eye_transform\n\n        if len(transform) &lt; self.ndims + 1:\n            transform3d = eye_transform\n            transform3d[1:, 1:] = transform\n            transform = transform3d\n\n        return {\n            \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n        }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.cross_check","title":"<code>cross_check = params.get('cross_check', True)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.downscale_factor","title":"<code>downscale_factor = params.get('downscale_factor', params.get('downscale', np.sqrt(2)))</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.full_size_gaussian_sigma","title":"<code>full_size_gaussian_sigma = params.get('gaussian_sigma', params.get('sigma', 1))</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.inlier_threshold_factor","title":"<code>inlier_threshold_factor = params.get('inlier_threshold_factor', 0.05)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.lowe_ratio","title":"<code>lowe_ratio = params.get('lowe_ratio', 0.92)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.max_rotation","title":"<code>max_rotation = 10</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.max_trails","title":"<code>max_trails = params.get('max_trials', 100)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.method","title":"<code>method = params.get('name', 'sift').lower()</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.min_matches","title":"<code>min_matches = params.get('min_matches', 10)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.nkeypoints","title":"<code>nkeypoints = params.get('nkeypoints', 5000)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.ransac_iterations","title":"<code>ransac_iterations = params.get('ransac_iterations', 10)</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.transform_type","title":"<code>transform_type = AffineTransform</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.__init__","title":"<code>__init__(source, params, debug=False)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>def __init__(self, source, params, debug=False):\n    super().__init__(source, params, debug=debug)\n    self.method = params.get('name', 'sift').lower()\n    self.full_size_gaussian_sigma = params.get('gaussian_sigma', params.get('sigma', 1))\n    self.downscale_factor = params.get('downscale_factor', params.get('downscale', np.sqrt(2)))\n    self.nkeypoints = params.get('nkeypoints', 5000)\n    self.cross_check = params.get('cross_check', True)\n    self.lowe_ratio = params.get('lowe_ratio', 0.92)\n    self.inlier_threshold_factor = params.get('inlier_threshold_factor', 0.05)\n    self.min_matches = params.get('min_matches', 10)\n    self.max_trails = params.get('max_trials', 100)\n    self.ransac_iterations = params.get('ransac_iterations', 10)\n\n    transform_type = params.get('transform_type', '').lower()\n    if transform_type == 'affine':\n        self.transform_type = AffineTransform\n    else:\n        self.transform_type = EuclideanTransform\n\n    if transform_type in ['translation', 'translate']:\n        self.max_rotation = 10  # rotation should be ~0; check &lt;10 degrees\n    else:\n        self.max_rotation = None\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.detect_features","title":"<code>detect_features(data0, gaussian_sigma=None)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>def detect_features(self, data0, gaussian_sigma=None):\n    points = []\n    desc = []\n\n    if 'z' in data0.dims:\n        # make data 2D\n        data0 = data0.max('z')\n    data = self.convert_data_to_float(data0)\n    data = norm_image_variance(data)\n    if gaussian_sigma:\n        data = gaussian(data, sigma=gaussian_sigma)\n\n    try:\n        # not thread-safe - create instance that is not re-used in other thread\n        if 'orb' in self.method:\n            feature_model = ORB(n_keypoints=self.nkeypoints, downscale=self.downscale_factor)\n        else:\n            feature_model = SIFT()\n        feature_model.detect_and_extract(data)\n        points = feature_model.keypoints\n        desc = feature_model.descriptors\n        if len(points) &gt; self.nkeypoints:\n            if self.debug:\n                print('#keypoints0', len(points))\n            indices = np.random.choice(len(points), self.nkeypoints, replace=False)\n            points = points[indices]\n            desc = desc[indices]\n        if len(points) == 0:\n            logging.error('No features detected!')\n    except RuntimeError as e:\n        logging.error(e)\n\n    if len(points) &lt; self.nkeypoints / 100:\n        # TODO: if #points is too low: alternative feature detection?\n        logging.warning(f'Low number of features: {len(points)}')\n\n    #inliers = filter_edge_points(points, np.flip(data0.shape[:2]))\n    #points = points[inliers]\n    #desc = desc[inliers]\n\n    #show_image(draw_keypoints(data, np.flip(self.feature_model.keypoints, axis=-1)))\n\n    return points, desc, data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.match","title":"<code>match(fixed_points, fixed_desc, moving_points, moving_desc, min_matches, cross_check, lowe_ratio, inlier_threshold, mean_size_dist)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>def match(self, fixed_points, fixed_desc, moving_points, moving_desc,\n          min_matches, cross_check, lowe_ratio, inlier_threshold, mean_size_dist):\n    transform = None\n    quality = 0\n    inliers = []\n\n    matches = match_descriptors(fixed_desc, moving_desc, cross_check=cross_check, max_ratio=lowe_ratio)\n    if len(matches) &gt;= min_matches:\n        fixed_points2 = np.array([fixed_points[match[0]] for match in matches])\n        moving_points2 = np.array([moving_points[match[1]] for match in matches])\n\n        transforms = []\n        inliers_list = []\n        translations = []\n        tot_weight = 0\n        tot_translation = None\n        for i in range(self.ransac_iterations):\n            transform, inliers = ransac((fixed_points2, moving_points2), self.transform_type,\n                                        min_samples=min_matches,\n                                        residual_threshold=inlier_threshold,\n                                        max_trials=self.max_trails)\n            if inliers is None:\n                inliers = []\n            if len(inliers) &gt; 0 and validate_transform(transform, max_rotation=self.max_rotation):\n                weight = np.mean(inliers)\n                weighted_translation = transform.translation * weight\n                tot_weight += weight\n                if tot_translation is None:\n                    tot_translation = weighted_translation\n                else:\n                    tot_translation += weighted_translation\n                translations.append(transform.translation)\n                transforms.append(transform)\n                inliers_list.append(inliers)\n                quality += (np.sum(inliers) / self.nkeypoints) ** (1/3) # ^1/3 to decrease sensitivity\n\n        quality /= self.ransac_iterations\n\n        if tot_weight &gt; 0:\n            mean_translation = tot_translation / tot_weight\n            best_index = np.argmin(np.linalg.norm(translations - mean_translation, axis=1))\n            transform = transforms[best_index]\n            inliers = inliers_list[best_index]\n            quality *= 1 - np.clip(np.linalg.norm(np.std(translations, axis=0)) / mean_size_dist, 0, 1) ** 3  # ^3 to increase sensitivity\n            if self.debug:\n                print('norm translation', mean_translation / mean_size_dist, 'norm SD', np.linalg.norm(np.std(translations, axis=0)) / mean_size_dist)\n        if self.debug:\n            print('%inliers', np.mean(inliers), '#good ransac iterations', len(inliers_list))\n\n    return transform, quality, matches, inliers\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    eye_transform = np.eye(self.ndims + 1)\n    transform = eye_transform\n    quality = 0\n    matches = []\n    inliers = []\n\n    #print(self.count, fixed_data.name, moving_data.name)\n    #self.count+=1\n    #return {\"affine_matrix\": transform, \"quality\": 1}\n\n    if np.isnan(fixed_data).all() or np.isnan(moving_data).all():\n        logging.warning('No overlapping data')\n        return {\n            \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n            \"quality\": 0  # float between 0 and 1 (if not available, set to 1.0)\n        }\n\n    full_size_dist = np.linalg.norm(self.full_size)\n    mean_size_dist = np.mean([np.linalg.norm(data.shape) for data in [fixed_data, moving_data]])\n    scale = mean_size_dist / full_size_dist\n    gaussian_sigma = self.full_size_gaussian_sigma * (scale ** (1/3))\n    mean_size = np.mean([np.linalg.norm(data.shape) / np.sqrt(self.ndims) for data in [fixed_data, moving_data]])\n    inlier_threshold = mean_size * self.inlier_threshold_factor\n\n    fixed_points, fixed_desc, fixed_data2 = self.detect_features(fixed_data, gaussian_sigma)\n    moving_points, moving_desc, moving_data2 = self.detect_features(moving_data, gaussian_sigma)\n\n    if len(fixed_desc) &gt; 0 and len(moving_desc) &gt; 0:\n        transform, quality, matches, inliers = self.match(fixed_points, fixed_desc, moving_points, moving_desc,\n                                                          min_matches=self.min_matches, cross_check=self.cross_check,\n                                                          lowe_ratio=self.lowe_ratio, inlier_threshold=inlier_threshold,\n                                                          mean_size_dist=mean_size_dist)\n\n        #landmark_initializer = sitk.LandmarkBasedTransformInitializerFilter()\n        #landmark_initializer.SetFixedLandmarks(fixed_points2)\n        #landmark_initializer.SetMovingLandmarks(moving_points2)\n        #transform = sitk.Euler2DTransform()\n        #output_transform = landmark_initializer.Execute(transform)\n        #print(output_transform)\n\n        transform = np.array(transform)\n\n    if self.debug:\n        print(f'#keypoints: {len(fixed_desc)},{len(moving_desc)}'\n              f' #matches: {len(matches)} #inliers: {np.sum(inliers):.0f} quality: {quality:.3f}')\n\n        #output_filename = 'matches_' + datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]\n        #save_tiff(output_filename + '_f.tiff', fixed_data.astype(self.source_type))\n        #save_tiff(output_filename + '_m.tiff', moving_data.astype(self.source_type))\n\n        #if np.sum(inliers) &gt; 0:\n        #    draw_keypoints_matches_sk(fixed_data2, fixed_points,\n        #                              moving_data2, moving_points,\n        #                              matches[inliers],\n        #                              show_plot=False, output_filename=output_filename + '_i.tiff')\n\n        #draw_keypoints_matches(fixed_data2, fixed_points,\n        #                       moving_data2, moving_points,\n        #                       matches, inliers,\n        #                       show_plot=False, output_filename=output_filename + '.tiff')\n\n    if quality == 0 or np.sum(inliers) == 0:\n        logging.error('Unable to find feature-based registration')\n        transform = eye_transform\n\n    if len(transform) &lt; self.ndims + 1:\n        transform3d = eye_transform\n        transform3d[1:, 1:] = transform\n        transform = transform3d\n\n    return {\n        \"affine_matrix\": transform,  # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": quality  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.RegistrationMethodSkFeatures.registration_physical_space","title":"<code>registration_physical_space(fixed_data, moving_data, *, fixed_origin, moving_origin, fixed_spacing, moving_spacing, initial_affine, transform_types=None, **ants_registration_kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMethodSkFeatures.py</code> <pre><code>def registration_physical_space(\n        self,\n        fixed_data,\n        moving_data,\n        *,\n        fixed_origin,\n        moving_origin,\n        fixed_spacing,\n        moving_spacing,\n        initial_affine,\n        transform_types=None,\n        **ants_registration_kwargs,\n):\n    return {\n        \"affine_matrix\": np.eye(self.ndims + 1),\n        # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": 1  # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.blur_image","title":"<code>blur_image(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image(image, sigma):\n    nchannels = image.shape[2] if image.ndim == 3 else 1\n    if nchannels not in [1, 3]:\n        new_image = np.zeros_like(image)\n        for channeli in range(nchannels):\n            new_image[..., channeli] = blur_image_single(image[..., channeli], sigma)\n    else:\n        new_image = blur_image_single(image, sigma)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.blur_image_single","title":"<code>blur_image_single(image, sigma)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def blur_image_single(image, sigma):\n    return gaussian_filter(image, sigma)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.calc_foreground_map","title":"<code>calc_foreground_map(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_foreground_map(sims):\n    if len(sims) &lt;= 2:\n        return [True] * len(sims)\n    sims = [sim.squeeze().astype(np.float32) for sim in sims]\n    median_image = calc_images_median(sims).astype(np.float32)\n    difs = [np.mean(np.abs(sim - median_image), (0, 1)) for sim in sims]\n    # or use stddev instead of mean?\n    threshold = np.mean(difs, 0)\n    #threshold, _ = cv.threshold(np.array(difs).astype(np.uint16), 0, 1, cv.THRESH_OTSU)\n    #threshold, foregrounds = filter_noise_images(channel_images)\n    map = (difs &gt; threshold)\n    if np.all(map == False):\n        return [True] * len(sims)\n    return map\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.calc_images_median","title":"<code>calc_images_median(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_median(images):\n    out_image = np.zeros(shape=images[0].shape, dtype=images[0].dtype)\n    median_image = np.median(images, 0, out_image)\n    return median_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.calc_images_quantiles","title":"<code>calc_images_quantiles(images, quantiles)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_images_quantiles(images, quantiles):\n    quantile_images = [image.astype(np.float32) for image in np.quantile(images, quantiles, 0)]\n    return quantile_images\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.calc_output_properties","title":"<code>calc_output_properties(sims, transform_key, z_scale=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_output_properties(sims, transform_key, z_scale=None):\n    output_spacing = si_utils.get_spacing_from_sim(sims[0])\n    if z_scale is not None:\n        output_spacing['z'] = z_scale\n    output_properties = fusion.calc_fusion_stack_properties(\n        sims,\n        [si_utils.get_affine_from_sim(sim, transform_key) for sim in sims],\n        output_spacing,\n        mode='union',\n    )\n    return output_properties\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.calc_pyramid","title":"<code>calc_pyramid(xyzct, npyramid_add=0, pyramid_downsample=2, volumetric_resize=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def calc_pyramid(xyzct: tuple, npyramid_add: int = 0, pyramid_downsample: float = 2,\n                 volumetric_resize: bool = False) -&gt; list:\n    x, y, z, c, t = xyzct\n    if volumetric_resize and z &gt; 1:\n        size = (x, y, z)\n    else:\n        size = (x, y)\n    sizes_add = []\n    scale = 1\n    for _ in range(npyramid_add):\n        scale /= pyramid_downsample\n        scaled_size = np.maximum(np.round(np.multiply(size, scale)).astype(int), 1)\n        sizes_add.append(scaled_size)\n    return sizes_add\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.color_image","title":"<code>color_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def color_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 1:\n        return cv.cvtColor(np.array(image), cv.COLOR_GRAY2RGB)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.combine_transforms","title":"<code>combine_transforms(transforms)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def combine_transforms(transforms):\n    combined_transform = None\n    for transform in transforms:\n        if combined_transform is None:\n            combined_transform = transform\n        else:\n            combined_transform = np.dot(transform, combined_transform)\n    return combined_transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.convert_image_sign_type","title":"<code>convert_image_sign_type(image, target_dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def convert_image_sign_type(image: np.ndarray, target_dtype: np.dtype) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    if source_dtype.kind == target_dtype.kind:\n        new_image = image\n    elif source_dtype.kind == 'i':\n        new_image = ensure_unsigned_image(image)\n    else:\n        # conversion without overhead\n        offset = 2 ** (8 * target_dtype.itemsize - 1)\n        new_image = (image - offset).astype(target_dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.create_compression_filter","title":"<code>create_compression_filter(compression)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def create_compression_filter(compression: list) -&gt; tuple:\n    compressor, compression_filters = None, None\n    compression = ensure_list(compression)\n    if compression is not None and len(compression) &gt; 0:\n        compression_type = compression[0].lower()\n        if len(compression) &gt; 1:\n            level = int(compression[1])\n        else:\n            level = None\n        if 'lzw' in compression_type:\n            from imagecodecs.numcodecs import Lzw\n            compression_filters = [Lzw()]\n        elif '2k' in compression_type or '2000' in compression_type:\n            from imagecodecs.numcodecs import Jpeg2k\n            compression_filters = [Jpeg2k(level=level)]\n        elif 'jpegls' in compression_type:\n            from imagecodecs.numcodecs import Jpegls\n            compression_filters = [Jpegls(level=level)]\n        elif 'jpegxr' in compression_type:\n            from imagecodecs.numcodecs import Jpegxr\n            compression_filters = [Jpegxr(level=level)]\n        elif 'jpegxl' in compression_type:\n            from imagecodecs.numcodecs import Jpegxl\n            compression_filters = [Jpegxl(level=level)]\n        else:\n            compressor = compression\n    return compressor, compression_filters\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.detect_area_points","title":"<code>detect_area_points(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def detect_area_points(image):\n    method = cv.THRESH_OTSU\n    threshold = -5\n    contours = []\n    while len(contours) &lt;= 1 and threshold &lt;= 255:\n        _, binimage = cv.threshold(np.array(uint8_image(image)), threshold, 255, method)\n        contours0 = cv.findContours(binimage, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        contours = contours0[0] if len(contours0) == 2 else contours0[1]\n        method = cv.THRESH_BINARY\n        threshold += 5\n    area_contours = [(contour, cv.contourArea(contour)) for contour in contours]\n    area_contours.sort(key=lambda contour_area: contour_area[1], reverse=True)\n    min_area = max(np.mean([area for contour, area in area_contours]), 1)\n    area_points = [(get_center(contour), area) for contour, area in area_contours if area &gt; min_area]\n\n    #image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)\n    #for point in area_points:\n    #    radius = int(np.round(np.sqrt(point[1]/np.pi)))\n    #    cv.circle(image, tuple(np.round(point[0]).astype(int)), radius, (255, 0, 0), -1)\n    #show_image(image)\n    return area_points\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.draw_keypoints","title":"<code>draw_keypoints(image, points, color=(255, 0, 0))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints(image, points, color=(255, 0, 0)):\n    out_image = color_image(float2int_image(image))\n    for point in points:\n        point = np.round(point).astype(int)\n        cv.drawMarker(out_image, tuple(point), color=color, markerType=cv.MARKER_CROSS, markerSize=5, thickness=1)\n    return out_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.draw_keypoints_matches","title":"<code>draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None, points_color='black', match_color='red', inlier_color='lime', show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches(image1, points1, image2, points2, matches=None, inliers=None,\n                           points_color='black', match_color='red', inlier_color='lime',\n                           show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape = np.max([image.shape for image in [image1, image2]], axis=0)\n    shape_y, shape_x = shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        merge_axis = 0\n        offset2 = [shape_y, 0]\n    else:\n        merge_axis = 1\n        offset2 = [0, shape_x]\n    image = np.concatenate([\n        np.pad(image1, ((0, shape[0] - image1.shape[0]), (0, shape[1] - image1.shape[1]))),\n        np.pad(image2, ((0, shape[0] - image2.shape[0]), (0, shape[1] - image2.shape[1])))\n    ], axis=merge_axis)\n    ax.imshow(image, cmap='gray')\n\n    ax.scatter(\n        points1[:, 1],\n        points1[:, 0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n    ax.scatter(\n        points2[:, 1] + offset2[1],\n        points2[:, 0] + offset2[0],\n        facecolors='none',\n        edgecolors=points_color,\n    )\n\n    for i, match in enumerate(matches):\n        color = match_color\n        if i &lt; len(inliers) and inliers[i]:\n            color = inlier_color\n        index1, index2 = match\n        ax.plot(\n            (points1[index1, 1], points2[index2, 1] + offset2[1]),\n            (points1[index1, 0], points2[index2, 0] + offset2[0]),\n            '-', linewidth=1, alpha=0.5, color=color,\n        )\n\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n\n    return fig, ax\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.draw_keypoints_matches_cv","title":"<code>draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None, color=(255, 0, 0), inlier_color=(0, 255, 0), radius=15, thickness=2)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_cv(image1, points1, image2, points2, matches=None, inliers=None,\n                              color=(255, 0, 0), inlier_color=(0, 255, 0), radius = 15, thickness = 2):\n    # based on https://gist.github.com/woolpeeker/d7e1821e1b5c556b32aafe10b7a1b7e8\n    image1 = uint8_image(image1)\n    image2 = uint8_image(image2)\n    # We're drawing them side by side.  Get dimensions accordingly.\n    new_shape = (max(image1.shape[0], image2.shape[0]), image1.shape[1] + image2.shape[1], 3)\n    out_image = np.zeros(new_shape, image1.dtype)\n    # Place images onto the new image.\n    out_image[0:image1.shape[0], 0:image1.shape[1]] = color_image(image1)\n    out_image[0:image2.shape[0], image1.shape[1]:image1.shape[1] + image2.shape[1]] = color_image(image2)\n\n    if matches is not None:\n        # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n        for index, match in enumerate(matches):\n            if inliers is not None and inliers[index]:\n                line_color = inlier_color\n            else:\n                line_color = color\n            # So the keypoint locs are stored as a tuple of floats.  cv2.line() wants locs as a tuple of ints.\n            end1 = tuple(np.round(points1[match[0]]).astype(int))\n            end2 = tuple(np.round(points2[match[1]]).astype(int) + np.array([image1.shape[1], 0]))\n            cv.line(out_image, end1, end2, line_color, thickness)\n            cv.circle(out_image, end1, radius, line_color, thickness)\n            cv.circle(out_image, end2, radius, line_color, thickness)\n    else:\n        # Draw all points if no matches are provided.\n        for point in points1:\n            point = tuple(np.round(point).astype(int))\n            cv.circle(out_image, point, radius, color, thickness)\n        for point in points2:\n            point = tuple(np.round(point).astype(int) + np.array([image1.shape[1], 0]))\n            cv.circle(out_image, point, radius, color, thickness)\n    return out_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.draw_keypoints_matches_sk","title":"<code>draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None, show_plot=True, output_filename=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def draw_keypoints_matches_sk(image1, points1, image2, points2, matches=None,\n                              show_plot=True, output_filename=None):\n    fig, ax = plt.subplots(figsize=(16, 8))\n    shape_y, shape_x = image1.shape[:2]\n    if shape_x &gt; 2 * shape_y:\n        alignment = 'vertical'\n    else:\n        alignment = 'horizontal'\n    plot_matched_features(\n        image1,\n        image2,\n        keypoints0=points1,\n        keypoints1=points2,\n        matches=matches,\n        ax=ax,\n        alignment=alignment,\n        only_matches=True,\n    )\n    plt.tight_layout()\n    if output_filename is not None:\n        plt.savefig(output_filename)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.ensure_unsigned_image","title":"<code>ensure_unsigned_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_image(image: np.ndarray) -&gt; np.ndarray:\n    source_dtype = image.dtype\n    dtype = ensure_unsigned_type(source_dtype)\n    if dtype != source_dtype:\n        # conversion without overhead\n        offset = 2 ** (8 * dtype.itemsize - 1)\n        new_image = image.astype(dtype) + offset\n    else:\n        new_image = image\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.ensure_unsigned_type","title":"<code>ensure_unsigned_type(dtype)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def ensure_unsigned_type(dtype: np.dtype) -&gt; np.dtype:\n    new_dtype = dtype\n    if dtype.kind == 'i' or dtype.byteorder == '&gt;' or dtype.byteorder == '&lt;':\n        new_dtype = np.dtype(f'u{dtype.itemsize}')\n    return new_dtype\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.filter_noise_images","title":"<code>filter_noise_images(images)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def filter_noise_images(images):\n    dtype = images[0].dtype\n    maxval = 2 ** (8 * dtype.itemsize) - 1\n    image_vars = [np.asarray(np.std(image)).item() for image in images]\n    threshold, mask0 = cv.threshold(np.array(image_vars).astype(dtype), 0, maxval, cv.THRESH_OTSU)\n    mask = [flag.item() for flag in mask0.astype(bool)]\n    return int(threshold), mask\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.float2int_image","title":"<code>float2int_image(image, target_dtype=np.dtype(np.uint8))</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def float2int_image(image, target_dtype=np.dtype(np.uint8)):\n    source_dtype = image.dtype\n    if source_dtype.kind not in ('i', 'u') and not target_dtype.kind == 'f':\n        maxval = 2 ** (8 * target_dtype.itemsize) - 1\n        return (image * maxval).astype(target_dtype)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_data_mapping","title":"<code>get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_data_mapping(data, transform_key=None, transform=None, translation0=None, rotation=None):\n    if rotation is None:\n        rotation = 0\n\n    if isinstance(data, DataTree):\n        sim = msi_utils.get_sim_from_msim(data)\n    else:\n        sim = data\n    sdims = ''.join(si_utils.get_spatial_dims_from_sim(sim))\n    sdims = sdims.replace('zyx', 'xyz').replace('yx', 'xy')   # order xy(z)\n    origin = si_utils.get_origin_from_sim(sim)\n    translation = [origin[sdim] for sdim in sdims]\n\n    if len(translation) == 0:\n        translation = [0, 0]\n    if len(translation) == 2:\n        if translation0 is not None and len (translation0) == 3:\n            z = translation0[2]\n        else:\n            z = 0\n        translation = list(translation) + [z]\n\n    if transform is not None:\n        translation1, rotation1, _ = get_properties_from_transform(transform, invert=True)\n        translation = np.array(translation) + translation1\n        rotation += rotation1\n\n    if transform_key is not None:\n        transform1 = sim.transforms[transform_key]\n        translation1, rotation1, _ = get_properties_from_transform(transform1, invert=True)\n        rotation += rotation1\n\n    return translation, rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_image_quantile","title":"<code>get_image_quantile(image, quantile, axis=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_quantile(image: np.ndarray, quantile: float, axis=None) -&gt; float:\n    value = np.quantile(image, quantile, axis=axis).astype(image.dtype)\n    return value\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_image_size_info","title":"<code>get_image_size_info(sizes_xyzct, pixel_nbytes, pixel_type, channels)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_size_info(sizes_xyzct: list, pixel_nbytes: int, pixel_type: np.dtype, channels: list) -&gt; str:\n    image_size_info = 'XYZCT:'\n    size = 0\n    for i, size_xyzct in enumerate(sizes_xyzct):\n        w, h, zs, cs, ts = size_xyzct\n        size += np.int64(pixel_nbytes) * w * h * zs * cs * ts\n        if i &gt; 0:\n            image_size_info += ','\n        image_size_info += f' {w} {h} {zs} {cs} {ts}'\n    image_size_info += f' Pixel type: {pixel_type} Uncompressed: {print_hbytes(size)}'\n    if sizes_xyzct[0][3] == 3:\n        channel_info = 'rgb'\n    else:\n        channel_info = ','.join([channel.get('Name', '') for channel in channels])\n    if channel_info != '':\n        image_size_info += f' Channels: {channel_info}'\n    return image_size_info\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_image_window","title":"<code>get_image_window(image, low=0.01, high=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_image_window(image, low=0.01, high=0.99):\n    window = (\n        get_image_quantile(image, low),\n        get_image_quantile(image, high)\n    )\n    return window\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_max_downsamples","title":"<code>get_max_downsamples(shape, npyramid_add, pyramid_downsample)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_max_downsamples(shape, npyramid_add, pyramid_downsample):\n    shape = list(shape)\n    for i in range(npyramid_add):\n        shape[-1] //= pyramid_downsample\n        shape[-2] //= pyramid_downsample\n        if shape[-1] &lt; 1 or shape[-2] &lt; 1:\n            return i\n    return npyramid_add\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_numpy_slicing","title":"<code>get_numpy_slicing(dimension_order, **slicing)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_numpy_slicing(dimension_order, **slicing):\n    slices = []\n    for axis in dimension_order:\n        index = slicing.get(axis)\n        index0 = slicing.get(axis + '0')\n        index1 = slicing.get(axis + '1')\n        if index0 is not None and index1 is not None:\n            slice1 = slice(int(index0), int(index1))\n        elif index is not None:\n            slice1 = int(index)\n        else:\n            slice1 = slice(None)\n        slices.append(slice1)\n    return tuple(slices)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_properties_from_transform","title":"<code>get_properties_from_transform(transform, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_properties_from_transform(transform, invert=False):\n    if len(transform.shape) == 3:\n        transform = transform[0]\n    if invert:\n        transform = param_utils.invert_coordinate_order(transform)\n    transform = np.array(transform)\n    translation = param_utils.translation_from_affine(transform)\n    if len(translation) == 2:\n        translation = list(translation) + [0]\n    rotation = get_rotation_from_transform(transform)\n    scale = get_scale_from_transform(transform)\n    return translation, rotation, scale\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_sim_physical_size","title":"<code>get_sim_physical_size(sim, invert=False)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_physical_size(sim, invert=False):\n    size = si_utils.get_shape_from_sim(sim, asarray=True) * si_utils.get_spacing_from_sim(sim, asarray=True)\n    if invert:\n        size = np.flip(size)\n    return size\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_sim_position_final","title":"<code>get_sim_position_final(sim)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_position_final(sim):\n    transform_keys = si_utils.get_tranform_keys_from_sim(sim)\n    transform = combine_transforms([np.array(si_utils.get_affine_from_sim(sim, transform_key))\n                                    for transform_key in transform_keys])\n    position = apply_transform([si_utils.get_origin_from_sim(sim, asarray=True)], transform)[0]\n    return position\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_sim_shape_2d","title":"<code>get_sim_shape_2d(sim, transform_key=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def get_sim_shape_2d(sim, transform_key=None):\n    if 't' in sim.coords.xindexes:\n        # work-around for points error in get_overlap_bboxes()\n        sim1 = si_utils.sim_sel_coords(sim, {'t': 0})\n    else:\n        sim1 = sim\n    stack_props = si_utils.get_stack_properties_from_sim(sim1, transform_key=transform_key)\n    vertices = mv_graph.get_vertices_from_stack_props(stack_props)\n    if vertices.shape[1] == 3:\n        # remove z coordinate\n        vertices = vertices[:, 1:]\n    if len(vertices) &gt;= 8:\n        # remove redundant x/y vertices\n        vertices = vertices[:4]\n    if len(vertices) &gt;= 4:\n        # last 2 vertices appear to be swapped\n        vertices[2:] = np.array(list(reversed(vertices[2:])))\n    return vertices\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.grayscale_image","title":"<code>grayscale_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def grayscale_image(image):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if nchannels == 4:\n        return cv.cvtColor(image, cv.COLOR_RGBA2GRAY)\n    elif nchannels &gt; 1:\n        return cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.group_sims_by_z","title":"<code>group_sims_by_z(sims)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def group_sims_by_z(sims):\n    grouped_sims = []\n    z_positions = [si_utils.get_origin_from_sim(sim).get('z') for sim in sims]\n    is_mixed_3dstack = len(set(z_positions)) &lt; len(z_positions)\n    if is_mixed_3dstack:\n        sims_by_z = {}\n        for simi, z_pos in enumerate(z_positions):\n            if z_pos is not None and z_pos not in sims_by_z:\n                sims_by_z[z_pos] = []\n            sims_by_z[z_pos].append(simi)\n        grouped_sims = list(sims_by_z.values())\n    if len(grouped_sims) == 0:\n        grouped_sims = [list(range(len(sims)))]\n    return grouped_sims\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.image_reshape","title":"<code>image_reshape(image, target_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_reshape(image: np.ndarray, target_size: tuple) -&gt; np.ndarray:\n    tw, th = target_size\n    sh, sw = image.shape[0:2]\n    if sw &lt; tw or sh &lt; th:\n        dw = max(tw - sw, 0)\n        dh = max(th - sh, 0)\n        padding = [(dh // 2, dh - dh //  2), (dw // 2, dw - dw // 2)]\n        if len(image.shape) == 3:\n            padding += [(0, 0)]\n        image = np.pad(image, padding, mode='constant', constant_values=(0, 0))\n    if tw &lt; sw or th &lt; sh:\n        image = image[0:th, 0:tw]\n    return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.image_resize","title":"<code>image_resize(image, target_size0, dimension_order='yxc')</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def image_resize(image: np.ndarray, target_size0: tuple, dimension_order: str = 'yxc') -&gt; np.ndarray:\n    shape = image.shape\n    x_index = dimension_order.index('x')\n    y_index = dimension_order.index('y')\n    c_is_at_end = ('c' in dimension_order and dimension_order.endswith('c'))\n    size = shape[x_index], shape[y_index]\n    if np.mean(np.divide(size, target_size0)) &lt; 1:\n        interpolation = cv.INTER_CUBIC\n    else:\n        interpolation = cv.INTER_AREA\n    dtype0 = image.dtype\n    image = ensure_unsigned_image(image)\n    target_size = tuple(np.maximum(np.round(target_size0).astype(int), 1))\n    if dimension_order in ['yxc', 'yx']:\n        new_image = cv.resize(np.asarray(image), target_size, interpolation=interpolation)\n    elif dimension_order == 'cyx':\n        new_image = np.moveaxis(image, 0, -1)\n        new_image = cv.resize(np.asarray(new_image), target_size, interpolation=interpolation)\n        new_image = np.moveaxis(new_image, -1, 0)\n    else:\n        ts = image.shape[dimension_order.index('t')] if 't' in dimension_order else 1\n        zs = image.shape[dimension_order.index('z')] if 'z' in dimension_order else 1\n        target_shape = list(image.shape).copy()\n        target_shape[x_index] = target_size[0]\n        target_shape[y_index] = target_size[1]\n        new_image = np.zeros(target_shape, dtype=image.dtype)\n        for t in range(ts):\n            for z in range(zs):\n                slices = get_numpy_slicing(dimension_order, z=z, t=t)\n                image1 = image[slices]\n                if not c_is_at_end:\n                    image1 = np.moveaxis(image1, 0, -1)\n                new_image1 = np.atleast_3d(cv.resize(np.asarray(image1), target_size, interpolation=interpolation))\n                if not c_is_at_end:\n                    new_image1 = np.moveaxis(new_image1, -1, 0)\n                new_image[slices] = new_image1\n    new_image = convert_image_sign_type(new_image, dtype0)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.int2float_image","title":"<code>int2float_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def int2float_image(image):\n    source_dtype = image.dtype\n    if not source_dtype.kind == 'f':\n        maxval = 2 ** (8 * source_dtype.itemsize) - 1\n        return image / np.float32(maxval)\n    else:\n        return image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.norm_image_quantiles","title":"<code>norm_image_quantiles(image0, quantile=0.99)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_quantiles(image0, quantile=0.99):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    min_value = np.quantile(image, 1 - quantile)\n    max_value = np.quantile(image, quantile)\n    normimage = (image - np.mean(image)) / (max_value - min_value)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.norm_image_variance","title":"<code>norm_image_variance(image0)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def norm_image_variance(image0):\n    if len(image0.shape) == 3 and image0.shape[2] == 4:\n        image, alpha = image0[..., :3], image0[..., 3]\n    else:\n        image, alpha = image0, None\n    normimage = (image - np.mean(image)) / np.std(image)\n    normimage = normimage.clip(0, 1).astype(np.float32)\n    if alpha is not None:\n        normimage = np.dstack([normimage, alpha])\n    return normimage\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.normalise","title":"<code>normalise(sims, transform_key, use_global=True)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise(sims, transform_key, use_global=True):\n    new_sims = []\n    dtype = sims[0].dtype\n    # global mean and stddev\n    if use_global:\n        mins = []\n        ranges = []\n        for sim in sims:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n            #min, max = get_image_window(sim, low=0.01, high=0.99)\n            #range = max - min\n            mins.append(min)\n            ranges.append(range)\n        min = np.mean(mins)\n        range = np.mean(ranges)\n    else:\n        min = 0\n        range = 1\n    # normalise all images\n    for sim in sims:\n        if not use_global:\n            min = np.mean(sim, dtype=np.float32)\n            range = np.std(sim, dtype=np.float32)\n        image = (sim - min) / range\n        image = float2int_image(image.clip(0, 1), dtype)    # np.clip(image) is not dask-compatible, use image.clip() instead\n        new_sim = si_utils.get_sim_from_array(\n            image,\n            dims=sim.dims,\n            scale=si_utils.get_spacing_from_sim(sim),\n            translation=si_utils.get_origin_from_sim(sim),\n            transform_key=transform_key,\n            affine=si_utils.get_affine_from_sim(sim, transform_key),\n            c_coords=sim.c.data,\n            t_coords=sim.t.data\n        )\n        new_sims.append(new_sim)\n    return new_sims\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.normalise_values","title":"<code>normalise_values(image, min_value, max_value)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def normalise_values(image: np.ndarray, min_value: float, max_value: float) -&gt; np.ndarray:\n    image = (image.astype(np.float32) - min_value) / (max_value - min_value)\n    return image.clip(0, 1)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.pilmode_to_pixelinfo","title":"<code>pilmode_to_pixelinfo(mode)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def pilmode_to_pixelinfo(mode: str) -&gt; tuple:\n    pixelinfo = (np.uint8, 8, 1)\n    mode_types = {\n        'I': (np.uint32, 32, 1),\n        'F': (np.float32, 32, 1),\n        'RGB': (np.uint8, 24, 3),\n        'RGBA': (np.uint8, 32, 4),\n        'CMYK': (np.uint8, 32, 4),\n        'YCbCr': (np.uint8, 24, 3),\n        'LAB': (np.uint8, 24, 3),\n        'HSV': (np.uint8, 24, 3),\n    }\n    if '16' in mode:\n        pixelinfo = (np.uint16, 16, 1)\n    elif '32' in mode:\n        pixelinfo = (np.uint32, 32, 1)\n    elif mode in mode_types:\n        pixelinfo = mode_types[mode]\n    pixelinfo = (np.dtype(pixelinfo[0]), pixelinfo[1])\n    return pixelinfo\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.precise_resize","title":"<code>precise_resize(image, factors)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def precise_resize(image: np.ndarray, factors) -&gt; np.ndarray:\n    if image.ndim &gt; len(factors):\n        factors = list(factors) + [1]\n    new_image = downscale_local_mean(np.asarray(image), tuple(factors)).astype(image.dtype)\n    return new_image\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.redimension_data","title":"<code>redimension_data(data, old_order, new_order, **indices)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def redimension_data(data, old_order, new_order, **indices):\n    # able to provide optional dimension values e.g. t=0, z=0\n    if new_order == old_order:\n        return data\n\n    new_data = data\n    order = old_order\n    # remove\n    for o in old_order:\n        if o not in new_order:\n            index = order.index(o)\n            dim_value = indices.get(o, 0)\n            new_data = np.take(new_data, indices=dim_value, axis=index)\n            order = order[:index] + order[index + 1:]\n    # add\n    for o in new_order:\n        if o not in order:\n            new_data = np.expand_dims(new_data, 0)\n            order = o + order\n    # move\n    old_indices = [order.index(o) for o in new_order]\n    new_indices = list(range(len(new_order)))\n    new_data = np.moveaxis(new_data, old_indices, new_indices)\n    return new_data\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.resize_image","title":"<code>resize_image(image, new_size)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def resize_image(image, new_size):\n    if not isinstance(new_size, (tuple, list, np.ndarray)):\n        # use single value for width; apply aspect ratio\n        size = np.flip(image.shape[:2])\n        new_size = new_size, new_size * size[1] // size[0]\n    return cv.resize(image, new_size)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.show_image","title":"<code>show_image(image, title='', cmap=None)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def show_image(image, title='', cmap=None):\n    nchannels = image.shape[2] if len(image.shape) &gt; 2 else 1\n    if cmap is None:\n        cmap = 'gray' if nchannels == 1 else None\n    plt.imshow(image, cmap=cmap)\n    if title != '':\n        plt.title(title)\n    plt.show()\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.uint8_image","title":"<code>uint8_image(image)</code>","text":"Source code in <code>src\\image\\util.py</code> <pre><code>def uint8_image(image):\n    source_dtype = image.dtype\n    if source_dtype.kind == 'f':\n        image = image * 255\n    elif source_dtype.itemsize != 1:\n        factor = 2 ** (8 * (source_dtype.itemsize - 1))\n        image = image // factor\n    return image.astype(np.uint8)\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMethodSkFeatures.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMetrics","title":"<code>RegistrationMetrics</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics","title":"<code>RegistrationMetrics</code>","text":"<p>               Bases: <code>RegistrationMethod</code></p> Source code in <code>src\\registration_methods\\RegistrationMetrics.py</code> <pre><code>class RegistrationMetrics(RegistrationMethod):\n    def __init__(self, source_type, reg_function):\n        super().__init__(source_type)\n        self.reg_function = reg_function\n        self.nccs = []\n        self.ssims = []\n\n    def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n        results = self.reg_function(fixed_data, moving_data, **kwargs)\n\n        # TODO: move moving_data using returned transform - see phase_correlation_registration()\n        fixed_data = fixed_data.squeeze()\n        moving_data = moving_data.squeeze()\n        self.nccs.append(calc_ncc(fixed_data, moving_data))\n        self.ssims.append(calc_ssim(fixed_data, moving_data))\n\n        return results\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics.nccs","title":"<code>nccs = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics.reg_function","title":"<code>reg_function = reg_function</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics.ssims","title":"<code>ssims = []</code>  <code>instance-attribute</code>","text":""},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics.__init__","title":"<code>__init__(source_type, reg_function)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMetrics.py</code> <pre><code>def __init__(self, source_type, reg_function):\n    super().__init__(source_type)\n    self.reg_function = reg_function\n    self.nccs = []\n    self.ssims = []\n</code></pre>"},{"location":"references/#src.registration_methods.RegistrationMetrics.RegistrationMetrics.registration","title":"<code>registration(fixed_data, moving_data, **kwargs)</code>","text":"Source code in <code>src\\registration_methods\\RegistrationMetrics.py</code> <pre><code>def registration(self, fixed_data: SpatialImage, moving_data: SpatialImage, **kwargs) -&gt; dict:\n    results = self.reg_function(fixed_data, moving_data, **kwargs)\n\n    # TODO: move moving_data using returned transform - see phase_correlation_registration()\n    fixed_data = fixed_data.squeeze()\n    moving_data = moving_data.squeeze()\n    self.nccs.append(calc_ncc(fixed_data, moving_data))\n    self.ssims.append(calc_ssim(fixed_data, moving_data))\n\n    return results\n</code></pre>"},{"location":"references/#src.util","title":"<code>util</code>","text":""},{"location":"references/#src.util.apply_transform","title":"<code>apply_transform(points, transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def apply_transform(points, transform):\n    new_points = []\n    for point in points:\n        point_len = len(point)\n        while len(point) &lt; len(transform):\n            point = list(point) + [1]\n        new_point = np.dot(point, np.transpose(np.array(transform)))\n        new_points.append(new_point[:point_len])\n    return new_points\n</code></pre>"},{"location":"references/#src.util.check_round_significants","title":"<code>check_round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def check_round_significants(a: float, significant_digits: int) -&gt; float:\n    rounded = round_significants(a, significant_digits)\n    if a != 0:\n        dif = 1 - rounded / a\n    else:\n        dif = rounded - a\n    if abs(dif) &lt; 10 ** -significant_digits:\n        return rounded\n    return a\n</code></pre>"},{"location":"references/#src.util.convert_rational_value","title":"<code>convert_rational_value(value)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_rational_value(value) -&gt; float:\n    if value is not None and isinstance(value, tuple):\n        if value[0] == value[1]:\n            value = value[0]\n        else:\n            value = value[0] / value[1]\n    return value\n</code></pre>"},{"location":"references/#src.util.convert_to_um","title":"<code>convert_to_um(value, unit)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def convert_to_um(value, unit):\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    return value * conversions.get(unit, 1)\n</code></pre>"},{"location":"references/#src.util.create_transform","title":"<code>create_transform(center, angle, matrix_size=3)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform(center, angle, matrix_size=3):\n    if isinstance(center, dict):\n        center = dict_to_xyz(center)\n    if len(center) == 2:\n        center = np.array(list(center) + [0])\n    if angle is None:\n        angle = 0\n    r = Rotation.from_euler('z', angle, degrees=True)\n    t = center - r.apply(center, inverse=True)\n    transform = np.eye(matrix_size)\n    transform[:3, :3] = np.transpose(r.as_matrix())\n    transform[:3, -1] += t\n    return transform\n</code></pre>"},{"location":"references/#src.util.create_transform0","title":"<code>create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def create_transform0(center=(0, 0), angle=0, scale=1, translate=(0, 0)):\n    transform = cv.getRotationMatrix2D(center[:2], angle, scale)\n    transform[:, 2] += translate\n    if len(transform) == 2:\n        transform = np.vstack([transform, [0, 0, 1]])   # create 3x3 matrix\n    return transform\n</code></pre>"},{"location":"references/#src.util.desc_to_dict","title":"<code>desc_to_dict(desc)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def desc_to_dict(desc: str) -&gt; dict:\n    desc_dict = {}\n    if desc.startswith('{'):\n        try:\n            metadata = ast.literal_eval(desc)\n            return metadata\n        except:\n            pass\n    for item in re.split(r'[\\r\\n\\t|]', desc):\n        item_sep = '='\n        if ':' in item:\n            item_sep = ':'\n        if item_sep in item:\n            items = item.split(item_sep)\n            key = items[0].strip()\n            value = items[1].strip()\n            for dtype in (int, float, bool):\n                try:\n                    value = dtype(value)\n                    break\n                except:\n                    pass\n            desc_dict[key] = value\n    return desc_dict\n</code></pre>"},{"location":"references/#src.util.dict_to_xyz","title":"<code>dict_to_xyz(dct, keys='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dict_to_xyz(dct, keys='xyz'):\n    return [dct[key] for key in keys if key in dct]\n</code></pre>"},{"location":"references/#src.util.dir_regex","title":"<code>dir_regex(pattern)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def dir_regex(pattern):\n    files = []\n    for pattern_item in ensure_list(pattern):\n        files.extend(glob.glob(pattern_item, recursive=True))\n    files_sorted = sorted(files, key=lambda file: find_all_numbers(get_filetitle(file)))\n    return files_sorted\n</code></pre>"},{"location":"references/#src.util.draw_edge_filter","title":"<code>draw_edge_filter(bounds)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def draw_edge_filter(bounds):\n    out_image = np.zeros(np.flip(bounds))\n    y, x = np.where(out_image == 0)\n    points = np.transpose([x, y])\n\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) * 10, 0, 1)\n    return position_weights.reshape(np.flip(bounds))\n</code></pre>"},{"location":"references/#src.util.ensure_list","title":"<code>ensure_list(x)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def ensure_list(x) -&gt; list:\n    if x is None:\n        return []\n    elif isinstance(x, list):\n        return x\n    else:\n        return [x]\n</code></pre>"},{"location":"references/#src.util.eval_context","title":"<code>eval_context(data, key, default_value, context)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def eval_context(data, key, default_value, context):\n    value = data.get(key, default_value)\n    if isinstance(value, str):\n        try:\n            value = value.format_map(context)\n        except:\n            pass\n        try:\n            value = eval(value, context)\n        except:\n            pass\n    return value\n</code></pre>"},{"location":"references/#src.util.export_csv","title":"<code>export_csv(filename, data, header=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_csv(filename, data, header=None):\n    with open(filename, 'w', encoding='utf8', newline='') as file:\n        csvwriter = csv.writer(file)\n        if header is not None:\n            csvwriter.writerow(header)\n        for row in data:\n            csvwriter.writerow(row)\n</code></pre>"},{"location":"references/#src.util.export_json","title":"<code>export_json(filename, data)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def export_json(filename, data):\n    with open(filename, 'w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"references/#src.util.filter_dict","title":"<code>filter_dict(dict0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_dict(dict0: dict) -&gt; dict:\n    new_dict = {}\n    for key, value0 in dict0.items():\n        if value0 is not None:\n            values = []\n            for value in ensure_list(value0):\n                if isinstance(value, dict):\n                    value = filter_dict(value)\n                values.append(value)\n            if len(values) == 1:\n                values = values[0]\n            new_dict[key] = values\n    return new_dict\n</code></pre>"},{"location":"references/#src.util.filter_edge_points","title":"<code>filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def filter_edge_points(points, bounds, filter_factor=0.1, threshold=0.5):\n    center = np.array(bounds) / 2\n    dist_center = np.abs(points / center - 1)\n    position_weights = np.clip((1 - np.max(dist_center, axis=-1)) / filter_factor, 0, 1)\n    order_weights = 1 - np.array(range(len(points))) / len(points) / 2\n    weights = position_weights * order_weights\n    return weights &gt; threshold\n</code></pre>"},{"location":"references/#src.util.find_all_numbers","title":"<code>find_all_numbers(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def find_all_numbers(text: str) -&gt; list:\n    return list(map(int, re.findall(r'\\d+', text)))\n</code></pre>"},{"location":"references/#src.util.get_center","title":"<code>get_center(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center(data, offset=(0, 0)):\n    moments = get_moments(data, offset=offset)\n    if moments['m00'] != 0:\n        center = get_moments_center(moments)\n    else:\n        center = np.mean(data, 0).flatten()  # close approximation\n    return center.astype(np.float32)\n</code></pre>"},{"location":"references/#src.util.get_center_from_transform","title":"<code>get_center_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_center_from_transform(transform):\n    # from opencv:\n    # t0 = (1-alpha) * cx - beta * cy\n    # t1 = beta * cx + (1-alpha) * cy\n    # where\n    # alpha = cos(angle) * scale\n    # beta = sin(angle) * scale\n    # isolate cx and cy:\n    t0, t1 = transform[:2, 2]\n    scale = 1\n    angle = np.arctan2(transform[0][1], transform[0][0])\n    alpha = np.cos(angle) * scale\n    beta = np.sin(angle) * scale\n    cx = (t1 + t0 * (1 - alpha) / beta) / (beta + (1 - alpha) ** 2 / beta)\n    cy = ((1 - alpha) * cx - t0) / beta\n    return cx, cy\n</code></pre>"},{"location":"references/#src.util.get_default","title":"<code>get_default(x, default)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_default(x, default):\n    return default if x is None else x\n</code></pre>"},{"location":"references/#src.util.get_filetitle","title":"<code>get_filetitle(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_filetitle(filename: str) -&gt; str:\n    filebase = os.path.basename(filename)\n    title = os.path.splitext(filebase)[0].rstrip('.ome')\n    return title\n</code></pre>"},{"location":"references/#src.util.get_mean_nn_distance","title":"<code>get_mean_nn_distance(points1, points2)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_mean_nn_distance(points1, points2):\n    return np.mean([get_nn_distance(points1), get_nn_distance(points2)])\n</code></pre>"},{"location":"references/#src.util.get_moments","title":"<code>get_moments(data, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments(data, offset=(0, 0)):\n    moments = cv.moments((np.array(data) + offset).astype(np.float32))    # doesn't work for float64!\n    return moments\n</code></pre>"},{"location":"references/#src.util.get_moments_center","title":"<code>get_moments_center(moments, offset=(0, 0))</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_moments_center(moments, offset=(0, 0)):\n    return np.array([moments['m10'], moments['m01']]) / moments['m00'] + np.array(offset)\n</code></pre>"},{"location":"references/#src.util.get_nn_distance","title":"<code>get_nn_distance(points0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_nn_distance(points0):\n    points = list(set(map(tuple, points0)))     # get unique points\n    if len(points) &gt;= 2:\n        tree = KDTree(points, leaf_size=2)\n        dist, ind = tree.query(points, k=2)\n        nn_distance = np.median(dist[:, 1])\n    else:\n        nn_distance = 1\n    return nn_distance\n</code></pre>"},{"location":"references/#src.util.get_orthogonal_pairs","title":"<code>get_orthogonal_pairs(origins, image_size_um)</code>","text":"<p>Get pairs of orthogonal neighbors from a list of tiles. Tiles don't have to be placed on a regular grid.</p> Source code in <code>src\\util.py</code> <pre><code>def get_orthogonal_pairs(origins, image_size_um):\n    \"\"\"\n    Get pairs of orthogonal neighbors from a list of tiles.\n    Tiles don't have to be placed on a regular grid.\n    \"\"\"\n    pairs = []\n    angles = []\n    z_positions = [pos[0] for pos in origins if len(pos) == 3]\n    ordered_z = sorted(set(z_positions))\n    is_mixed_3dstack = len(ordered_z) &lt; len(z_positions)\n    for i, j in np.transpose(np.triu_indices(len(origins), 1)):\n        origini = np.array(origins[i])\n        originj = np.array(origins[j])\n        if is_mixed_3dstack:\n            # ignore z value for distance\n            distance = math.dist(origini[-2:], originj[-2:])\n            min_distance = max(image_size_um[-2:])\n            z_i, z_j = origini[0], originj[0]\n            is_same_z = (z_i == z_j)\n            is_close_z = abs(ordered_z.index(z_i) - ordered_z.index(z_j)) &lt;= 1\n            if not is_same_z:\n                # for tiles in different z stack, require greater overlap\n                min_distance *= 0.8\n            ok = (distance &lt; min_distance and is_close_z)\n        else:\n            distance = math.dist(origini, originj)\n            min_distance = max(image_size_um)\n            ok = (distance &lt; min_distance)\n        if ok:\n            pairs.append((i, j))\n            vector = origini - originj\n            angle = math.degrees(math.atan2(vector[1], vector[0]))\n            if distance &lt; min(image_size_um):\n                angle += 90\n            while angle &lt; -90:\n                angle += 180\n            while angle &gt; 90:\n                angle -= 180\n            angles.append(angle)\n    return pairs, angles\n</code></pre>"},{"location":"references/#src.util.get_rotation_from_transform","title":"<code>get_rotation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_rotation_from_transform(transform):\n    rotation = np.rad2deg(np.arctan2(transform[0][1], transform[0][0]))\n    return rotation\n</code></pre>"},{"location":"references/#src.util.get_scale_from_transform","title":"<code>get_scale_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_scale_from_transform(transform):\n    scale = np.mean(np.linalg.norm(transform, axis=0)[:-1])\n    return scale\n</code></pre>"},{"location":"references/#src.util.get_translation_from_transform","title":"<code>get_translation_from_transform(transform)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_translation_from_transform(transform):\n    ndim = len(transform) - 1\n    #translation = transform[:ndim, ndim]\n    translation = apply_transform([[0] * ndim], transform)[0]\n    return translation\n</code></pre>"},{"location":"references/#src.util.get_unique_file_labels","title":"<code>get_unique_file_labels(filenames)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_unique_file_labels(filenames: list) -&gt; list:\n    file_labels = []\n    file_parts = []\n    label_indices = set()\n    last_parts = None\n    for filename in filenames:\n        parts = split_numeric(filename)\n        if len(parts) == 0:\n            parts = split_numeric(filename)\n            if len(parts) == 0:\n                parts = filename\n        file_parts.append(parts)\n        if last_parts is not None:\n            for parti, (part1, part2) in enumerate(zip(last_parts, parts)):\n                if part1 != part2:\n                    label_indices.add(parti)\n        last_parts = parts\n    label_indices = sorted(list(label_indices))\n\n    for file_part in file_parts:\n        file_label = '_'.join([file_part[i] for i in label_indices])\n        file_labels.append(file_label)\n\n    if len(set(file_labels)) &lt; len(file_labels):\n        # fallback for duplicate labels\n        file_labels = [get_filetitle(filename) for filename in filenames]\n\n    return file_labels\n</code></pre>"},{"location":"references/#src.util.get_value_units_micrometer","title":"<code>get_value_units_micrometer(value_units0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def get_value_units_micrometer(value_units0: list|dict) -&gt; list|dict|None:\n    conversions = {\n        'nm': 1e-3,\n        '\u00b5m': 1, 'um': 1, 'micrometer': 1, 'micron': 1,\n        'mm': 1e3, 'millimeter': 1e3,\n        'cm': 1e4, 'centimeter': 1e4,\n        'm': 1e6, 'meter': 1e6\n    }\n    if value_units0 is None:\n        return None\n\n    if isinstance(value_units0, dict):\n        values_um = {}\n        for dim, value_unit in value_units0.items():\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um[dim] = value_um\n    else:\n        values_um = []\n        for value_unit in value_units0:\n            if isinstance(value_unit, (list, tuple)):\n                value_um = value_unit[0] * conversions.get(value_unit[1], 1)\n            else:\n                value_um = value_unit\n            values_um.append(value_um)\n    return values_um\n</code></pre>"},{"location":"references/#src.util.import_csv","title":"<code>import_csv(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_csv(filename):\n    with open(filename, encoding='utf8') as file:\n        data = csv.reader(file)\n    return data\n</code></pre>"},{"location":"references/#src.util.import_json","title":"<code>import_json(filename)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_json(filename):\n    with open(filename, encoding='utf8') as file:\n        data = json.load(file)\n    return data\n</code></pre>"},{"location":"references/#src.util.import_metadata","title":"<code>import_metadata(content, fields=None, input_path=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def import_metadata(content, fields=None, input_path=None):\n    # return dict[id] = {values}\n    if isinstance(content, str):\n        ext = os.path.splitext(content)[1].lower()\n        if input_path:\n            if isinstance(input_path, list):\n                input_path = input_path[0]\n            content = os.path.normpath(os.path.join(os.path.dirname(input_path), content))\n        if ext == '.csv':\n            content = import_csv(content)\n        elif ext in ['.json', '.ome.json']:\n            content = import_json(content)\n    if fields is not None:\n        content = [[data[field] for field in fields] for data in content]\n    return content\n</code></pre>"},{"location":"references/#src.util.normalise_rotated_positions","title":"<code>normalise_rotated_positions(positions0, rotations0, size, center)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def normalise_rotated_positions(positions0, rotations0, size, center):\n    # in [xy(z)]\n    positions = []\n    rotations = []\n    _, angles = get_orthogonal_pairs(positions0, size)\n    for position0, rotation in zip(positions0, rotations0):\n        if rotation is None and len(angles) &gt; 0:\n            rotation = -np.mean(angles)\n        angle = -rotation if rotation is not None else None\n        transform = create_transform(center=center, angle=angle, matrix_size=4)\n        position = apply_transform([position0], transform)[0]\n        positions.append(position)\n        rotations.append(rotation)\n    return positions, rotations\n</code></pre>"},{"location":"references/#src.util.normalise_rotation","title":"<code>normalise_rotation(rotation)</code>","text":"<p>Normalise rotation to be in the range [-180, 180].</p> Source code in <code>src\\util.py</code> <pre><code>def normalise_rotation(rotation):\n    \"\"\"\n    Normalise rotation to be in the range [-180, 180].\n    \"\"\"\n    while rotation &lt; -180:\n        rotation += 360\n    while rotation &gt; 180:\n        rotation -= 360\n    return rotation\n</code></pre>"},{"location":"references/#src.util.points_to_3d","title":"<code>points_to_3d(points)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def points_to_3d(points):\n    return [list(point) + [0] for point in points]\n</code></pre>"},{"location":"references/#src.util.print_dict","title":"<code>print_dict(dct, indent=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_dict(dct: dict, indent: int = 0) -&gt; str:\n    s = ''\n    if isinstance(dct, dict):\n        for key, value in dct.items():\n            s += '\\n'\n            if not isinstance(value, list):\n                s += '\\t' * indent + str(key) + ': '\n            if isinstance(value, dict):\n                s += print_dict(value, indent=indent + 1)\n            elif isinstance(value, list):\n                for v in value:\n                    s += print_dict(v)\n            else:\n                s += str(value)\n    else:\n        s += str(dct)\n    return s\n</code></pre>"},{"location":"references/#src.util.print_hbytes","title":"<code>print_hbytes(nbytes)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def print_hbytes(nbytes: int) -&gt; str:\n    exps = ['', 'K', 'M', 'G', 'T', 'P', 'E']\n    div = 1024\n    exp = 0\n\n    while nbytes &gt; div:\n        nbytes /= div\n        exp += 1\n    if exp &lt; len(exps):\n        e = exps[exp]\n    else:\n        e = f'e{exp * 3}'\n    return f'{nbytes:.1f}{e}B'\n</code></pre>"},{"location":"references/#src.util.reorder","title":"<code>reorder(items, old_order, new_order, default_value=0)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def reorder(items: list, old_order: str, new_order: str, default_value: int = 0) -&gt; list:\n    new_items = []\n    for label in new_order:\n        if label in old_order:\n            item = items[old_order.index(label)]\n        else:\n            item = default_value\n        new_items.append(item)\n    return new_items\n</code></pre>"},{"location":"references/#src.util.retuple","title":"<code>retuple(chunks, shape)</code>","text":"<p>Expand chunks to match shape.</p> <p>E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028) return (3, 4, 5, 64, 64)</p> <p>If chunks is an integer, it is applied to all dimensions, to match the behaviour of zarr-python.</p> Source code in <code>src\\util.py</code> <pre><code>def retuple(chunks, shape):\n    # from ome-zarr-py\n    \"\"\"\n    Expand chunks to match shape.\n\n    E.g. if chunks is (64, 64) and shape is (3, 4, 5, 1028, 1028)\n    return (3, 4, 5, 64, 64)\n\n    If chunks is an integer, it is applied to all dimensions, to match\n    the behaviour of zarr-python.\n    \"\"\"\n\n    if isinstance(chunks, int):\n        return tuple([chunks] * len(shape))\n\n    dims_to_add = len(shape) - len(chunks)\n    return *shape[:dims_to_add], *chunks\n</code></pre>"},{"location":"references/#src.util.round_significants","title":"<code>round_significants(a, significant_digits)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def round_significants(a: float, significant_digits: int) -&gt; float:\n    if a != 0:\n        round_decimals = significant_digits - int(np.floor(np.log10(abs(a)))) - 1\n        return round(a, round_decimals)\n    return a\n</code></pre>"},{"location":"references/#src.util.split_num_text","title":"<code>split_num_text(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_num_text(text: str) -&gt; list:\n    num_texts = []\n    block = ''\n    is_num0 = None\n    if text is None:\n        return None\n\n    for c in text:\n        is_num = (c.isnumeric() or c == '.')\n        if is_num0 is not None and is_num != is_num0:\n            num_texts.append(block)\n            block = ''\n        block += c\n        is_num0 = is_num\n    if block != '':\n        num_texts.append(block)\n\n    num_texts2 = []\n    for block in num_texts:\n        block = block.strip()\n        try:\n            block = float(block)\n        except:\n            pass\n        if block not in [' ', ',', '|']:\n            num_texts2.append(block)\n    return num_texts2\n</code></pre>"},{"location":"references/#src.util.split_numeric","title":"<code>split_numeric(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric(text: str) -&gt; list:\n    num_parts = []\n    parts = re.split(r'[_/\\\\.]', text)\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            num_parts.append(part)\n    return num_parts\n</code></pre>"},{"location":"references/#src.util.split_numeric_dict","title":"<code>split_numeric_dict(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_numeric_dict(text: str) -&gt; dict:\n    num_parts = {}\n    parts = re.split(r'[_/\\\\.]', text)\n    parti = 0\n    for part in parts:\n        num_span = re.search(r'\\d+', part)\n        if num_span:\n            index = num_span.start()\n            label = part[:index]\n            if label == '':\n                label = parti\n            num_parts[label] = num_span.group()\n            parti += 1\n    return num_parts\n</code></pre>"},{"location":"references/#src.util.split_path","title":"<code>split_path(path)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_path(path: str) -&gt; list:\n    return os.path.normpath(path).split(os.path.sep)\n</code></pre>"},{"location":"references/#src.util.split_value_unit_list","title":"<code>split_value_unit_list(text)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def split_value_unit_list(text: str) -&gt; list:\n    value_units = []\n    if text is None:\n        return None\n\n    items = split_num_text(text)\n    if isinstance(items[-1], str):\n        def_unit = items[-1]\n    else:\n        def_unit = ''\n\n    i = 0\n    while i &lt; len(items):\n        value = items[i]\n        if i + 1 &lt; len(items):\n            unit = items[i + 1]\n        else:\n            unit = ''\n        if not isinstance(value, str):\n            if isinstance(unit, str):\n                i += 1\n            else:\n                unit = def_unit\n            value_units.append((value, unit))\n        i += 1\n    return value_units\n</code></pre>"},{"location":"references/#src.util.validate_transform","title":"<code>validate_transform(transform, max_rotation=None)</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def validate_transform(transform, max_rotation=None):\n    if transform is None:\n        return False\n    transform = np.array(transform)\n    if np.any(np.isnan(transform)):\n        return False\n    if np.any(np.isinf(transform)):\n        return False\n    if np.linalg.det(transform) == 0:\n        return False\n    if  max_rotation is not None and abs(normalise_rotation(get_rotation_from_transform(transform))) &gt; max_rotation:\n        return False\n    return True\n</code></pre>"},{"location":"references/#src.util.xyz_to_dict","title":"<code>xyz_to_dict(xyz, axes='xyz')</code>","text":"Source code in <code>src\\util.py</code> <pre><code>def xyz_to_dict(xyz, axes='xyz'):\n    dct = {dim: value for dim, value in zip(axes, xyz)}\n    return dct\n</code></pre>"}]}